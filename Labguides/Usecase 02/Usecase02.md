**Introdu√ß√£o**

O Apache Spark √© um mecanismo de c√≥digo aberto para processamento
distribu√≠do de dados, amplamente utilizado para explorar, processar e
analisar grandes volumes de dados em data lakes. O Spark est√° dispon√≠vel
como op√ß√£o de processamento em diversos produtos de plataforma de dados,
incluindo Azure HDInsight, Azure Databricks, Azure Synapse Analytics e
Microsoft Fabric. Um dos benef√≠cios do Spark √© o suporte a uma ampla
gama de linguagens de programa√ß√£o, incluindo Java, Scala, Python e SQL,
tornando o Spark uma solu√ß√£o muito flex√≠vel para cargas de trabalho de
processamento de dados, incluindo limpeza e manipula√ß√£o de dados,
an√°lise estat√≠stica e aprendizado de m√°quina, al√©m de an√°lise e
visualiza√ß√£o de dados.

As tabelas em um lakehouse do Microsoft Fabric s√£o baseadas no c√≥digo
aberto¬†Formato *Delta Lake* para Apache Spark. O Delta Lake adiciona
suporte √† sem√¢ntica relacional para opera√ß√µes de dados em lote e
streaming e permite a cria√ß√£o de uma arquitetura Lakehouse na qual o
Apache Spark pode ser usado para processar e consultar dados em tabelas
baseadas em arquivos subjacentes em um data lake.

No Microsoft Fabric, os Fluxos de Dados (Gen2) se conectam a v√°rias
fontes de dados e realizam transforma√ß√µes no Power Query Online. Eles
podem ser usados em Pipelines de Dados para inserir dados em um
lakehouse ou outro armazenamento anal√≠tico, ou para definir um conjunto
de dados para um relat√≥rio do Power BI.

Este laborat√≥rio foi projetado para apresentar os diferentes elementos
dos Dataflows (Gen2), e n√£o criar uma solu√ß√£o complexa que possa existir
em uma empresa.

**Objetivos** :

- Crie um workspace no Microsoft Fabric com a vers√£o de avalia√ß√£o do
  Fabric habilitada.

- Estabele√ßa um ambiente de lakehouse e carregue arquivos de dados para
  an√°lise.

- Gere um notebook para explora√ß√£o e an√°lise interativa de dados.

- Carregue os dados em um dataframe para posterior processamento e
  visualiza√ß√£o.

- Aplique transforma√ß√µes aos dados usando o PySpark .

- Salve e participe dos dados transformados para otimizar as consultas.

- Crie uma tabela no metastore do Spark para gerenciamento de dados
  estruturados

- Salve o DataFrame como uma tabela delta gerenciada chamada "
  salesorders".

- Salve o DataFrame como uma tabela delta externa chamada
  "external_salesorder" com um caminho espec√≠fico.

- Descreva e compare propriedades de tabelas gerenciadas e externas.

- Execute consultas SQL em tabelas para an√°lise e gera√ß√£o de relat√≥rios.

- Visualize dados usando bibliotecas Python, como matplotlib e seaborn.

- Estabele√ßa um data lakehouse na experi√™ncia de Engenharia de Dados e
  insira dados relevantes para an√°lise subsequente.

- Defina um fluxo de dados para extrair, transformar e carregar dados no
  lakehouse .

- Configure destinos de dados no Power Query para armazenar os dados
  transformados no lakehouse .

- Incorporar o fluxo de dados em um pipeline para permitir o
  processamento e a inser√ß√£o de dados programados.

- Remova o workspace e os elementos associados para concluir o
  exerc√≠cio.

# Exerc√≠cio 1: Crie um workspace, um lakehouse , um notebook e carregue os dados no dataframe 

## Tarefa 1: Criar um workspace

Antes de trabalhar com dados no Fabric, crie um workspace com o teste do
Fabric habilitado.

1.  Abra seu navegador, navegue at√© a barra de endere√ßo e digite ou cole
    o seguinte URL: <https://app.fabric.microsoft.com/> ent√£o pressione
    o bot√£o **Enter .**

> **Observa√ß√£o** : se voc√™ for direcionado para a p√°gina inicial do
> Microsoft Fabric, pule as etapas de 2 a 4.
>
> ![](./media/image1.png)

2.  Na janela **do Microsoft Fabric** ,insira suas credenciais e clique
    no bot√£o **Submit**.

> ![](./media/image2.png)

3.  Em seguida, na janela da **Microsoft**, digite a senha e clique no
    bot√£o **Sign in.**

> ![A login screen with a red box and blue text Description
> automatically generated](./media/image3.png)

4.  Na janela **Stay signed in?,** clique no bot√£o **Yes**.

> ![A screenshot of a computer error Description automatically
> generated](./media/image4.png)

5.  P√°gina inicial do Fabric , selecione **+New workspace**.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image5.png)

6.  Na **aba Create a workspace**, insira os seguintes detalhes e clique
    no bot√£o **Apply**.

[TABLE]

> ![A screenshot of a computer Description automatically
> generated](./media/image6.png)
>
> ![](./media/image7.png)

![](./media/image8.png)

![](./media/image9.png)

7.  Aguarde a conclus√£o da implementa√ß√£o. Leva de 2 a 3 minutos. Quando
    o seu novo workspace for aberto, ele dever√° estar vazio.

## Tarefa 2: Criar um lakehouse e enviar arquivos

Agora que voc√™ tem um workspace, √© hora de mudar para a experi√™ncia *de
Engenharia de dados* no portal e criar um data lakehouse para os
arquivos de dados que voc√™ vai analisar.

1.  Crie um novo Eventhouse clicando no bot√£o **+New item** na barra de
    navega√ß√£o.

![A screenshot of a browser AI-generated content may be
incorrect.](./media/image10.png)

2.  Clique em "**Lakehouse**".

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image11.png)

3.  Na caixa de di√°logo **New lakehouse**, digite
    **+++Fabric_lakehouse+++** no campo **Nome**, clique no bot√£o
    **Create** e abra o novo lakehouse .

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image12.png)

4.  Ap√≥s cerca de um minuto, um novo lakehouse vazio ser√° criado. Voc√™
    precisar√° inserir alguns dados no data lakehouse para an√°lise.

![](./media/image13.png)

5.  Voc√™ ver√° uma notifica√ß√£o informando **Successfully created SQL
    endpoint**.

![](./media/image14.png)

6.  Na se√ß√£o **Explorer**, em **fabric_lakehouse**, passe o mouse sobre
    **Files folder** e clique no menu de retic√™ncias horizontais **(‚Ä¶)**
    . Navegue e clique em **Upload** .Em seguida, clique na **Upload
    folder**, conforme mostrado na imagem abaixo.

![](./media/image15.png)

7.  No painel **Upload folder** que aparece no lado direito, selecione o
    **folder icon** em **Files/** e navegue at√© **C:\LabFiles** e
    selecione a pasta **orders** e clique no bot√£o **Upload**.

![](./media/image16.png)

8.  Caso a caixa de di√°logo **Upload 3 files to this site**? apare√ßa,
    clique no bot√£o **Upload**.

![](./media/image17.png)

9.  No painel Upload folder, clique no bot√£o **Upload** .

> ![](./media/image18.png)

10. Depois que os arquivos forem carregados, **close** o painel **Upload
    folder**.

> ![](./media/image19.png)

11. Expanda **Files**, selecione a pasta **orders** e verifique se os
    arquivos CSV foram carregados.

![](./media/image20.png)

## Tarefa 3: Criar um notebook

Para trabalhar com dados no Apache Spark, voc√™ pode criar um *notebook*
. Os notebooks oferecem um ambiente interativo no qual voc√™ pode
escrever e executar c√≥digo (em v√°rias linguagens) e adicionar notas para
document√°-lo.

1.  Na p√°gina **Home**, enquanto visualiza o conte√∫do da pasta
    **orders** no seu datalake , no menu **Open notebook**, selecione
    **New notebook**.

![](./media/image21.png)

2.  Ap√≥s alguns segundos, um novo notebook contendo uma √∫nica *c√©lula*
    ser√° aberto. Notebooks s√£o compostos por uma ou mais c√©lulas que
    podem conter *c√≥digo* ou *markdown* (texto formatado).

![](./media/image22.png)

3.  Selecione a primeira c√©lula (que atualmente √© uma c√©lula *de
    c√≥digo*) e, na barra de ferramentas din√¢mica no canto superior
    direito, use o bot√£o **M‚Üì para converter a c√©lula em uma c√©lula de
    markdown** .

![](./media/image23.png)

4.  Quando a c√©lula muda para uma c√©lula markdown, o texto que ela
    cont√©m √© processado.

![](./media/image24.png)

5.  Use o bot√£o **üñâ** (Edit) para alternar a c√©lula para o modo de
    edi√ß√£o, substitua todo o texto e modifique o markdown da seguinte
    forma:

> CodeCopy
>
> \# Sales order data exploration
>
> Use the code in this notebook to explore sales order data.

![](./media/image25.png)

![A screenshot of a computer Description automatically
generated](./media/image26.png)

6.  Clique em qualquer lugar do notebook fora da c√©lula para parar de
    edit√°-lo e ver a marca√ß√£o processada.

![A screenshot of a computer Description automatically
generated](./media/image27.png)

## Tarefa 4: Carregar dados em um dataframe

Agora voc√™ est√° pronto para executar o c√≥digo que carrega os dados em um
*dataframe* .Os dataframes no Spark s√£o semelhantes aos dataframes do
Pandas em Python e fornecem uma estrutura comum para trabalhar com dados
em linhas e colunas.

**Observa√ß√£o** :O Spark suporta diversas linguagens de programa√ß√£o,
incluindo Scala, Java e outras. Neste exerc√≠cio, usaremos *o PySpark* ,
uma variante do Python otimizada para Spark. O PySpark √© uma das
linguagens mais usadas no Spark e √© a linguagem padr√£o nos notebooks do
Fabric.

1.  Com o notebook vis√≠vel, expanda a lista **Files** e selecione a
    pasta **orders** para que os arquivos CSV sejam listados ao lado do
    editor de notebook.

![](./media/image28.png)

2.  Agora, mova o cursor do mouse at√© o arquivo 2019.csv. Clique nas
    retic√™ncias horizontais **(...)** ao lado de 2019.csv. Navegue e
    clique em **Load data** e selecione **Spark**. Uma nova c√©lula de
    c√≥digo contendo o seguinte c√≥digo ser√° adicionada ao notebook:

> CodeCopy
>
> df =
> spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
>
> \# df now is a Spark DataFrame containing CSV data from
> "Files/orders/2019.csv".
>
> display(df)

![](./media/image29.png)

![](./media/image30.png)

**Dica** : Voc√™ pode ocultar os pain√©is do Lakehouse Explorer √† esquerda
usando seus √≠cones **¬´.** azer isso ajudar√° voc√™ a se concentrar melhor
no notebook.

3.  Use o bot√£o **‚ñ∑ Run cell** √† esquerda da c√©lula para execut√°-la.

![](./media/image31.png)

**Observa√ß√£o** : Como esta √© a primeira vez que voc√™ executa um c√≥digo
Spark, uma sess√£o Spark precisa ser iniciada. Isso significa que a
primeira execu√ß√£o da sess√£o pode levar cerca de um minuto para ser
conclu√≠da. As execu√ß√µes subsequentes ser√£o mais r√°pidas.

4.  Quando o comando da c√©lula for conclu√≠do, revise a sa√≠da abaixo da
    c√©lula, que deve ser semelhante a esta:

![](./media/image32.png)

5.  A sa√≠da mostra as linhas e colunas de dados do arquivo 2019.csv. No
    entanto, observe que os cabe√ßalhos das colunas n√£o parecem corretos.
    O c√≥digo padr√£o usado para carregar os dados em um dataframe
    pressup√µe que o arquivo CSV inclui os nomes das colunas na primeira
    linha, mas, neste caso, o arquivo CSV inclui apenas os dados, sem
    informa√ß√µes de cabe√ßalho.

6.  Modifique o c√≥digo para definir a op√ß√£o de **cabe√ßalho** como
    **falsa**. Substitua todo o c√≥digo na **c√©lula** pelo seguinte
    c√≥digo e clique no bot√£o **‚ñ∑ Run cell** e revise a sa√≠da.

> CodeCopy
>
> df =
> spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
>
> \# df now is a Spark DataFrame containing CSV data from
> "Files/orders/2019.csv".
>
> display(df)
>
> ![](./media/image33.png)

7.  Agora, o dataframe inclui corretamente a primeira linha como valores
    de dados, mas os nomes das colunas s√£o gerados automaticamente e n√£o
    s√£o muito √∫teis. Para entender os dados, voc√™ precisa definir
    explicitamente o esquema e o tipo de dados corretos para os valores
    de dados no arquivo.

8.  Substitua todo o c√≥digo na **c√©lula** pelo seguinte c√≥digo e clique
    no bot√£o **‚ñ∑ Run cell** e revise a sa√≠da

> CodeCopy
>
> from pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> StructField("SalesOrderNumber", StringType()),
>
> StructField("SalesOrderLineNumber", IntegerType()),
>
> StructField("OrderDate", DateType()),
>
> StructField("CustomerName", StringType()),
>
> StructField("Email", StringType()),
>
> StructField("Item", StringType()),
>
> StructField("Quantity", IntegerType()),
>
> StructField("UnitPrice", FloatType()),
>
> StructField("Tax", FloatType())
>
> \])
>
> df =
> spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")
>
> display(df)

![](./media/image34.png)

> ![](./media/image35.png)

9.  Agora, o dataframe inclui os nomes de coluna corretos (al√©m do
    **Index**, que √© uma coluna integrada em todos os dataframes com
    base na posi√ß√£o ordinal de cada linha). Os tipos de dados das
    colunas s√£o especificados usando um conjunto padr√£o de tipos
    definidos na biblioteca Spark SQL, que foram importados no in√≠cio da
    c√©lula.

10. Confirme se suas altera√ß√µes foram aplicadas aos dados visualizando o
    dataframe .

11. Use o √≠cone **+Code** abaixo da sa√≠da da c√©lula para adicionar uma
    nova c√©lula de c√≥digo ao notebook e insira o seguinte c√≥digo nela.
    Clique no bot√£o **‚ñ∑ Run cell** e revise a sa√≠da.

> CodeCopy
>
> display(df)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image36.png)

12. O dataframe inclui apenas os dados do arquivo **2019.csv**.
    Modifique o c√≥digo para que o caminho do arquivo use um curinga \*
    para ler os dados do pedido de vendas de todos os arquivos na pasta
    **orders.**

13. Use o √≠cone **+ Code** abaixo da sa√≠da da c√©lula para adicionar uma
    nova c√©lula de c√≥digo ao notebook e insira o seguinte c√≥digo nela.

CodeCopy

> from pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> ¬† ¬† StructField("SalesOrderNumber", StringType()),
>
> ¬† ¬† StructField("SalesOrderLineNumber", IntegerType()),
>
> ¬† ¬† StructField("OrderDate", DateType()),
>
> ¬† ¬† StructField("CustomerName", StringType()),
>
> ¬† ¬† StructField("Email", StringType()),
>
> ¬† ¬† StructField("Item", StringType()),
>
> ¬† ¬† StructField("Quantity", IntegerType()),
>
> ¬† ¬† StructField("UnitPrice", FloatType()),
>
> ¬† ¬† StructField("Tax", FloatType())
>
> ¬† ¬† \])
>
> df =
> spark.read.format("csv").schema(orderSchema).load("Files/orders/\*.csv")

display(df)

![](./media/image37.png)

14. Execute a c√©lula de c√≥digo modificada e revise a sa√≠da, que agora
    deve incluir as vendas de 2019, 2020 e 2021.

![](./media/image38.png)

**Observa√ß√£o** : Sum subconjunto das linhas √© exibido, ent√£o talvez voc√™
n√£o consiga ver exemplos de todos os anos.

# Exerc√≠cio 2: Explorar dados em um dataframe

O objeto dataframe inclui uma ampla gama de fun√ß√µes que voc√™ pode usar
para filtrar, agrupar e manipular os dados que ele cont√©m.

## Tarefa 1: Filtrar um dataframe

1.  Use o √≠cone **+ Code** abaixo da sa√≠da da c√©lula para adicionar uma
    nova c√©lula de c√≥digo ao notebook e insira o seguinte c√≥digo nela.

**CodeCopy**

> customers = df\['CustomerName', 'Email'\]
>
> print(customers.count())
>
> print(customers.distinct().count())
>
> display(customers.distinct())
>
> ![](./media/image39.png)

2.  **Execute** a nova c√©lula de c√≥digo e revise os resultados. Observe
    os seguintes detalhes:

    - Quando voc√™ executa uma opera√ß√£o em um dataframe , o resultado √©
      um novo dataframe (neste caso, um novo **customers**¬†o dataframe √©
      criado selecionando um subconjunto espec√≠fico de colunas do
      **df**¬†dataframe)

    - Os dataframes fornecem fun√ß√µes como **count** e **distinct** que
      podem ser usadas para resumir e filtrar os dados que eles cont√™m.

    - A sintaxe dataframe\['Field1', 'Field2', ...\] √© uma forma
      abreviada de definir um subconjunto de colunas. Voc√™ tamb√©m pode
      usar o m√©todo **select**, ent√£o a primeira linha do c√≥digo acima
      poderia ser escrita como customers = df.select("CustomerName",
      "Email")

> ![](./media/image40.png)

3.  Modifique o c√≥digo, substitua todo o c√≥digo da **cell** pelo c√≥digo
    a seguir e clique no bot√£o **‚ñ∑ Run cell** da seguinte maneira:

> CodeCopy
>
> customers = df.select("CustomerName",
> "Email").where(df\['Item'\]=='Road-250 Red, 52')
>
> print(customers.count())
>
> print(customers.distinct().count())
>
> display(customers.distinct())

4.  **Execute** o c√≥digo modificado para visualizar os clientes que
    compraram o ***Road-250 Red, 52*¬†product**. Observe que voc√™ pode
    ‚Äú**chain**‚Äù v√°rias fun√ß√µes para que a sa√≠da de uma fun√ß√£o se torne a
    entrada da pr√≥xima ‚Äî neste caso, o dataframe criado pelo m√©todo
    **select** √© o dataframe de origem para o m√©todo **where**, usado
    para aplicar os crit√©rios de filtragem.

> ![](./media/image41.png)

## Tarefa 2: Agregar e agrupar dados em um dataframe

1.  Clique em **+ Code** e copie e cole o c√≥digo abaixo e depois clique
    no bot√£o **Run cell**.

CodeCopy

> productSales = df.select("Item", "Quantity").groupBy("Item").sum()
>
> display(productSales)
>
> ![](./media/image42.png)

2.  Observe que os resultados mostram a soma das quantidades pedidas
    agrupadas por produto. O m√©todo **groupBy** agrupa as linhas por
    *Item* e a fun√ß√£o de agrega√ß√£o **Sum** subsequente √© aplicada a
    todas as colunas num√©ricas restantes (neste caso, *Quantidade*).

![](./media/image43.png)

3.  Clique em **+ Code** e copie e cole o c√≥digo abaixo e depois clique
    no bot√£o **Run cell**.

> **CodeCopy**
>
> from pyspark.sql.functions import \*
>
> yearlySales =
> df.select(year("OrderDate").alias("Year")).groupBy("Year").count().orderBy("Year")
>
> display(yearlySales)

![](./media/image44.png)

4.  Observe que os resultados mostram o n√∫mero de pedidos de venda por
    ano. Observe que o m√©todo **select** inclui uma fun√ß√£o SQL **year**
    para extrair o componente year do campo *OrderDate* (motivo pelo
    qual o c√≥digo inclui uma instru√ß√£o **import** para importar fun√ß√µes
    da biblioteca Spark SQL). Em seguida, ele usa um m√©todo **alias**
    para atribuir um nome de coluna ao valor do ano extra√≠do. Os dados
    s√£o ent√£o agrupados pela coluna *Year derivada* e a contagem de
    linhas em cada grupo √© calculada antes que o m√©todo **orderBy** seja
    usado para classificar o dataframe resultante .

![](./media/image45.png)

# Exerc√≠cio 3: Use o Spark para transformar arquivos de dados

Uma tarefa comum para engenheiros de dados √© inserir dados em um formato
ou estrutura espec√≠fica e transform√°-los para processamento ou an√°lise
posterior.

## Tarefa 1: Use m√©todos e fun√ß√µes de dataframe para transformar dados

1.  Clique em + Code e copie e cole o c√≥digo abaixo

**CodeCopy**

> from pyspark.sql.functions import \*
>
> \## Create Year and Month columns
>
> transformed_df = df.withColumn("Year",
> year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))
>
> \# Create the new FirstName and LastName fields
>
> transformed_df = transformed_df.withColumn("FirstName",
> split(col("CustomerName"), " ").getItem(0)).withColumn("LastName",
> split(col("CustomerName"), " ").getItem(1))
>
> \# Filter and reorder columns
>
> transformed_df = transformed_df\["SalesOrderNumber",
> "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName",
> "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"\]
>
> \# Display the first five orders
>
> display(transformed_df.limit(5))

![](./media/image46.png)

2.  **Execute** o c√≥digo para criar um novo dataframe a partir dos dados
    do pedido original com as seguintes transforma√ß√µes:

    - Adicione as colunas **Year** e **Month** com base na coluna
      **OrderDate**.

    - Adicione as colunas **FirstName** e **LastName** com base na
      coluna **CustomerName** .

    - Filtre e reordene as colunas, removendo a coluna **CustomerName**
      .

![](./media/image47.png)

3.  Revise a sa√≠da e verifique se as transforma√ß√µes foram feitas nos
    dados.

![](./media/image48.png)

Voc√™ pode usar todo o poder da biblioteca Spark SQL para transformar os
dados filtrando linhas, derivando, removendo, renomeando colunas e
aplicando quaisquer outras modifica√ß√µes de dados necess√°rias.

**Dica** : consulte a [*Spark dataframe
documentation*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)
para saber mais sobre os m√©todos do objeto Dataframe .

## Tarefa 2: Salvar os dados transformados

1.  **Add a new cell** com o seguinte c√≥digo para salvar o dataframe
    transformado no formato Parquet (substituindo os dados se j√°
    existirem). **Execute** a c√©lula e aguarde a mensagem informando que
    os dados foram salvos.

> CodeCopy
>
> transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')
>
> print ("Transformed data saved!")
>
> **Observa√ß√£o** : Normalmente, o formato *Parquet* √© o preferido para
> arquivos de dados que voc√™ usar√° para an√°lises posteriores ou inser√ß√£o
> em um reposit√≥rio anal√≠tico. O Parquet √© um formato muito eficiente,
> suportado pela maioria dos sistemas de an√°lise de dados em larga
> escala . Na verdade, √†s vezes, sua necessidade de transforma√ß√£o de
> dados pode ser simplesmente converter dados de outro formato (como
> CSV) para Parquet!

![](./media/image49.png)

![](./media/image50.png)

2.  Em seguida, no painel **Lakehouse explorer** √† esquerda, no menu
    **‚Ä¶** do n√≥ **Files** , selecione **Refresh**.

> ![](./media/image51.png)

3.  Clique na pasta **transformed_data** para verificar se cont√©m uma
    nova pasta chamada **orders**, que por sua vez cont√©m um ou mais
    **Parquet files**.

![](./media/image52.png)

4.  Clique em **+ Code** seguindo o c√≥digo para carregar um novo
    dataframe dos arquivos parquet na pasta **transformed_data -\>
    orders** :

> **CodeCopy**
>
> orders_df =
> spark.read.format("parquet").load("Files/transformed_data/orders")
>
> display(orders_df)
>
> ![](./media/image53.png)

5.  **Execute** a c√©lula e verifique se os resultados mostram os dados
    do pedido que foram carregados dos arquivos parquet.

> ![](./media/image54.png)

## Tarefa 3: Salvar dados em arquivos particionados

1.  Adicione uma nova c√©lula. Clique em **+ Code** com o seguinte
    c√≥digo; que salva o dataframe , particionando os dados por **Year**
    e **Month**. **Execute** a c√©lula e aguarde a mensagem de que os
    dados foram salvos.

> CodeCopy
>
> orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")
>
> print ("Transformed data saved!")
>
> ![](./media/image55.png)
>
> ![](./media/image56.png)

2.  Em seguida, no painel **Lakehouse explorer** √† esquerda, no menu
    **‚Ä¶** do n√≥ **Files**, selecione **Refresh.**

![](./media/image57.png)

![](./media/image58.png)

3.  Expanda a pasta **partitioned_orders** para verificar se ela cont√©m
    uma hierarquia de pastas denominada **Year= *xxxx ***, cada uma
    contendo pastas denominadas **Month= *xxxx ***. Cada pasta de m√™s
    cont√©m um arquivo parquet com os pedidos daquele m√™s.

![](./media/image59.png)

![](./media/image60.png)

> Particionar arquivos de dados √© uma maneira comum de otimizar o
> desempenho ao lidar com grandes volumes de dados. Essa t√©cnica pode
> melhorar significativamente o desempenho e facilitar a filtragem de
> dados.

4.  Adicione uma nova c√©lula, clique **+ Code** com o seguinte c√≥digo
    para carregar um novo dataframe do arquivo **orders.parquet**:

> CodeCopy
>
> orders_2021_df =
> spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=\*")
>
> display(orders_2021_df)

![](./media/image61.png)

5.  **Execute** a c√©lula e verifique se os resultados mostram os dados
    do pedido de vendas em 2021. Observe que as colunas de
    particionamento especificadas no caminho (**Year** e **Month**) n√£o
    est√£o inclu√≠das no dataframe.

![](./media/image62.png)

# **Exerc√≠cio 3: Trabalhar com tabelas e SQL**

Como voc√™ viu, os m√©todos nativos do objeto dataframe permitem consultar
e analisar dados de um arquivo com bastante efici√™ncia. No entanto,
muitos analistas de dados se sentem mais confort√°veis trabalhando com
tabelas que podem consultar usando a sintaxe SQL. O Spark fornece um
*metastore* no qual voc√™ pode definir tabelas relacionais. A biblioteca
Spark SQL que fornece o objeto dataframe tamb√©m suporta o uso de
instru√ß√µes SQL para consultar tabelas no metastore. Ao usar esses
recursos do Spark, voc√™ pode combinar a flexibilidade de um data lake
com o esquema de dados estruturados e as consultas baseadas em SQL de um
data warehouse relacional ‚Äì da√≠ o termo "data lakehouse".

## Tarefa 1: Criar uma tabela gerenciada

As tabelas em um metastore do Spark s√£o abstra√ß√µes relacionais sobre
arquivos no data lake. As tabelas podem ser *gerenciadas* (nesse caso,
os arquivos s√£o gerenciados pelo metastore) ou *externas* (nesse caso, a
tabela faz refer√™ncia a um local de arquivo no data lake que voc√™
gerencia independentemente do metastore).

1.  Adicione um novo c√≥digo, clique na c√©lula **+ Code** para o notebook
    e insira o seguinte c√≥digo, que salva o dataframe de dados do pedido
    de vendas como uma tabela chamada **salesorders**:

> CodeCopy
>
> \# Create a new table
>
> df.write.format("delta").saveAsTable("salesorders")
>
> \# Get the table description
>
> spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)

![A screenshot of a computer Description automatically
generated](./media/image63.png)

**Observa√ß√£o** : Vale a pena observar alguns pontos sobre este exemplo.
Primeiro, nenhum caminho expl√≠cito √© fornecido, portanto, os arquivos da
tabela ser√£o gerenciados pelo metastore. Segundo, a tabela √© salva no
formato **delta** . Voc√™ pode criar tabelas com base em v√°rios formatos
de arquivo (incluindo CSV, Parquet, Avro e outros), mas *o Delta Lake* √©
uma tecnologia Spark que adiciona recursos de banco de dados relacional
√†s tabelas, versionamento de linhas e outros recursos √∫teis. Criar
tabelas no formato Delta √© a op√ß√£o preferida para data lakehouses no
Fabric.

2.  **Execute** a c√©lula de c√≥digo e revise a sa√≠da, que descreve a
    defini√ß√£o da nova tabela.

![A screenshot of a computer Description automatically
generated](./media/image64.png)

3.  No painel **Lakehouse** **explorer**, no menu **‚Ä¶** da pasta
    **Tables**, selecione **Refresh.**

![A screenshot of a computer Description automatically
generated](./media/image65.png)

4.  Em seguida, expanda o n√≥ **Tables** e verifique se a tabela
    **salesorders** foi criada.

> ![A screenshot of a computer Description automatically
> generated](./media/image66.png)

5.  Passe o mouse sobre a tabela **salesorders** e clique nas
    retic√™ncias horizontais (...). Navegue e clique em **Load data** e
    selecione **Spark** .

![A screenshot of a computer Description automatically
generated](./media/image67.png)

6.  Clique no bot√£o **‚ñ∑ Run cell**, que usa a biblioteca Spark SQL para
    inserir uma consulta SQL na tabela **salesorder** no c√≥digo PySpark
    e carregar os resultados da consulta em um dataframe.

> CodeCopy
>
> df = spark.sql("SELECT \* FROM \[your_lakehouse\].salesorders LIMIT
> 1000")
>
> display(df)

![A screenshot of a computer program Description automatically
generated](./media/image68.png)

![](./media/image69.png)

## Tarefa 2: Criar uma tabela externa

Voc√™ tamb√©m pode criar tabelas *externas* para as quais os metadados do
esquema s√£o definidos no metastore do lakehouse, mas os arquivos de
dados s√£o armazenados em um local externo.

1.  Abaixo dos resultados retornados pela primeira c√©lula de c√≥digo, use
    o bot√£o **+Code** para adicionar uma nova c√©lula de c√≥digo, caso
    ainda n√£o exista uma. Em seguida, insira o seguinte c√≥digo na nova
    c√©lula.

CodeCopy

df.write.format("delta").saveAsTable("external_salesorder",
path="\<abfs_path\>/external_salesorder")

![A screenshot of a computer Description automatically
generated](./media/image70.png)

2.  No painel **Lakehouse explorer**, no menu **‚Ä¶** da pasta **Files**,
    selecione **Copy ABFS path** no bloco de notas.

> O caminho ABFS √© o caminho totalmente qualificado para a pasta
> **Files** no armazenamento OneLake para seu lakehouse - semelhante a
> este:

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

![A screenshot of a computer Description automatically
generated](./media/image71.png)

3.  Agora, v√° para a c√©lula de c√≥digo e substitua **\<abfs_path\>** pelo
    **path** que voc√™ copiou para o bloco de notas, para que o c√≥digo
    salve o dataframe como uma tabela externa com arquivos de dados em
    uma pasta chamada **external_salesorder** na sua pasta **Files**. O
    caminho completo deve ser semelhante a este:

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

4.  Use o bot√£o **‚ñ∑ (*Run cell*)** √† esquerda da c√©lula para execut√°-la.

![](./media/image72.png)

5.  No painel **Lakehouse explorer**, no menu **‚Ä¶** da pasta **Tables**,
    selecione **Refresh**.

![A screenshot of a computer Description automatically
generated](./media/image73.png)

6.  Em seguida, expanda o n√≥ **Tables** e verifique se a tabela
    **external_salesorder** foi criada.

![A screenshot of a computer Description automatically
generated](./media/image74.png)

7.  No painel **Lakehouse explorer**, no menu **‚Ä¶** da pasta **Files**,
    selecione **Refresh**.

![A screenshot of a computer Description automatically
generated](./media/image75.png)

8.  Em seguida, expanda o n√≥ **Files** e verifique se a pasta
    **external_salesorder** foi criada para os arquivos de dados da
    tabela.

![A screenshot of a computer Description automatically
generated](./media/image76.png)

## Tarefa 3: Comparar tabelas gerenciadas e externas

Vamos explorar as diferen√ßas entre tabelas gerenciadas e externas.

1.  Abaixo dos resultados retornados pela c√©lula de c√≥digo, use o bot√£o
    **+ Code** para adicionar uma nova c√©lula de c√≥digo. Copie o c√≥digo
    abaixo na c√©lula de c√≥digo e use o bot√£o **‚ñ∑ (*Run cell*)** √†
    esquerda da c√©lula para execut√°-lo.

> SqlCopy
>
> %%sql
>
> DESCRIBE FORMATTED salesorders;

![A screenshot of a computer Description automatically
generated](./media/image77.png)

![A screenshot of a computer Description automatically
generated](./media/image78.png)

2.  Nos resultados, visualize a propriedade **Location** da tabela, que
    deve ser um caminho para o armazenamento OneLake para o lakehouse
    terminando em **/Tables/salesorders** (talvez seja necess√°rio
    ampliar a coluna **Data type** para ver o caminho completo).

![A screenshot of a computer Description automatically
generated](./media/image79.png)

3.  Modifique o comando **DESCRIBE** para mostrar os detalhes da tabela
    **external_saleorder**, conforme mostrado aqui.

4.  Abaixo dos resultados retornados pela c√©lula de c√≥digo, use o bot√£o
    **+ Code** para adicionar uma nova c√©lula de c√≥digo. Copie o c√≥digo
    abaixo e use o bot√£o **‚ñ∑ (*Run cell*)** √† esquerda da c√©lula para
    execut√°-lo.

> SqlCopy
>
> %%sql
>
> DESCRIBE FORMATTED external_salesorder;

![A screenshot of a email Description automatically
generated](./media/image80.png)

5.  Nos resultados, visualize a propriedade **Location** da tabela, que
    deve ser um caminho para o armazenamento OneLake para o lakehouse
    terminando em **/Files/external_saleorder** (talvez seja necess√°rio
    ampliar a coluna **Data type** para ver o caminho completo).

![A screenshot of a computer Description automatically
generated](./media/image81.png)

## Tarefa 4: Executar c√≥digo SQL em uma c√©lula

Embora seja √∫til poder inserir instru√ß√µes SQL em uma c√©lula que cont√©m
c√≥digo PySpark, os analistas de dados geralmente querem trabalhar
diretamente em SQL.

1.  Clique em **+ Code** para a c√©lula do notebook e insira o seguinte
    c√≥digo. Clique no bot√£o **‚ñ∑ Run cell** e revise os resultados.
    Observe que:

    - A linha %%sql no in√≠cio da c√©lula (chamada de *magic*) indica que
      o tempo de execu√ß√£o da linguagem Spark SQL deve ser usado para
      executar o c√≥digo nesta c√©lula em vez do PySpark.

    - O c√≥digo SQL faz refer√™ncia √† tabela **salesorders** que voc√™
      criou anteriormente.

    - A sa√≠da da consulta SQL √© exibida automaticamente como resultado
      na c√©lula

> SqlCopy
>
> %%sql
>
> SELECT YEAR(OrderDate) AS OrderYear,
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue
>
> FROM salesorders
>
> GROUP BY YEAR(OrderDate)
>
> ORDER BY OrderYear;

![A screenshot of a computer Description automatically
generated](./media/image82.png)

![](./media/image83.png)

**Observa√ß√£o** : para obter mais informa√ß√µes sobre Spark SQL e
dataframes, consulte a [*Spark SQL
documentation*](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html).

# Exerc√≠cio 4: Visualizar dados com Spark

Uma imagem vale mais que mil palavras, e um gr√°fico costuma ser melhor
do que mil linhas de dados. Embora os notebooks no Fabric incluam uma
visualiza√ß√£o de gr√°fico integrada para dados exibidos a partir de um
dataframe ou consulta Spark SQL, n√£o foram projetados para gr√°ficos
abrangentes. No entanto, voc√™ pode usar bibliotecas gr√°ficas Python,
como **matplotlib** e **seaborn**, para criar gr√°ficos a partir de dados
em dataframes.

## Tarefa 1: Visualizar os resultados como um gr√°fico

1.  Clique em **+ Code** para a c√©lula do notebook e insira o seguinte
    c√≥digo. Clique no bot√£o **‚ñ∑ Run cell** e observe que ele retorna os
    dados da visualiza√ß√£o **salesorders** que voc√™ criou anteriormente.

> SqlCopy
>
> %%sql
>
> SELECT \* FROM salesorders

![A screenshot of a computer Description automatically
generated](./media/image84.png)

![A screenshot of a computer Description automatically
generated](./media/image85.png)

2.  Na se√ß√£o de resultados abaixo da c√©lula, altere a op√ß√£o **View** da
    **Table** para **Chart**.

![A screenshot of a computer Description automatically
generated](./media/image86.png)

3.  Use o bot√£o **View options** no canto superior direito do gr√°fico
    para exibir o painel de op√ß√µes do gr√°fico. Em seguida, defina as
    op√ß√µes a seguir e selecione **Apply**:

    - **Chart type**: Bar chart

    - **Key**: Item

    - **Values**: Quantity

    - **Series Group**: *deixar em branco*

    - **Aggregation**: Sum

    - **Stacked**: *N√£o selecionado*

![A blue barcode on a white background Description automatically
generated](./media/image87.png)

![A screenshot of a graph Description automatically
generated](./media/image88.png)

4.  Verifique se o gr√°fico √© semelhante a este

![A screenshot of a computer Description automatically
generated](./media/image89.png)

## Tarefa 2: Comece a usar o matplotlib

1.  Clique em **+ Code** e copie e cole o c√≥digo abaixo. **Execute** o
    c√≥digo e observe que ele retorna um dataframe do Spark contendo a
    receita anual.

> CodeCopy
>
> sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue \\
>
> FROM salesorders \\
>
> GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\
>
> ORDER BY OrderYear"
>
> df_spark = spark.sql(sqlQuery)
>
> df_spark.show()

![A screenshot of a computer Description automatically
generated](./media/image90.png)

![](./media/image91.png)

2.  Para visualizar os dados como um gr√°fico, come√ßaremos usando a
    biblioteca **matplotlib** Python. Esta biblioteca √© a principal
    biblioteca de plotagem na qual muitas outras se baseiam e oferece
    bastante flexibilidade na cria√ß√£o de gr√°ficos.

3.  Clique em **+ Code** e copie e cole o c√≥digo abaixo.

**CodeCopy**

> from matplotlib import pyplot as plt
>
> \# matplotlib requires a Pandas dataframe, not a Spark one
>
> df_sales = df_spark.toPandas()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\])
>
> \# Display the plot
>
> plt.show()

![A screenshot of a computer Description automatically
generated](./media/image92.png)

4.  Clique no bot√£o **Run cell** e revise os resultados, que consistem
    em um gr√°fico de colunas com a receita bruta total de cada ano.
    Observe as seguintes caracter√≠sticas do c√≥digo usado para gerar este
    gr√°fico:

    - A biblioteca **matplotlib** requer um dataframe *Pandas*, ent√£o
      voc√™ precisa converter o dataframe *Spark* retornado pela consulta
      Spark SQL para este formato.

    - No centro da biblioteca **matplotlib** est√° o objeto **pyplot**.
      Esta √© a base para a maioria das funcionalidades de plotagem.

    - As configura√ß√µes padr√£o resultam em um gr√°fico utiliz√°vel, mas h√°
      um grande espa√ßo para personaliz√°-lo.

![A screenshot of a computer screen Description automatically
generated](./media/image93.png)

5.  Modifique o c√≥digo para plotar o gr√°fico da seguinte forma,
    substitua todo o c√≥digo na **cell** pelo seguinte c√≥digo e clique no
    bot√£o **‚ñ∑ Run cell** e revise a sa√≠da

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure
>
> plt.show()

![A screenshot of a graph Description automatically
generated](./media/image94.png)

6.  O gr√°fico agora inclui um pouco mais de informa√ß√µes. Tecnicamente,
    um gr√°fico est√° contido em uma **Figure**. Nos exemplos anteriores,
    a figura foi criada implicitamente para voc√™; mas voc√™ pode cri√°-la
    explicitamente.

7.  Modifique o c√≥digo para plotar o gr√°fico da seguinte maneira:
    substitua todo o c√≥digo na **cell** pelo c√≥digo a seguir.

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a Figure
>
> fig = plt.figure(figsize=(8,3))
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure
>
> plt.show()

![A screenshot of a computer program Description automatically
generated](./media/image95.png)

8.  **Execute novamente** a c√©lula de c√≥digo e visualize os resultados.
    A figura determina o formato e o tamanho do gr√°fico.

> Uma figura pode conter v√°rios subgr√°ficos, cada um em seu pr√≥prio
> *eixo* .

![A screenshot of a computer Description automatically
generated](./media/image96.png)

9.  Modifique o c√≥digo para plotar o gr√°fico da seguinte forma.
    **Execute novamente** a c√©lula de c√≥digo e visualize os resultados.
    A figura cont√©m os subplots especificados no c√≥digo.

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a figure for 2 subplots (1 row, 2 columns)
>
> fig, ax = plt.subplots(1, 2, figsize = (10,4))
>
> \# Create a bar plot of revenue by year on the first axis
>
> ax\[0\].bar(x=df_sales\['OrderYear'\],
> height=df_sales\['GrossRevenue'\], color='orange')
>
> ax\[0\].set_title('Revenue by Year')
>
> \# Create a pie chart of yearly order counts on the second axis
>
> yearly_counts = df_sales\['OrderYear'\].value_counts()
>
> ax\[1\].pie(yearly_counts)
>
> ax\[1\].set_title('Orders per Year')
>
> ax\[1\].legend(yearly_counts.keys().tolist())
>
> \# Add a title to the Figure
>
> fig.suptitle('Sales Data')
>
> \# Show the figure
>
> plt.show()

![A screenshot of a computer program Description automatically
generated](./media/image97.png)

![](./media/image98.png)

**Observa√ß√£o:** Para saber mais sobre plotagem com matplotlib, consulte
a [*matplotlib documentation*](https://matplotlib.org/).

## Tarefa 3: Use a biblioteca seaborn

Embora o **matplotlib** permita criar gr√°ficos complexos de v√°rios
tipos, ele pode exigir um c√≥digo complexo para obter os melhores
resultados. Por esse motivo, ao longo dos anos, muitas bibliotecas novas
foram criadas com base no matplotlib para abstrair sua complexidade e
aprimorar seus recursos. Uma dessas bibliotecas √© a **Seaborn** .

1.  Clique em **+ Code** e copie e cole o c√≥digo abaixo.

CodeCopy

> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar chart
>
> ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()

![A screenshot of a graph Description automatically
generated](./media/image99.png)

2.  **Execute** o c√≥digo e observe que ele exibe um gr√°fico de barras
    usando a biblioteca seaborn.

![A screenshot of a graph Description automatically
generated](./media/image100.png)

3.  **Modifique** o c√≥digo da seguinte forma. **Execute** o c√≥digo
    modificado e observe que o Seaborn permite que voc√™ defina um tema
    de cores consistente para seus gr√°ficos.

> CodeCopy
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Set the visual theme for seaborn
>
> sns.set_theme(style="whitegrid")
>
> \# Create a bar chart
>
> ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()
>
> ![](./media/image101.png)

4.  **Modifique** o c√≥digo novamente da seguinte forma. **Execute** o
    c√≥digo modificado para visualizar a receita anual como um gr√°fico de
    linhas.

> CodeCopy
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar chart
>
> ax = sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()

![](./media/image102.png)

**Observa√ß√£o:** Para saber mais sobre plotagem com o Seaborn, consulte a
[*seaborn documentation*](https://seaborn.pydata.org/index.html).

## Tarefa 4: Usar tabelas delta para streaming de dados

O Delta Lake suporta streaming de dados. As tabelas Delta podem ser um
*coletor* ou uma *fonte* para fluxos de dados criados usando a API Spark
Structured Streaming. Neste exemplo, voc√™ usar√° uma tabela Delta como
coletor para alguns dados de streaming em um cen√°rio simulado de
internet das coisas (IoT).

1.  Clique em **+ Code** e copie e cole o c√≥digo abaixo e depois clique
    no bot√£o **Run cell**.

CodeCopy

> from notebookutils import mssparkutils
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import \*
>
> \# Create a folder
>
> inputPath = 'Files/data/'
>
> mssparkutils.fs.mkdirs(inputPath)
>
> \# Create a stream that reads data from the folder, using a JSON
> schema
>
> jsonSchema = StructType(\[
>
> StructField("device", StringType(), False),
>
> StructField("status", StringType(), False)
>
> \])
>
> iotstream =
> spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger",
> 1).json(inputPath)
>
> \# Write some event data to the folder
>
> device_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "data.txt", device_data, True)
>
> print("Source stream created...")

![A screenshot of a computer program Description automatically
generated](./media/image103.png)

![A screenshot of a computer Description automatically
generated](./media/image104.png)

2.  Certifique-se de que a mensagem ***Source stream created‚Ä¶*** seja
    impressa. O c√≥digo que voc√™ acabou de executar criou uma fonte de
    dados de streaming com base em uma pasta na qual alguns dados foram
    salvos, representando leituras de dispositivos IoT hipot√©ticos.

3.  Clique em **+ Code** e copie e cole o c√≥digo abaixo e depois clique
    no bot√£o **Run cell**.

CodeCopy

> \# Write the stream to a delta table
>
> delta_stream_table_path = 'Tables/iotdevicedata'
>
> checkpointpath = 'Files/delta/checkpoint'
>
> deltastream =
> iotstream.writeStream.format("delta").option("checkpointLocation",
> checkpointpath).start(delta_stream_table_path)
>
> print("Streaming to delta sink...")

![A screenshot of a computer Description automatically
generated](./media/image105.png)

4.  Este c√≥digo escreve os dados do dispositivo de streaming em formato
    delta em uma pasta chamada **iotdevicedata**. O caminho para o local
    da pasta est√° na pasta **Tables**, uma tabela ser√° criada
    automaticamente para isso. Clique nas retic√™ncias horizontais ao
    lado da tabela e, em seguida, clique em **Refresh**.

![](./media/image106.png)

![A screenshot of a computer Description automatically
generated](./media/image107.png)

5.  Clique em **+ Code** e copie e cole o c√≥digo abaixo e depois clique
    no bot√£o **Run cell**.

> SqlCopy
>
> %%sql
>
> SELECT \* FROM IotDeviceData;

![A screenshot of a computer Description automatically
generated](./media/image108.png)

6.  Este c√≥digo consulta a tabela **IotDeviceData**, que cont√©m os dados
    do dispositivo da fonte de streaming.

7.  Clique em **+ Code** e copie e cole o c√≥digo abaixo e depois clique
    no bot√£o **Run cell**.

> CodeCopy
>
> \# Add more data to the source stream
>
> more_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)

![A screenshot of a computer Description automatically
generated](./media/image109.png)

8.  Este c√≥digo escreve mais dados hipot√©ticos do dispositivo na fonte
    de streaming.

9.  Clique em **+ Code** e copie e cole o c√≥digo abaixo e depois clique
    no bot√£o **Run cell**.

> SqlCopy
>
> %%sql
>
> SELECT \* FROM IotDeviceData;
>
> ![A screenshot of a computer Description automatically
> generated](./media/image110.png)

10. Este c√≥digo consulta a tabela **IotDeviceData** novamente, que agora
    deve incluir os dados adicionais que foram adicionados √† fonte de
    streaming.

11. Clique em **+ Code** e copie e cole o c√≥digo abaixo e depois clique
    no bot√£o **Run cell**.

> CodeCopy
>
> deltastream.stop()

![A screenshot of a computer Description automatically
generated](./media/image111.png)

12. Este c√≥digo interrompe o fluxo.

## Tarefa 5: Salve o notebook e encerre a sess√£o do Spark

Agora que voc√™ terminou de trabalhar com os dados, pode salvar o
notebook com um nome significativo e encerrar a sess√£o do Spark.

1.  Na barra de menu do notebook, use o √çcone ‚öôÔ∏è¬†**Settings** para
    visualizar as configura√ß√µes do notebook.

![A screenshot of a computer Description automatically
generated](./media/image112.png)

2.  Defina o **Name** do notebook como ++**Explore Sales Orders++** e
    feche o painel de configura√ß√µes.

![A screenshot of a computer Description automatically
generated](./media/image113.png)

3.  No menu do notebook, selecione **Stop session** para encerrar a
    sess√£o do Spark.

![A screenshot of a computer Description automatically
generated](./media/image114.png)

![A screenshot of a computer Description automatically
generated](./media/image115.png)

# Exerc√≠cio 5: Criar um fluxo de dados (Gen2) no Microsoft Fabric

No Microsoft Fabric, os Fluxos de Dados (Gen2) se conectam a v√°rias
fontes de dados e realizam transforma√ß√µes no Power Query Online. Eles
podem ser usados em Pipelines de Dados para inserir dados em um
lakehouse ou outro armazenamento anal√≠tico, ou para definir um conjunto
de dados para um relat√≥rio do Power BI.

Este exerc√≠cio foi desenvolvido para apresentar os diferentes elementos
dos Dataflows (Gen2), e n√£o para criar uma solu√ß√£o complexa que possa
existir em uma empresa.

## Tarefa 1: Criar um fluxo de dados (Gen2) para inserir dados

Agora que voc√™ tem um lakehouse, precisa inserir alguns dados nele. Uma
maneira de fazer isso √© definir um fluxo de dados que encapsule um
processo de *extra√ß√£o, transforma√ß√£o e carregamento* (ETL).

1.  Agora, clique em **Fabric_lakehouse** no painel de navega√ß√£o do lado
    esquerdo.

![A screenshot of a computer Description automatically
generated](./media/image116.png)

2.  Na p√°gina inicial do **Fabric_lakehouse**, clique na seta suspensa
    em **Get data** e selecione **New Dataflow Gen2.** O editor do Power
    Query para o seu novo fluxo de dados ser√° aberto.

![](./media/image117.png)

3.  No painel do **Power Query** , na **aba Home**, clique em **Import
    from a Text/CSV file**.

![](./media/image118.png)

4.  No painel **Connect to data source**, em **Connection settings**,
    selecione o bot√£o **Link to file (Preview)**

- **Link to file**:¬†*Selected*

- **File path or
  URL**:¬†<https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/orders.csv>

![](./media/image119.png)

5.  No painel **Connect to data source**, em **Connection credentials,**
    insira os seguintes detalhes e clique no bot√£o **Next**.

    - **Connection**: Create new connection

    - **data gateway**: (none)

    - **Authentication kind**: Organizational account

![](./media/image120.png)

6.  No painel **Preview file data,** clique em **Create** para criar a
    fonte de dados.![A screenshot of a computer Description
    automatically generated](./media/image121.png)

7.  O editor do **Power Query** mostra a fonte de dados e um conjunto
    inicial de etapas de consulta para formatar os dados.

![A screenshot of a computer Description automatically
generated](./media/image122.png)

8.  Na barra de ferramentas, selecione a aba **Add column**. Em seguida,
    selecione **Custom column.**

![A screenshot of a computer Description automatically
generated](./media/image123.png)¬†

9.  Defina o nome da nova coluna como **MonthNo**, defina o tipo de dado
    como **Whole Number** e adicione a seguinte f√≥rmula:
    **Date.Month(\[OrderDate\])** em **Custom column formula**.
    Selecione **OK** .

![A screenshot of a computer Description automatically
generated](./media/image124.png)

10. Observe como a etapa para adicionar a coluna personalizada √©
    adicionada √† consulta. A coluna resultante √© exibida no painel de
    dados.

![A screenshot of a computer Description automatically
generated](./media/image125.png)

**Dica:** No painel Configura√ß√µes da Consulta, √† direita, observe que as
**Applied Steps** incluem cada etapa da transforma√ß√£o. Na parte
inferior, voc√™ tamb√©m pode alternar o bot√£o **Diagram flow** para ativar
o Diagrama Visual das etapas.

As etapas podem ser movidas para cima ou para baixo, editadas
selecionando o √≠cone de engrenagem, e voc√™ pode selecionar cada etapa
para ver as transforma√ß√µes aplicadas no painel de visualiza√ß√£o.

Tarefa 2: Adicionar destino de dados para o Dataflow

1.  Na barra de ferramentas do **Power Query**, selecione a aba
    **Home**. Em seguida, no menu suspenso **Data destination,**
    selecione **Lakehouse** (se ainda n√£o estiver selecionado).

![](./media/image126.png)

![A screenshot of a computer Description automatically
generated](./media/image127.png)

**Observa√ß√£o:** se esta op√ß√£o estiver desativada, talvez voc√™ j√° tenha
um destino de dados definido. Verifique o destino de dados na parte
inferior do painel Configura√ß√µes da consulta, no lado direito do editor
do Power Query. Se um destino j√° estiver definido, voc√™ poder√° alter√°-lo
usando a engrenagem.

2.  Clique no √≠cone **Settings** ao lado da op√ß√£o **Lakehouse**
    selecionada.

![A screenshot of a computer Description automatically
generated](./media/image128.png)

3.  Na caixa de di√°logo **Connect to data destination**, selecione
    **Edit connection.**

![A screenshot of a computer Description automatically
generated](./media/image129.png)

4.  Na caixa de di√°logo **Connect to data destination**, selecione
    **sign in** usando sua conta organizacional do Power BI para definir
    a identidade que o fluxo de dados usa para acessar o lakehouse.

![A screenshot of a computer Description automatically
generated](./media/image130.png)

![A screenshot of a computer Description automatically
generated](./media/image131.png)

5.  Na caixa de di√°logo Connect to data destination, selecione **Next**

![A screenshot of a computer Description automatically
generated](./media/image132.png)

6.  Na caixa de di√°logo Connect to data destination, selecione **New
    table**. Clique na **pasta Lakehouse** , selecione seu workspace ‚Äì
    **dp_FabricXX** e ent√£o selecione seu lakehouse, ou seja,
    **Fabric_lakehouse.** Em seguida, especifique o nome da tabela como
    **orders** e selecione o bot√£o **Next** .

![A screenshot of a computer Description automatically
generated](./media/image133.png)

7.  Na caixa de di√°logo **Choose destination settings**, em **Use
    automatic settings off** e o **Update method,** selecione **Append**
    e clique no bot√£o **Save settings**.

![](./media/image134.png)

8.  O destino **Lakehouse** √© indicado como um **icon** na **query** no
    editor do Power Query.

![A screenshot of a computer Description automatically
generated](./media/image135.png)

![A screenshot of a computer Description automatically
generated](./media/image136.png)

9.  Selecione **Publish** para publicar o fluxo de dados. Em seguida,
    aguarde a cria√ß√£o do fluxo de dados do **Dataflow 1** no seu
    workspace**.**

![A screenshot of a computer Description automatically
generated](./media/image137.png)

10. Ap√≥s a publica√ß√£o, voc√™ pode clicar com o bot√£o direito no fluxo de
    dados do seu workspace, selecionar **Properties** e renomear seu
    fluxo de dados.

![A screenshot of a computer Description automatically
generated](./media/image138.png)

11. Na caixa de di√°logo **Dataflow1**, insira o **Name** como
    **Gen2_Dataflow** e clique no bot√£o **Save**.

![A screenshot of a computer Description automatically
generated](./media/image139.png)

![](./media/image140.png)

## Tarefa 3: Adicionar um fluxo de dados a um pipeline

Voc√™ pode incluir um fluxo de dados como uma atividade em um pipeline.
Pipelines s√£o usados para orquestrar atividades de inser√ß√£o e
processamento de dados, permitindo combinar fluxos de dados com outros
tipos de opera√ß√£o em um √∫nico processo agendado. Pipelines podem ser
criados em algumas experi√™ncias diferentes, incluindo a experi√™ncia do
Data Factory.

1.  Na p√°gina inicial do Synapse Data Engineering, no painel
    **dp_FabricXX**, selecione **+New item** -\> **Data pipeline**

![](./media/image141.png)

2.  Na caixa de di√°logo **New pipeline**, insira **Load data** no campo
    **Name** e clique no bot√£o **Create** para abrir o novo pipeline.

![A screenshot of a computer Description automatically
generated](./media/image142.png)

3.  O editor de pipeline √© aberto.

![A screenshot of a computer Description automatically
generated](./media/image143.png)

> **Dica** : Se o assistente Copiar Dados abrir automaticamente,
> feche-o!

4.  Selecione **Pipeline activity** e adicione uma atividade
    **Dataflow** ao pipeline.

![](./media/image144.png)

5.  Com a nova atividade **Dataflow1** selecionada, na aba **Settings**,
    na lista suspensa **Dataflow ,** selecione **Gen2_Dataflow** (o
    fluxo de dados que voc√™ criou anteriormente)

![](./media/image145.png)

6.  Na aba **Home**, salve o pipeline usando o √≠cone **üñ´ (*Save*)** .

![A screenshot of a computer Description automatically
generated](./media/image146.png)

7.  Use o bot√£o **‚ñ∑ Run** para executar o pipeline e aguarde a
    conclus√£o. Pode levar alguns minutos.

> ![A screenshot of a computer Description automatically
> generated](./media/image147.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image148.png)

![A screenshot of a computer Description automatically
generated](./media/image149.png)

8.  Na barra de menus na borda esquerda, selecione seu workspace, ou
    seja, **dp_FabricXX** .

![A screenshot of a computer Description automatically
generated](./media/image150.png)

9.  No painel **Fabric_lakehouse** , selecione o
    **Gen2_FabricLakehouse** do tipo Lakehouse.

![A screenshot of a computer Description automatically
generated](./media/image151.png)

10. No painel **Explorer,** selecione o menu **‚Ä¶** para **Tables** e
    selecione **refresh**. Em seguida, expanda **Tables** e selecione a
    tabela **orders** criada pelo seu fluxo de dados.

![A screenshot of a computer Description automatically
generated](./media/image152.png)

![](./media/image153.png)

**Dica** : use o *conector de fluxos de dados do Power BI Desktop* para
se conectar diretamente √†s transforma√ß√µes de dados feitas com seu fluxo
de dados.

Voc√™ tamb√©m pode fazer transforma√ß√µes adicionais, publicar como um novo
conjunto de dados e distribuir para o p√∫blico-alvo de conjuntos de dados
especializados.

## Tarefa 4: Limpar recursos

Neste exerc√≠cio, voc√™ aprendeu a usar o Spark para trabalhar com dados
no Microsoft Fabric.

Quando voc√™ terminar de explorar seu lakehouse, poder√° excluir o
workspace que criou para este exerc√≠cio.

1.  Na barra √† esquerda, selecione o √≠cone do seu workspace para
    visualizar todos os itens que ele cont√©m.

> ![A screenshot of a computer Description automatically
> generated](./media/image154.png)

2.  No menu **‚Ä¶** na barra de ferramentas, selecione **Workspace
    settings**.

![](./media/image155.png)

3.  Selecione **General** e clique em **Remove this workspace.**

![A screenshot of a computer settings Description automatically
generated](./media/image156.png)

4.  Na caixa de di√°logo **Delete workspace?,** clique no bot√£o
    **Delete**.

> ![A screenshot of a computer Description automatically
> generated](./media/image157.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image158.png)

**Resumo**

Este caso de uso orienta no processo de trabalho com o Microsoft Fabric
no Power BI. Abrangendo diversas tarefas, incluindo a configura√ß√£o de um
workspace, a cria√ß√£o de um lakehouse, o upload e o gerenciamento de
arquivos de dados e o uso de notebooks para explora√ß√£o de dados. Os
participantes aprender√£o a manipular e transformar dados usando o
PySpark, criar visualiza√ß√µes e salvar e particionar dados para consultas
eficientes.

Neste caso de uso , os participantes realizar√£o uma s√©rie de tarefas
focadas no trabalho com tabelas delta no Microsoft Fabric. As tarefas
abrangem o upload e a explora√ß√£o de dados, a cria√ß√£o de tabelas delta
gerenciadas e externas, a compara√ß√£o de suas propriedades. O laborat√≥rio
apresenta recursos de SQL para o gerenciamento de dados estruturados e
fornece insights sobre visualiza√ß√£o de dados usando bibliotecas Python
como matplotlib e seaborn. Os exerc√≠cios visam fornecer uma compreens√£o
abrangente da utiliza√ß√£o do Microsoft Fabric para an√°lise de dados e da
incorpora√ß√£o de tabelas delta para streaming de dados em um contexto de
IoT.

Este caso de uso orienta voc√™ no processo de configura√ß√£o de um
workspace do Fabric, cria√ß√£o de um data lakehouse e inser√ß√£o de dados
para an√°lise. Ele demonstra como definir um fluxo de dados para lidar
com opera√ß√µes de ETL e configurar destinos de dados para armazenar os
dados transformados. Al√©m disso, voc√™ aprender√° como integrar o fluxo de
dados a um pipeline para processamento automatizado. Por fim, voc√™
receber√° instru√ß√µes para limpar os recursos ap√≥s a conclus√£o do
exerc√≠cio.

Este laborat√≥rio fornece a voc√™ habilidades essenciais para trabalhar
com o Fabric, permitindo que voc√™ crie e gerencie workspaces, estabele√ßa
data lakehouses e execute transforma√ß√µes de dados com efici√™ncia. Ao
incorporar fluxos de dados em pipelines, voc√™ aprender√° a automatizar
tarefas de processamento de dados, otimizando seu fluxo de trabalho e
aumentando a produtividade em cen√°rios reais. As instru√ß√µes de limpeza
garantem que voc√™ n√£o deixe recursos desnecess√°rios, promovendo uma
abordagem organizada e eficiente de gerenciamento de workspaces.
