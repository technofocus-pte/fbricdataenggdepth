**Introduction**

Apache Spark est un moteur open source pour le traitement des donn√©es
distribu√©es, et est largement utilis√© pour explorer, traiter et analyser
d'√©normes volumes de donn√©es dans le stockage de lacs de donn√©es. Spark
est disponible en tant qu'option de traitement dans de nombreux produits
de plateforme de donn√©es, notamment Azure HDInsight, Azure Databricks,
Azure Synapse Analytics et Microsoft Fabric. L'un des avantages de Spark
est la prise en charge d'un large √©ventail de langages de programmation,
notamment Java, Scala, Python et SQL ; ce qui fait de Spark une solution
tr√®s flexible pour les charges de travail de traitement des donn√©es,
notamment le nettoyage et la manipulation des donn√©es, l'analyse
statistique et l'apprentissage automatique, ainsi que l'analyse et la
visualisation des donn√©es.

Les tables d'un lakehouse Microsoft Fabric sont bas√©es sur le format
open source *Delta Lake* pour Apache Spark. Delta Lake ajoute la prise
en charge de la s√©mantique relationnelle pour les op√©rations de donn√©es
par lots et en continu, et permet la cr√©ation d'une architecture
Lakehouse dans laquelle Apache Spark peut √™tre utilis√© pour traiter et
interroger des donn√©es dans des tables bas√©es sur des fichiers
sous-jacents dans un lac de donn√©es.

Dans Microsoft Fabric, les flux de donn√©es (Gen2) se connectent √†
diverses sources de donn√©es et effectuent des transformations dans Power
Query Online. Ils peuvent ensuite √™tre utilis√©s dans les pipelines de
donn√©es pour ing√©rer des donn√©es dans un lakehouse ou un autre magasin
analytique, ou pour d√©finir un jeu de donn√©es pour un rapport Power BI.

Cet atelier est con√ßu pour pr√©senter les diff√©rents √©l√©ments de
Dataflows (Gen2), et non pour cr√©er une solution complexe qui peut
exister dans une entreprise.

**Objectives**:

- Cr√©ez un workspace dans Microsoft Fabric avec la version d'√©valuation
  de Fabric activ√©e.

- √âtablissez un environnement de maison de lac et t√©l√©chargez des
  fichiers de donn√©es pour analyse.

- G√©n√©rez un Notebook pour l'exploration et l'analyse interactives des
  donn√©es.

- Chargez des donn√©es dans une trame de donn√©es pour un traitement et
  une visualisation ult√©rieurs.

- Appliquez des transformations aux donn√©es √† l'aide de PySpark.

- Enregistrez et partitionnez les donn√©es transform√©es pour une
  interrogation optimis√©e.

- Cr√©er une table dans le metastore Spark pour la gestion des donn√©es
  structur√©es

- Enregistrez DataFrame en tant que table delta g√©r√©e nomm√©e ¬´
  salesorders ¬ª.

- Enregistrez DataFrame en tant que table delta externe nomm√©e ¬´
  external_salesorder ¬ª avec un chemin d'acc√®s sp√©cifi√©.

- D√©crivez et comparez les propri√©t√©s des tables manag√©es et externes.

- Ex√©cutez (Run) des requ√™tes SQL sur des tables √† des fins d'analyse et
  de cr√©ation de rapports.

- Visualisez les donn√©es √† l'aide de biblioth√®ques Python telles que
  matplotlib et seaborn.

- √âtablissez un data lakehouse dans l'exp√©rience Data Engineering et
  ing√©rez des donn√©es pertinentes pour une analyse ult√©rieure.

- D√©finissez un flux de donn√©es pour l'extraction, la transformation et
  le chargement des donn√©es dans le lakehouse.

- Configurez les destinations de donn√©es dans Power Query pour stocker
  les donn√©es transform√©es dans le lakehouse.

- Incorporez le flux de donn√©es dans un pipeline pour permettre le
  traitement et l'ingestion planifi√©s des donn√©es.

- Supprimez l'espace de travail (workspace) et les √©l√©ments associ√©s
  pour conclure l'exercice.

# Exercice 1 : Cr√©er un workspace, un lakehouse, un notebook et charger des donn√©es dans une trame de donn√©es 

## T√¢che 1 : Cr√©er un workspace 

Avant d'utiliser des donn√©es dans Fabric, cr√©ez un workspace avec la
version d'essai de Fabric activ√©e.

1.  Ouvrez votre navigateur, acc√©dez √† la barre d'adresse et tapez ou
    collez l'URL suivante : <https://app.fabric.microsoft.com/> puis
    appuyez sur le bouton **Enter**.

> **Remarque** : Si vous √™tes redirig√© vers la page d'accueil de
> Microsoft Fabric, ignorez les √©tapes de \#2 √† \#4.
>
> S![](./media/image1.png)

2.  Dans la fen√™tre **Microsoft Fabric**, entrez vos informations
    d'identification, puis cliquez sur le bouton **Submit**.

> ![](./media/image2.png)

3.  Ensuite, dans la fen√™tre **Microsoft**, entrez le mot de passe et
    cliquez sur le bouton **Sign in.**

> ![Un √©cran de connexion avec une bo√Æte rouge et un texte bleu
> Description g√©n√©r√©e automatiquement](./media/image3.png)

4.  Dans **Stay signed in?**, cliquez sur le bouton **Yes**.

> ![Une capture d'√©cran d'une erreur informatique Description g√©n√©r√©e
> automatiquement](./media/image4.png)

5.  Page d'accueil du Fabric, s√©lectionnez vignette **+New workspace**.

> ![Une capture d'√©cran d'un ordinateur Le contenu g√©n√©r√© par l'IA peut
> √™tre incorrect.](./media/image5.png)

6.  Dans l'onglet **Create a workspace tab**, entrez les d√©tails
    suivants et cliquez sur le bouton **Apply**.

[TABLE]

> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image6.png)
>
> ![](./media/image7.png)

![](./media/image8.png)

![](./media/image9.png)

7.  Attendez la fin du d√©ploiement. Cela prend 2-3 minutes √† compl√©ter.
    Lorsque votre nouvel espace de travail (workspace) s'ouvre, il doit
    √™tre vide.

## T√¢che 2 : Cr√©er une Lakehouse et t√©l√©charger des fichiers

Maintenant que vous disposez d'un workspace, il est temps de passer √†
l'*exp√©rience d'ing√©nierie des donn√©es* dans le portail et de cr√©er un
data lakehouse pour les fichiers de donn√©es que vous allez analyser.

1.  Cr√©ez une nouvelle Eventhouse en cliquant sur le bouton **+New
    item** dans la barre de navigation.

![Une capture d'√©cran d'un navigateur Le contenu g√©n√©r√© par l'IA peut
√™tre incorrect.](./media/image10.png)

2.  Cliquez sur la tuile **¬´¬†Lakehouse**¬†¬ª.

![Une capture d'√©cran d'un ordinateur Le contenu g√©n√©r√© par l'IA peut
√™tre incorrect.](./media/image11.png)

3.  Dans la bo√Æte de dialogue **New lakehouse**, entrez
    **+++Fabric_lakehouse+++** dans le champ **Name**, cliquez sur le
    bouton **Create** et ouvrez le New lakehouse.

![Une capture d'√©cran d'un ordinateur Le contenu g√©n√©r√© par l'IA peut
√™tre incorrect.](./media/image12.png)

4.  Apr√®s environ une minute, une nouvelle maison de lac vide sera
    cr√©√©e. Vous devez ing√©rer des donn√©es dans le data lakehouse √† des
    fins d'analyse.

![](./media/image13.png)

5.  Une notification indiquant **Successfully created SQL endpoint**,
    s'affiche.

![](./media/image14.png)

6.  Dans la section **Explorer**, sous le **fabric_lakehouse**, passez
    votre souris √† c√¥t√© du **Files folder**, puis cliquez sur les points
    de suspension horizontaux **(...)** menu. Naviguez et cliquez sur
    **Upload**, puis cliquez sur le **Upload folder** comme indiqu√© dans
    l'image ci-dessous.

![](./media/image15.png)

7.  Dans le volet **Upload folder** qui s'affiche sur le c√¥t√© droit,
    s√©lectionnez **folder icon** sous **Files/**, puis acc√©dez √†
    **C¬†:\LabFiles**, puis s√©lectionnez le dossier **orders** et cliquez
    sur le bouton **Upload**.

![](./media/image16.png)

8.  Dans le cas, **Upload 3 files to this site?** s'affiche, puis
    cliquez sur le bouton **Upload**.

![](./media/image17.png)

9.  Dans le volet Upload le dossier, cliquez sur le bouton **Upload**.

> ![](./media/image18.png)

10. Une fois les fichiers charg√©s, **close** le volet **Upload folder**.

> ![](./media/image19.png)

11. D√©veloppez **Files**, s√©lectionnez le dossier **orders** et v√©rifiez
    que les fichiers CSV ont √©t√© t√©l√©charg√©s.

![](./media/image20.png)

## T√¢che 3 : Cr√©er un Notebook

Pour utiliser des donn√©es dans Apache Spark, vous pouvez cr√©er un
*Notebook*. Les blocs-notes fournissent un environnement interactif dans
lequel vous pouvez √©crire et ex√©cuter du code (dans plusieurs langues)
et ajouter des notes pour le documenter.

1.  Sur la page **d'accueil (Home)**, lors de l'affichage du contenu du
    dossier **orders** dans votre datalake, dans le menu **Open
    notebook**, s√©lectionnez **New notebook**.

![](./media/image21.png)

2.  Apr√®s quelques secondes, un nouveau Notebook contenant une seule
    *cellule* s'ouvre. Les notebooks sont constitu√©s d'une ou plusieurs
    cellules qui peuvent contenir du *code* ou du *markdown* (texte
    format√©).

![](./media/image22.png)

3.  S√©lectionnez la premi√®re cellule (qui est actuellement une *cellule
    de code*), puis dans la barre d'outils dynamique en haut √† droite,
    utilisez le bouton **M‚Üì** pour **convert the cell to
    a¬†markdown¬†cell**.

![](./media/image23.png)

4.  Lorsque la cellule se transforme en cellule Markdown, le texte
    qu'elle contient est affich√©.

![](./media/image24.png)

5.  Utilisez le **üñâ** bouton (Modifier) pour passer la cellule en mode
    √©dition, remplacez tout le texte puis modifiez la d√©marque comme
    suit :

> CodeCopy
>
> \# Exploration des donn√©es de commande client
>
> Utilisez le code de ce notebook pour explorer les donn√©es de commande
> client.

![](./media/image25.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image26.png)

6.  Cliquez n'importe o√π dans le Notebook en dehors de la cellule pour
    arr√™ter de le modifier et voir le markdown rendu.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image27.png)

## T√¢che 4 : Charger des donn√©es dans une trame de donn√©es

Vous √™tes maintenant pr√™t √† ex√©cuter du code qui charge les donn√©es dans
une *trame de donn√©es*. Les dataframes dans Spark sont similaires aux
dataframes Pandas dans Python et fournissent une structure commune pour
travailler avec les donn√©es dans les lignes et les colonnes.

**Remarque** : Spark prend en charge plusieurs langages de codage,
notamment Scala, Java et autres. Dans cet exercice, nous allons utiliser
*PySpark*, qui est une variante de Python optimis√©e pour Spark. PySpark
est l'un des langages les plus couramment utilis√©s sur Spark et est le
langage par d√©faut dans les notebooks Fabric.

1.  Une fois le Notebook visible, d√©veloppez la liste **Files** et
    s√©lectionnez le dossier **orders** afin que les fichiers CSV soient
    r√©pertori√©s √† c√¥t√© de l'√©diteur de Notebook.

![](./media/image28.png)

2.  Maintenant, passez votre souris pour 2019.csv fichier. Cliquez sur
    les points de suspension horizontaux **(...)** √† c√¥t√© de 2019.csv.
    Naviguez et cliquez sur **Load data**, puis s√©lectionnez **Spark**.
    Une nouvelle cellule de code contenant le code suivant sera ajout√©e
    au carnet :

> CodeCopy
>
> df =
> spark.read.format(¬´¬†csv¬†¬ª).option(¬´¬†en-t√™te¬†¬ª,"true¬†¬ª).load(¬´¬†Fichiers/commandes/2019.csv¬†¬ª)
>
> \# df now est un DataFrame Spark contenant des donn√©es CSV de
> ¬´¬†Files/orders/2019.csv¬†¬ª.
>
> Affichage(DF)

![](./media/image29.png)

![](./media/image30.png)

**Astuce** : Vous pouvez masquer les volets de l'explorateur Lakehouse
sur la gauche en utilisant leurs **¬´** ic√¥nes ¬ª. Faire

Cela vous aidera √† vous concentrer sur le carnet.

3.  Utilisez le bouton **‚ñ∑ Run cell** √† gauche de la cellule pour
    l'ex√©cuter.

![](./media/image31.png)

**Remarque** : Comme c'est la premi√®re fois que vous ex√©cutez (Run( du
code Spark, une session Spark doit √™tre d√©marr√©e. Cela signifie que la
premi√®re ex√©cution de la session peut prendre environ une minute. Les
ex√©cutions suivantes seront plus rapides.

4.  Une fois la commande de cellule termin√©e, examinez le r√©sultat sous
    la cellule, qui doit ressembler √† ceci :

![](./media/image32.png)

5.  La sortie affiche les lignes et les colonnes de donn√©es du fichier
    2019.csv. Cependant, notez que les en-t√™tes de colonne ne sont pas
    corrects. Le code par d√©faut utilis√© pour charger les donn√©es dans
    un dataframe suppose que le fichier CSV inclut les noms de colonne
    dans la premi√®re ligne, mais dans ce cas, le fichier CSV inclut
    uniquement les donn√©es sans informations d'en-t√™te.

6.  Modifiez le code pour d√©finir l'option **header** sur **false**.
    Remplacez tout le code de la **cell** par le code suivant et cliquez
    sur le bouton **‚ñ∑ Run cell** et v√©rifiez la sortie

> CodeCopy
>
> df =
> spark.read.format(¬´¬†csv¬†¬ª).option(¬´¬†en-t√™te¬†¬ª,"false¬†¬ª).load(¬´¬†Fichiers/commandes/2019.csv¬†¬ª)
>
> \# df now est un DataFrame Spark contenant des donn√©es CSV de
> ¬´¬†Files/orders/2019.csv¬†¬ª.
>
> Affichage(DF)
>
> ![](./media/image33.png)

7.  D√©sormais, le dataframe inclut correctement la premi√®re ligne en
    tant que valeurs de donn√©es, mais les noms de colonne sont g√©n√©r√©s
    automatiquement et ne sont pas tr√®s utiles. Pour donner un sens aux
    donn√©es, vous devez d√©finir explicitement le sch√©ma et le type de
    donn√©es corrects pour les valeurs de donn√©es dans le fichier.

8.  Remplacez tout le code de la **cell** par le code suivant et cliquez
    sur le bouton **‚ñ∑ Run cell** et v√©rifiez la sortie

> CodeCopy
>
> √† partir de pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> StructField(¬´¬†SalesOrderNumber¬†¬ª, StringType()),
>
> StructField(¬´¬†SalesOrderLineNumber¬†¬ª, IntegerType()),
>
> StructField(¬´¬†OrderDate¬†¬ª, DateType()),
>
> StructField(¬´¬†CustomerName¬†¬ª, StringType()),
>
> StructField(¬´¬†email¬†¬ª, StringType()),
>
> StructField(¬´¬†√âl√©ment¬†¬ª, StringType()),
>
> StructField(¬´¬†Quantit√©¬†¬ª, IntegerType()),
>
> StructField(¬´¬†UnitPrice¬†¬ª, FloatType()),
>
> StructField(¬´¬†tax¬†¬ª, floatType())
>
> \])
>
> df =
> spark.read.format(¬´¬†csv¬†¬ª).schema(orderSchema).load(¬´¬†Fichiers/commandes/2019.csv¬†¬ª)
>
> Affichage(DF)

![](./media/image34.png)

> ![](./media/image35.png)

9.  D√©sormais, la trame de donn√©es inclut les noms de colonne corrects
    (en plus de **Index**, qui est une colonne int√©gr√©e dans toutes les
    trames de donn√©es en fonction de la position ordinale de chaque
    ligne). Les types de donn√©es des colonnes sont sp√©cifi√©s √† l'aide
    d'un ensemble standard de types d√©finis dans la biblioth√®que SQL
    Spark, qui ont √©t√© import√©s au d√©but de la cellule.

10. V√©rifiez que vos modifications ont √©t√© appliqu√©es aux donn√©es en
    affichant le dataframe.

11. Utilisez l'ic√¥ne de **Code +** sous la sortie de la cellule pour
    ajouter une nouvelle cellule de code au Notebook et entrez-y le code
    suivant. Cliquez sur le bouton **‚ñ∑ Run cell** et v√©rifiez la sortie
    (output)

> CodeCopy
>
> Affichage(DF)
>
> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image36.png)

12. Le dataframe inclut uniquement les donn√©es du fichier **2019.csv**.
    Modifiez le code afin que le chemin d'acc√®s au fichier utilise un
    caract√®re g√©n√©rique \* pour lire les donn√©es de la commande client √†
    partir de tous les fichiers du dossier **orders**

13. Utilisez l‚Äôic√¥ne **+ Code** sous la sortie de la cellule pour
    ajouter une nouvelle cellule de code au Notebook et entrez-y le code
    suivant.

CodeCopy

> √† partir de pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> StructField(¬´¬†SalesOrderNumber¬†¬ª, StringType()),
>
> StructField(¬´¬†SalesOrderLineNumber¬†¬ª, IntegerType()),
>
> StructField(¬´¬†OrderDate¬†¬ª, DateType()),
>
> StructField(¬´¬†CustomerName¬†¬ª, StringType()),
>
> StructField(¬´¬†email¬†¬ª, StringType()),
>
> StructField(¬´¬†√âl√©ment¬†¬ª, StringType()),
>
> StructField(¬´¬†Quantit√©¬†¬ª, IntegerType()),
>
> StructField(¬´¬†UnitPrice¬†¬ª, FloatType()),
>
> StructField(¬´¬†tax¬†¬ª, floatType())
>
> \])
>
> df =
> spark.read.format(¬´¬†csv¬†¬ª).schema(orderSchema).load(¬´¬†Fichiers/commandes/\*.csv¬†¬ª)
>
> Affichage(DF)

![](./media/image37.png)

14. Ex√©cutez (Run) la cellule de code modifi√©e et examinez la sortie,
    qui doit maintenant inclure les ventes de 2019, 2020 et 2021.

![](./media/image38.png)

**Remarque** : Seul un sous-ensemble des lignes est affich√©, il se peut
donc que vous ne puissiez pas voir les exemples de toutes les ann√©es.

# Exercice 2 : Explorer les donn√©es d'une trame de donn√©es

L'objet dataframe comprend un large √©ventail de fonctions que vous
pouvez utiliser pour filtrer, regrouper et manipuler les donn√©es qu'il
contient.

## T√¢che 1 : Filtrer une trame de donn√©es

1.  Utilisez l'ic√¥ne **de code +** sous la sortie de la cellule pour
    ajouter une nouvelle cellule de code au Notebook et entrez-y le code
    suivant.

**CodeCopy**

> customers = df\['NomduClient', 'E-mail'\]
>
> print(clients.count())
>
> print(clients.distinct().count())
>
> display(clients.distinct())
>
> ![](./media/image39.png)

2.  **Ex√©cutez (Run)** la nouvelle cellule de code et examinez les
    r√©sultats. Observez les d√©tails suivants :

    - Lorsque vous effectuez une op√©ration sur une trame de donn√©es, le
      r√©sultat est une nouvelle datatrame (dans ce cas, une nouvelle
      **customers** dataframe clients est cr√©√©e en s√©lectionnant un
      sous-ensemble sp√©cifique de colonnes dans la datatrame **df**)

    - Les trames de donn√©es fournissent des fonctions telles que
      **count** et **distinct** qui peuvent √™tre utilis√©es pour r√©sumer
      et filtrer les donn√©es qu'elles contiennent.

    - La trame de donn√©es\['Field1', 'Field2', ...\] La syntaxe est une
      fa√ßon abr√©g√©e de d√©finir un sous-ensemble de colonnes. Vous pouvez
      √©galement utiliser la m√©thode **select**, de sorte que la premi√®re
      ligne du code ci-dessus puisse √™tre √©crite comme customers =
      df.select(¬´¬†CustomerName¬†¬ª, ¬´¬†Email¬†¬ª)

> ![](./media/image40.png)

3.  Modifiez le code, remplacez tout le code de la **cellule** par le
    code suivant et cliquez sur le bouton **‚ñ∑ Run cell** comme suit :

> CodeCopy
>
> customers = df.select(¬´¬†NomduClient¬†¬ª,
> ¬´¬†E-mail¬†¬ª).where(df\['Item'\]=='Route-250 Rouge, 52')
>
> print(clients.count())
>
> print(clients.distinct().count())
>
> display(clients.distinct())

4.  **Ex√©cutez (Run)** le code modifi√© pour afficher les clients qui ont
    achet√© ***Road-250 Red, 52*¬†product**. Notez que vous pouvez ¬´
    **chain** ¬ª plusieurs fonctions afin que la sortie d'une fonction
    devienne l'entr√©e de la suivante - dans ce cas, la trame de donn√©es
    cr√©√©e par la m√©thode **select** est la trame de donn√©es source de la
    m√©thode **where** utilis√©e pour appliquer des crit√®res de filtrage.

> ![](./media/image41.png)

## T√¢che 2 : Agr√©ger et regrouper des donn√©es dans une trame de donn√©es

1.  Cliquez sur **+ Code** et copiez et collez le code ci-dessous, puis
    cliquez sur le bouton **Run cell**.

CodeCopy

> productSales = df.select(¬´¬†Article¬†¬ª,
> ¬´¬†Quantit√©¬†¬ª).groupBy(¬´¬†Item¬†¬ª).sum()
>
> display(productSales)
>
> ![](./media/image42.png)

2.  Notez que les r√©sultats indiquent la somme des quantit√©s command√©es
    regroup√©es par produit. La m√©thode **groupBy** regroupe les lignes
    par *√©l√©ment*, et la fonction d'agr√©gation **sum** suivante est
    appliqu√©e √† toutes les colonnes num√©riques restantes (dans ce cas,
    *Quantity*)

![](./media/image43.png)

3.  Cliquez sur **+ Code** et copiez et collez le code ci-dessous, puis
    cliquez sur le bouton **Run cell**.

> **CodeCopy**
>
> √† partir de pyspark.sql.functions import \*
>
> yearlySales =
> df.select(year(¬´¬†OrderDate¬†¬ª).alias(¬´¬†Ann√©e¬†¬ª)).groupBy(¬´¬†Ann√©e¬†¬ª).count().orderBy(¬´¬†Year¬†¬ª)
>
> display(annuelVentes)

![](./media/image44.png)

4.  Notez que les r√©sultats indiquent le nombre de commandes client par
    an. Notez que la m√©thode **select** inclut une fonction SQL **year**
    pour extraire le composant year du champ *OrderDate* (c'est pourquoi
    le code inclut une instruction import pour importer des fonctions √†
    partir de la biblioth√®que SQL Spark). Il utilise ensuite une
    **m√©thod d'alias** pour attribuer un nom de colonne √† la valeur de
    l'ann√©e extraite. Les donn√©es sont ensuite regroup√©es par la colonne
    d√©riv√©e *Ann√©e* et le nombre de lignes dans chaque groupe est
    calcul√© avant que la m√©thode **orderBy** ne soit utilis√©e pour trier
    la datatrame r√©sultante.

![](./media/image45.png)

# Exercice 3 : Utiliser Spark pour transformer des fichiers de donn√©es

Une t√¢che courante des ing√©nieurs de donn√©es consiste √† ing√©rer des
donn√©es dans un format ou une structure particuli√®re, et √† les
transformer pour un traitement ou une analyse ult√©rieure en aval.

## T√¢che 1 : Utiliser des m√©thodes et des fonctions de dataframe pour transformer des donn√©es

1.  Cliquez sur + Code et copiez et collez le code ci-dessous

**CodeCopy**

> from pyspark.sql.functions import \*
>
> \## Create Year and Month columns
>
> transformed_df = df.withColumn("Year",
> year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))
>
> \# Create the new FirstName and LastName fields
>
> transformed_df = transformed_df.withColumn("FirstName",
> split(col("CustomerName"), " ").getItem(0)).withColumn("LastName",
> split(col("CustomerName"), " ").getItem(1))
>
> \# Filter and reorder columns
>
> transformed_df = transformed_df\["SalesOrderNumber",
> "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName",
> "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"\]
>
> \# Display the first five orders
>
> display(transformed_df.limit(5))

![](./media/image46.png)

2.  **Ex√©cutez (Run)** le code pour cr√©er un nouveau dataframe √† partir
    des donn√©es de commande d'origine avec les transformations suivantes
    :

    - Ajoutez les colonnes **Year** et **Month** en fonction de la
      colonne **OrderDate**.

    - Ajoutez **FirstName** et **LastName,** en fonction de la colonne
      **CustomerName**.

    - Filtrez et r√©organisez les colonnes, en supprimant la colonne
      **CustomerName**.

![](./media/image47.png)

3.  Examinez la sortie et v√©rifiez que les modifications ont √©t√©
    apport√©es aux donn√©es.

![](./media/image48.png)

Vous pouvez utiliser toute la puissance de la biblioth√®que SQL Spark
pour transformer les donn√©es en filtrant les lignes, en d√©rivant, en
supprimant, en renommant les colonnes et en appliquant toute autre
modification de donn√©es requise.

**Conseil** : Consultez [*Spark dataframe
documentation*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)
pour en savoir plus sur les m√©thodes de l'objet Dataframe.

## T√¢che 2 : Enregistrer les donn√©es transform√©es

1.  **Add a new cell** avec le code suivant pour enregistrer le
    dataframe transform√© au format Parquet (√âcrasement des donn√©es si
    elles existent d√©j√†). **Ex√©cutez (Run)** la cellule et attendez le
    message indiquant que les donn√©es ont √©t√© enregistr√©es.

> CodeCopy
>
> transformed_df.write.mode(¬´¬†√©craser¬†¬ª).parquet('Fichiers/transformed_data/commandes')
>
> print (¬´¬†Donn√©es transform√©es sauvegard√©es !¬†¬ª)
>
> **Remarque** : En g√©n√©ral, le *format Parquet* est pr√©f√©r√© pour les
> fichiers de donn√©es que vous utiliserez pour une analyse plus
> approfondie ou l'ingestion dans un magasin analytique. Parquet est un
> format tr√®s efficace qui est pris en charge par la plupart des
> syst√®mes d'analyse de donn√©es √† grande √©chelle. En fait, il arrive que
> votre besoin de transformation de donn√©es consiste simplement √†
> convertir des donn√©es d'un autre format (tel que CSV) vers Parquet !

![](./media/image49.png)

![](./media/image50.png)

2.  Ensuite, dans le volet de **Lakehouse explorer** √† gauche, dans le
    ... du n≈ìud **Files**, s√©lectionnez **Refresh**.

> ![](./media/image51.png)

3.  Cliquez sur le dossier **transformed_data** pour v√©rifier qu'il
    contient un nouveau dossier nomm√© **orders**, qui contient √† son
    tour un ou plusieurs **Parquet files**.

![](./media/image52.png)

4.  Cliquez sur **+ Code** suivant le code pour charger une nouvelle
    trame de donn√©es √† partir des fichiers parquet dans le dossier
    **transformed_data -\> orders** :

> **CodeCopy**
>
> orders_df =
> spark.read.format(¬´¬†parquet¬†¬ª).load(¬´¬†Fichiers/transformed_data/commandes¬†¬ª)
>
> affichage(orders_df)
>
> ![](./media/image53.png)

5.  **Ex√©cutez (Run)** la cellule et v√©rifiez que les r√©sultats
    affichent les donn√©es de commande qui ont √©t√© charg√©es √† partir des
    fichiers parquet.

> ![](./media/image54.png)

## T√¢che 3 : Enregistrer les donn√©es dans des fichiers partitionn√©s

1.  Ajoutez une nouvelle cellule, cliquez sur **+ Code** avec le code
    suivant, ce qui enregistre le dataframe, en partitionnant les
    donn√©es par **year** et par **Month**. **Ex√©cutez (Run)** la cellule
    et attendez le message indiquant que les donn√©es ont √©t√©
    enregistr√©es.

> CodeCopy
>
> orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")
>
> print ("Transformed data saved!")![](./media/image55.png)
>
> ![](./media/image56.png)

2.  Ensuite, dans le volet **Lakehouse explorer** √† gauche, dans le ...
    du n≈ìud **Files**, s√©lectionnez **Refresh.**

![](./media/image57.png)

![](./media/image58.png)

3.  D√©veloppez le dossier **partitioned_orders** pour v√©rifier qu'il
    contient une hi√©rarchie de dossiers nomm√©s **Year=xxxx**, chacun
    contenant des dossiers nomm√©s **Month=xxxx**. Chaque dossier mensuel
    contient un dossier parquet avec les commandes du mois.

![](./media/image59.png)

![](./media/image60.png)

> Le partitionnement des fichiers de donn√©es est un moyen courant
> d'optimiser les performances lors du traitement de gros volumes de
> donn√©es. Cette technique permet d'am√©liorer consid√©rablement les
> performances et de faciliter le filtrage des donn√©es.

4.  Ajoutez une nouvelle cellule, cliquez sur **+ Code** avec le code
    suivant pour charger un nouveau dataframe √† partir du file
    **orders.parquet** :

> CodeCopy
>
> orders_2021_df =
> spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=\*")
>
> display(orders_2021_df)

![](./media/image61.png)

5.  **Ex√©cutez (Run)** la cellule et v√©rifiez que les r√©sultats
    affichent les donn√©es de commande pour les ventes en 2021. Notez que
    les colonnes de partitionnement sp√©cifi√©es dans le chemin d'acc√®s
    (**(Year and Month)**) ne sont pas incluses dans la trame de
    donn√©es.

![](./media/image62.png)

# **Exercice 3 : Utilisation des tables et de SQL**

Comme vous l'avez vu, les m√©thodes natives de l'objet dataframe vous
permettent d'interroger et d'analyser les donn√©es d'un fichier de
mani√®re assez efficace. Cependant, de nombreux analystes de donn√©es sont
plus √† l'aise avec des tables qu'ils peuvent interroger √† l'aide de la
syntaxe SQL. Spark fournit un *metastore* dans lequel vous pouvez
d√©finir des tables relationnelles. La biblioth√®que SQL Spark qui fournit
l'objet dataframe prend √©galement en charge l'utilisation d'instructions
SQL pour interroger des tables dans le metastore. En utilisant ces
fonctionnalit√©s de Spark, vous pouvez combiner la flexibilit√© d'un lac
de donn√©es avec le sch√©ma de donn√©es structur√©es et les requ√™tes SQL
d'un entrep√¥t de donn√©es relationnel, d'o√π le terme ¬´ data lakehouse ¬ª.

## T√¢che 1 : Cr√©er une table g√©r√©e

Les tables d'un metastore Spark sont des abstractions relationnelles sur
les fichiers du lac de donn√©es. Les tables peuvent √™tre *g√©r√©es* (auquel
cas les fichiers sont g√©r√©s par le metastore) ou *externes* (auquel cas
la table fait r√©f√©rence √† un emplacement de fichier dans le lac de
donn√©es que vous g√©rez ind√©pendamment du metastore).

1.  Ajoutez un nouveau code, cliquez sur la cellule **+ Code** dans le
    carnet et entrez le code suivant, qui enregistre le dataframe des
    donn√©es de commande client sous la forme d'une table nomm√©e
    **salesorders** :

> CodeCopy
>
> \# Create a new table
>
> df.write.format("delta").saveAsTable("salesorders")
>
> \# Get the table description
>
> spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image63.png)

**Remarque** : Il convient de noter deux ou trois choses √† propos de cet
exemple. Tout d'abord, aucun chemin explicite n'est fourni, de sorte que
les fichiers de la table seront g√©r√©s par le metastore. Deuxi√®mement, la
table est enregistr√©e au format **delta**. Vous pouvez cr√©er des tables
bas√©es sur plusieurs formats de fichiers (notamment CSV, Parquet, Avro,
etc.), mais *delta lake* est une technologie Spark qui ajoute des
fonctionnalit√©s de base de donn√©es relationnelle aux tables, y compris
la prise en charge des transactions, la gestion des versions de ligne et
d'autres fonctionnalit√©s utiles. La cr√©ation de tables au format delta
est recommand√©e pour les lakehouses de donn√©es dans Fabric.

2.  **Ex√©cutez (Run)** la cellule de code et examinez la sortie, qui
    d√©crit la d√©finition de la nouvelle table.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image64.png)

3.  Dans le volet **Lakehouse explorer**, dans le ... du dossier
    **Tables**, s√©lectionnez **Refresh.**

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image65.png)

4.  Ensuite, d√©veloppez le n≈ìud **Tables** et v√©rifiez que la table
    **salesorders** a √©t√© cr√©√©e.

> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image66.png)

5.  Passez votre souris √† c√¥t√© du **salesorders**, puis cliquez sur les
    points de suspension horizontaux (...). Naviguez et cliquez sur
    **Load data**, puis s√©lectionnez **Spark**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image67.png)

6.  Cliquez sur le bouton **‚ñ∑ Run cell** et qui utilise la biblioth√®que
    SQL Spark pour int√©grer une requ√™te SQL sur la table de
    **salesorder** dans le code PySpark et charger les r√©sultats de la
    requ√™te dans un dataframe.

> CodeCopy
>
> df = spark.sql(¬´¬†SELECT \* FROM \[your_lakehouse\].salesorders LIMIT
> 1000¬†¬ª)
>
> Affichage(DF)

![Une capture d'√©cran d'un programme informatique Description g√©n√©r√©e
automatiquement](./media/image68.png)

![](./media/image69.png)

## T√¢che 2 : Cr√©er une table externe

Vous pouvez √©galement cr√©er des *tables externes* pour lesquelles les
m√©tadonn√©es de sch√©ma sont d√©finies dans le metastore du lakehouse, mais
les fichiers de donn√©es sont stock√©s dans un emplacement externe.

1.  Sous les r√©sultats renvoy√©s par la premi√®re cellule de code,
    utilisez le bouton **+ Code** pour ajouter une nouvelle cellule de
    code si elle n'en existe pas d√©j√†. Entrez ensuite le code suivant
    dans la nouvelle cellule.

CodeCopy

> df.write.format("delta").saveAsTable("external_salesorder",
> path="\<abfs_path\>/external_salesorder")

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image70.png)

2.  Dans le volet de **Lakehouse explorer**, dans le ... du dossier
    **Files**, s√©lectionnez **Copy ABFS path** dans le Notebook.

> Le chemin ABFS est le chemin d'acc√®s complet au dossier **Files** dans
> le stockage OneLake de votre Lakehouse - similaire √† ceci :

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image71.png)

3.  Maintenant, d√©placez-vous dans la cellule de code,
    remplacez**-\<abfs_path\>** par le **chemin d‚Äôacc√®s (path)** vous
    avez copi√© dans le Notebook afin que le code enregistre le cadre de
    donn√©es en tant que table externe avec des fichiers de donn√©es dans
    un dossier nomm√© **external_salesorder** dans votre emplacement de
    dossier Fichiers. Le chemin d'acc√®s complet doit ressembler √† ceci

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

4.  Utilisez le bouton **‚ñ∑ (*Run cell*)** √† gauche de la cellule pour
    l'ex√©cuter.

![](./media/image72.png)

5.  Dans le volet de **Lakehouse explorer**, dans le ... menu du dossier
    **Tables**, s√©lectionnez l'option **Refresh**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image73.png)

6.  D√©veloppez ensuite le n≈ìud **Tables** et v√©rifiez que la table
    **external_salesorder** a √©t√© cr√©√©e.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image74.png)

7.  Dans le volet de **Lakehouse** **explorer**, dans le ... menu du
    dossier **Files**, s√©lectionnez **Refresh**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image75.png)

8.  D√©veloppez ensuite le n≈ìud **Files** et v√©rifiez que le dossier
    **external_salesorder** a √©t√© cr√©√© pour les fichiers de donn√©es de
    la table.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image76.png)

## T√¢che 3 : Comparer les tables g√©r√©es et externes

Explorons les diff√©rences entre les tables manag√©es et externes.

1.  Sous les r√©sultats renvoy√©s par la cellule de code, utilisez le
    bouton **+ Code** pour ajouter une nouvelle cellule de code. Copiez
    le code ci-dessous dans la cellule Code et utilisez le bouton **‚ñ∑
    (*Run cell*)** √† gauche de la cellule pour l'ex√©cuter.

> SqlCopy
>
> %%sql

DESCRIBE FORMATTED salesorders;

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image77.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image78.png)

2.  Dans les r√©sultats, affichez la propri√©t√© **Location** de la table,
    qui doit √™tre un chemin d'acc√®s au stockage OneLake pour le
    lakehouse se terminant par **/Tables/salesorders** (vous devrez
    peut-√™tre √©largir la colonne **Data Type** pour voir le chemin
    d'acc√®s complet).

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image79.png)

3.  Modifiez la commande **DESCRIBE** pour afficher les d√©tails de la
    table **external_saleorder**, comme illustr√© ici.

4.  Sous les r√©sultats renvoy√©s par la cellule de code, utilisez le
    bouton **+ Code** pour ajouter une nouvelle cellule de code. Copiez
    le code ci-dessous et utilisez le bouton **‚ñ∑ (*Run cell*)** √† gauche
    de la cellule pour l'ex√©cuter.

> SqlCopy
>
> %%sql
>
> DESCRIBE FORMATTED external_salesorder;

![Une capture d'√©cran d'un e-mail Description g√©n√©r√©e
automatiquement](./media/image80.png)

5.  Dans les r√©sultats, affichez la propri√©t√© **Location** de la table,
    qui doit √™tre un chemin d'acc√®s au stockage OneLake pour le
    lakehouse se terminant par **/Files/external_saleorder** (vous
    devrez peut-√™tre √©largir la colonne **Data Type** pour voir le
    chemin d'acc√®s complet).

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image81.png)

## T√¢che 4 : Ex√©cuter du code SQL dans une cellule

Bien qu'il soit utile de pouvoir int√©grer des instructions SQL dans une
cellule contenant du code PySpark, les analystes de donn√©es souhaitent
souvent travailler directement en SQL.

1.  Cliquez sur la cellule **+ Code** du carnet et entrez le code
    suivant dans celui-ci. Cliquez sur **l**e bouton **‚ñ∑ Run cell** et
    examinez les r√©sultats. Observez que :

    - La ligne %%sql au d√©but de la cellule (appel√©e *magie*) indique
      que le runtime du langage SQL Spark doit √™tre utilis√© pour
      ex√©cuter le code dans cette cellule √† la place de PySpark.

    - Le code SQL fait r√©f√©rence √† la table **salesorders** que vous
      avez cr√©√©e pr√©c√©demment.

    - La sortie de la requ√™te SQL s'affiche automatiquement en tant que
      r√©sultat sous la cellule

> SqlCopy
>
> %%sql
>
> SELECT YEAR(OrderDate) AS OrderYear,
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue
>
> FROM salesorders
>
> GROUP BY YEAR(OrderDate)
>
> ORDER BY OrderYear;

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image82.png)

![](./media/image83.png)

**Remarque** : Pour plus d'informations sur Spark SQL et les dataframes,
consultez la [*documentation Spark
SQL*](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html).

# Exercice 4 : Visualiser des donn√©es avec Spark

Une image vaut proverbialement mille mots, et un graphique vaut souvent
mieux qu'un millier de lignes de donn√©es. Bien que les notebooks de
Fabric incluent une vue graphique int√©gr√©e pour les donn√©es affich√©es √†
partir d'une trame de donn√©es ou d'une requ√™te SQL Spark, elle n'est pas
con√ßue pour la cr√©ation de graphiques compl√®te. Cependant, vous pouvez
utiliser des biblioth√®ques graphiques Python telles que **matplotlib**
et **seaborn** pour cr√©er des graphiques √† partir de donn√©es dans des
dataframes.

## T√¢che 1 : Afficher les r√©sultats sous forme de graphique

1.  Cliquez sur la cellule **+ Code** du carnet et entrez le code
    suivant dans celui-ci. Cliquez sur le bouton **‚ñ∑ Run cell** et
    observez qu'il renvoie les donn√©es de la vue **salesorders** que
    vous avez cr√©√©e pr√©c√©demment.

> SqlCopy
>
> %%sql
>
> SELECT \* FROM salesorders

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image84.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image85.png)

2.  Dans la section des r√©sultats sous la cellule, remplacez l‚Äôoption
    **View** de **Table** √† **Chart**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image86.png)

3.  Utilisez le bouton **View options** en haut √† droite du graphique
    pour afficher le volet des options du graphique. D√©finissez ensuite
    les options comme suit et s√©lectionnez **Apply** :

    - **Chart type**: Bar chart

    - **Key**: Item

    - **Values**: Quantity

    - **Series Group**:¬†*leave blank*

    - **Aggregation**: Sum

    - **Stacked**:¬†*Non s√©lectionn√©*

![Un code-barres bleu sur fond blanc Description g√©n√©r√©e
automatiquement](./media/image87.png)

![Une capture d'√©cran d'un graphique Description g√©n√©r√©e
automatiquement](./media/image88.png)

4.  V√©rifiez que le graphique ressemble √† ceci

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image89.png)

## T√¢che 2 : D√©marrer avec matplotlib

1.  Cliquez sur **+ Code** et copiez et collez le code ci-dessous.
    **Ex√©cutez (Run)** le code et observez qu'il retourne une trame de
    donn√©es Spark contenant le chiffre d'affaires annuel.

> CodeCopy
>
> sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue \\
>
> FROM salesorders \\
>
> GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\
>
> ORDER BY OrderYear"
>
> df_spark = spark.sql(sqlQuery)
>
> df_spark.show()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image90.png)

![](./media/image91.png)

2.  Pour visualiser les donn√©es sous forme de graphique, nous allons
    commencer par utiliser la biblioth√®que **Python matplotlib**. Cette
    biblioth√®que est la biblioth√®que de tra√ßage de base sur laquelle de
    nombreuses autres sont bas√©es, et offre une grande flexibilit√© dans
    la cr√©ation de graphiques.

3.  Cliquez sur **+ Code** et copiez et collez le code ci-dessous.

**CodeCopy**

> from matplotlib import pyplot as plt
>
> \# matplotlib requires a Pandas dataframe, not a Spark one
>
> df_sales = df_spark.toPandas()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\])
>
> \# Display the plot
>
> plt.show()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image92.png)

5.  Cliquez sur le bouton **Run cell** et examinez les r√©sultats, qui
    consistent en un histogramme avec le revenu brut total pour chaque
    ann√©e. Notez les caract√©ristiques suivantes du code utilis√© pour
    produire ce graphique :

    - La biblioth√®que **matplotlib** n√©cessite une trame *de donn√©es
      Pandas*, vous devez donc convertir la trame de donn√©es *Spark*
      renvoy√©e par la requ√™te SQL Spark dans ce format.

    - Au c≈ìur de la biblioth√®que **matplotlib** se trouve l'objet
      **pyplot**. C'est la base de la plupart des fonctionnalit√©s de
      tra√ßage.

    - Les param√®tres par d√©faut permettent d'obtenir un graphique
      utilisable, mais il est possible de le personnaliser

![Une capture d'√©cran d'un √©cran d'ordinateur Description g√©n√©r√©e
automatiquement](./media/image93.png)

6.  Modifiez le code pour tracer le graphique comme suit, remplacez tout
    le code de la **cell** par le code suivant et cliquez sur le bouton
    **‚ñ∑ Run cell** et v√©rifiez la sortie

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure  
> plt.show()
>
> ![Une capture d'√©cran d'un graphique Description g√©n√©r√©e
> automatiquement](./media/image94.png)

7.  Le tableau comprend maintenant un peu plus d'informations. Un
    complot est techniquement contenu par une **figure**. Dans les
    exemples pr√©c√©dents, la figure a √©t√© cr√©√©e implicitement pour vous ;
    Mais vous pouvez le cr√©er explicitement.

8.  Modifiez le code pour tracer le graphique comme suit, remplacez tout
    le code de la **cell** par le code suivant.

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a Figure
>
> fig = plt.figure(figsize=(8,3))
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure
>
> plt.show()![Une capture d'√©cran d'un programme informatique
> Description g√©n√©r√©e automatiquement](./media/image95.png)

9.  **Re-run** la cellule de code et affichez les r√©sultats. La figure
    d√©termine la forme et la taille du trac√©.

> Une figure peut contenir plusieurs sous-trac√©s, chacun sur son propre
> *axe*.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image96.png)

10. Modifiez le code pour tracer le graphique comme suit. **Re-run** la
    cellule de code et affichez les r√©sultats. La figure contient les
    sous-parcelles sp√©cifi√©es dans le code.

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a figure for 2 subplots (1 row, 2 columns)
>
> fig, ax = plt.subplots(1, 2, figsize = (10,4))
>
> \# Create a bar plot of revenue by year on the first axis
>
> ax\[0\].bar(x=df_sales\['OrderYear'\],
> height=df_sales\['GrossRevenue'\], color='orange')
>
> ax\[0\].set_title('Revenue by Year')
>
> \# Create a pie chart of yearly order counts on the second axis
>
> yearly_counts = df_sales\['OrderYear'\].value_counts()
>
> ax\[1\].pie(yearly_counts)
>
> ax\[1\].set_title('Orders per Year')
>
> ax\[1\].legend(yearly_counts.keys().tolist())
>
> \# Add a title to the Figure
>
> fig.suptitle('Sales Data')
>
> \# Show the figure
>
> plt.show()

![Une capture d'√©cran d'un programme informatique Description g√©n√©r√©e
automatiquement](./media/image97.png)

![](./media/image98.png)

**Remarque** : Pour en savoir plus sur le tra√ßage avec matplotlib,
consultez la [*documentation de matplotlib*](https://matplotlib.org/).

## T√¢che 3 : Utiliser la biblioth√®que seaborn

Bien que **matplotlib** vous permette de cr√©er des graphiques complexes
de plusieurs types, il peut n√©cessiter un code complexe pour obtenir les
meilleurs r√©sultats. Pour cette raison, au fil des ans, de nombreuses
nouvelles biblioth√®ques ont √©t√© construites sur la base de matplotlib
pour abstraire sa complexit√© et am√©liorer ses capacit√©s. L'une de ces
biblioth√®ques est **seaborn**.

1.  Cliquez sur **+ Code** et copiez et collez le code ci-dessous.

CodeCopy

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a figure for 2 subplots (1 row, 2 columns)
>
> fig, ax = plt.subplots(1, 2, figsize = (10,4))
>
> \# Create a bar plot of revenue by year on the first axis
>
> ax\[0\].bar(x=df_sales\['OrderYear'\],
> height=df_sales\['GrossRevenue'\], color='orange')
>
> ax\[0\].set_title('Revenue by Year')
>
> \# Create a pie chart of yearly order counts on the second axis
>
> yearly_counts = df_sales\['OrderYear'\].value_counts()
>
> ax\[1\].pie(yearly_counts)
>
> ax\[1\].set_title('Orders per Year')
>
> ax\[1\].legend(yearly_counts.keys().tolist())
>
> \# Add a title to the Figure
>
> fig.suptitle('Sales Data')
>
> \# Show the figure
>
> plt.show()

![Une capture d'√©cran d'un graphique Description g√©n√©r√©e
automatiquement](./media/image99.png)

2.  **Ex√©cutez (Run)** le code et observez qu'il affiche un graphique √†
    barres √† l'aide de la biblioth√®que seaborn.

![Une capture d'√©cran d'un graphique Description g√©n√©r√©e
automatiquement](./media/image100.png)

3.  **Modify** le code comme suit. **Ex√©cutez (Run)** le code modifi√© et
    notez que seaborn vous permet de d√©finir un th√®me de couleur
    coh√©rent pour vos parcelles.

> CodeCopy
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Set the visual theme for seaborn
>
> sns.set_theme(style="whitegrid")
>
> \# Create a bar chart
>
> ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()
>
> ![](./media/image101.png)

4.  **Modify** √† nouveau le code comme suit. **Ex√©cutez (Run)** le code
    modifi√© pour afficher le chiffre d'affaires annuel sous forme de
    graphique lin√©aire.

> CodeCopy
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar chart
>
> ax = sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()
>
> ![](./media/image102.png)

**Remarque** : Pour en savoir plus sur le tra√ßage avec seaborn,
consultez la [*documentation
seaborn*](https://seaborn.pydata.org/index.html).

## T√¢che 4 : Utiliser des tables delta pour la diffusion en continu de donn√©es

Le lac Delta prend en charge les donn√©es en continu. Les tables delta
peuvent √™tre un *r√©cepteur* ou une *source* pour les flux de donn√©es
cr√©√©s √† l'aide de l'API Spark Structured Streaming. Dans cet exemple,
vous allez utiliser une table delta comme r√©cepteur pour certaines
donn√©es en streaming dans un sc√©nario IoT (Internet des objets) simul√©.

1.  Cliquez sur **+ Code** et copiez et collez le code ci-dessous, puis
    cliquez sur le bouton **Run cell**.

CodeCopy

> from notebookutils import mssparkutils
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import \*
>
> \# Create a folder
>
> inputPath = 'Files/data/'
>
> mssparkutils.fs.mkdirs(inputPath)
>
> \# Create a stream that reads data from the folder, using a JSON
> schema
>
> jsonSchema = StructType(\[
>
> StructField("device", StringType(), False),
>
> StructField("status", StringType(), False)
>
> \])
>
> iotstream =
> spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger",
> 1).json(inputPath)
>
> \# Write some event data to the folder
>
> device_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "data.txt", device_data, True)

print("Source stream created...")![Une capture d'√©cran d'un programme
informatique Description g√©n√©r√©e automatiquement](./media/image103.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image104.png)

2.  Assurez-vous que le message ***Source stream created‚Ä¶*** est
    imprim√©. Le code que vous venez d'ex√©cuter a cr√©√© une source de
    donn√©es de streaming bas√©e sur un dossier dans lequel certaines
    donn√©es ont √©t√© enregistr√©es, repr√©sentant des lectures d'appareils
    IoT hypoth√©tiques.

3.  Cliquez sur **+ Code** et copiez et collez le code ci-dessous, puis
    cliquez sur le bouton **Run cell**.

CodeCopy

> \# Write the stream to a delta table
>
> delta_stream_table_path = 'Tables/iotdevicedata'
>
> checkpointpath = 'Files/delta/checkpoint'
>
> deltastream =
> iotstream.writeStream.format("delta").option("checkpointLocation",
> checkpointpath).start(delta_stream_table_path)
>
> print("Streaming to delta sink...")
>
> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image105.png)

4.  Ce code √©crit les donn√©es de l'appareil de streaming au format delta
    dans un dossier nomm√© **iotdevicedata**. √âtant donn√© que le chemin
    d'acc√®s √† l'emplacement du dossier se trouve dans le dossier
    **Tables**, une table sera automatiquement cr√©√©e pour celui-ci.
    Cliquez sur les points de suspension horizontaux √† c√¥t√© du tableau,
    puis cliquez sur **Refresh**.

![](./media/image106.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image107.png)

5.  Cliquez sur **+ Code** et copiez et collez le code ci-dessous, puis
    cliquez sur le bouton **Run cell**.

> SqlCopy
>
> %%sql
>
> SELECT \* FROM IotDeviceData;

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image108.png)

6.  Ce code interroge la table **IotDeviceData**, qui contient les
    donn√©es de l'appareil √† partir de la source de streaming.

7.  Cliquez sur **+ Code** et copiez et collez le code ci-dessous, puis
    cliquez sur le bouton **Run cell**.

> CodeCopy
>
> \# Add more data to the source stream
>
> more_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image109.png)

8.  Ce code √©crit d'autres donn√©es d'appareil hypoth√©tiques dans la
    source de streaming.

9.  Cliquez sur **+ Code** et copiez et collez le code ci-dessous, puis
    cliquez sur le bouton **Run cell**.

> SqlCopy
>
> %%sql
>
> SELECT \* FROM IotDeviceData;
>
> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image110.png)

10. Ce code interroge √† nouveau la table **IotDeviceData**, qui doit
    maintenant inclure les donn√©es suppl√©mentaires qui ont √©t√© ajout√©es
    √† la source de streaming.

11. Cliquez sur **+ Code** et copiez et collez le code ci-dessous, puis
    cliquez sur le bouton **Run cell**.

> CodeCopy
>
> deltastream.stop()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image111.png)

12. Ce code arr√™te le flux.

## T√¢che 5 : Enregistrer le Notebook et terminer la session Spark

Maintenant que vous avez termin√© d'utiliser les donn√©es, vous pouvez
enregistrer le Notebook sous un nom significatif et mettre fin √† la
session Spark.

1.  Dans la barre de menus du Notebook, utilisez l'‚öôÔ∏è ic√¥ne **Settings**
    pour afficher les param√®tres du Notebook.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image112.png)

2.  D√©finissez le **Name** du Notebook sur ++ **Explore Sales Orders**
    ++, puis fermez le volet des param√®tres.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image113.png)

3.  Dans le menu du Notebook, s√©lectionnez **Stop session** pour mettre
    fin √† la session Spark.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image114.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image115.png)

# Exercice 5 : Cr√©ation d'un flux de donn√©es (Gen2) dans Microsoft Fabric

Dans Microsoft Fabric, les flux de donn√©es (Gen2) se connectent √†
diverses sources de donn√©es et effectuent des transformations dans Power
Query Online. Ils peuvent ensuite √™tre utilis√©s dans les pipelines de
donn√©es pour ing√©rer des donn√©es dans un lakehouse ou un autre magasin
analytique, ou pour d√©finir un jeu de donn√©es pour un rapport Power BI.

Cet exercice est con√ßu pour pr√©senter les diff√©rents √©l√©ments de
Dataflows (Gen2), et non pour cr√©er une solution complexe qui peut
exister dans une entreprise

## T√¢che 1 : Cr√©er un flux de donn√©es (Gen2) pour ing√©rer des donn√©es

Maintenant que vous avez un Lakehouse, vous devez y ing√©rer des donn√©es.
Pour ce faire, vous devez d√©finir un flux de donn√©es qui encapsule un
processus d'*extraction, de transformation et de chargement* (ETL).

1.  Maintenant, cliquez sur **Fabric_lakehouse** dans le volet de
    navigation de gauche.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image116.png)

2.  Sur la page d'accueil **Fabric_lakehouse**, cliquez sur la fl√®che
    d√©roulante dans **Get data** et s√©lectionnez **New Dataflow Gen2.**
    L'√©diteur Power Query de votre nouveau flux de donn√©es s'ouvre.

![](./media/image117.png)

3.  Dans le **volet Power Query**, sous l'onglet **Home**, cliquez sur
    **Import from a Text/CSV file**

![](./media/image118.png)

4.  Dans le volet **Connect to data source**, sous **Connection
    settings**, s√©lectionnez le bouton de radio **Link to file
    (Preview)**

- **Link to file**¬†: *S√©lectionn√©*

- **File path or URL**:
  <https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/orders.csv>

![](./media/image119.png)

5.  Dans le volet **Connect to data source**, sous **Connection
    credentials,** entrez les d√©tails suivants et cliquez sur le bouton
    **Next**.

    - **Connection**: Create new connection

    - **data gateway**: (none)

    - **Authentication kind**: Organizational account

![](./media/image120.png)

6.  Dans **Preview file data**, cliquez sur **Create** pour cr√©er la
    source de donn√©es. ![Une capture d'√©cran d'un ordinateur Description
    g√©n√©r√©e automatiquement](./media/image121.png)

7.  L'√©diteur **Power Query** affiche la source de donn√©es et un
    ensemble initial d'√©tapes de requ√™te pour mettre en forme les
    donn√©es.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image122.png)

8.  Dans le ruban de la barre d'outils, s√©lectionnez l'onglet **Add
    column**. Ensuite, s√©lectionnez **Custom column.**

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image123.png)¬†

9.  D√©finissez le nom de la nouvelle colonne sur **MonthNo** ,
    d√©finissez le type de donn√©es sur **Whole Number**, puis ajoutez la
    formule suivante :**Date.Month(\[OrderDate\])** sous **Custom column
    formula**. S√©lectionnez **OK**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image124.png)

10. Notez comment l'√©tape d'ajout de la colonne personnalis√©e est
    ajout√©e √† la requ√™te. La colonne r√©sultante s'affiche dans le volet
    de donn√©es.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image125.png)

**Conseil :** Dans le volet Param√®tres de requ√™te sur le c√¥t√© droit,
notez que les **Applied Steps** incluent chaque √©tape de transformation.
En bas, vous pouvez √©galement basculer le bouton **Diagram flow** pour
activer le diagramme visuel des √©tapes.

Les √©tapes peuvent √™tre d√©plac√©es vers le haut ou vers le bas, modifi√©es
en s√©lectionnant l'ic√¥ne d'engrenage, et vous pouvez s√©lectionner chaque
√©tape pour voir les transformations s'appliquer dans le volet d'aper√ßu.

T√¢che 2 : Ajouter une destination de donn√©es pour Dataflow

1.  Dans le ruban de la barre d'outils **Power Query, s√©lectionnez
    l**'onglet **Home**. Ensuite, dans le menu d√©roulant **Data
    destination**, s√©lectionnez **Lakehouse** (si ce n'est pas d√©j√†
    fait).

![](./media/image126.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image127.png)

**Remarque :** Si cette option est gris√©e, il se peut qu'une destination
de donn√©es soit d√©j√† d√©finie. V√©rifiez la destination des donn√©es en bas
du volet Param√®tres de requ√™te sur le c√¥t√© droit de l'√©diteur Power
Query. Si une destination est d√©j√† d√©finie, vous pouvez la modifier √†
l'aide de l'engrenage.

2.  Cliquez sur l‚Äôic√¥ne **Settings** √† c√¥t√© de l'option **Lakehouse**
    s√©lectionn√©e.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image128.png)

3.  Dans la bo√Æte de dialogue **Connect to data destination,
    s√©lectionnez Edit la connection.**

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image129.png)

4.  Dans la bo√Æte de dialogue **Connect to data destination,**
    s√©lectionnez **Sign** √† l'aide de votre compte d'organisation Power
    BI pour d√©finir l'identit√© que le flux de donn√©es utilise pour
    acc√©der au lakehouse.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image130.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image131.png)

5.  Dans la bo√Æte de dialogue Se connecter √† la destination des donn√©es,
    s√©lectionnez **Next**

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image132.png)

6.  Dans la bo√Æte de dialogue Se connecter √† la destination des donn√©es,
    s√©lectionnez **New table**. Cliquez sur le dossier **Lakehouse**,
    s√©lectionnez votre espace de travail (workspace) dp_FabricXX puis
    s√©lectionnez votre lakehouse, c'est-√†-dire **Fabric_lakehouse.**
    Sp√©cifiez ensuite le nom de la table sous forme **orders** et
    s√©lectionnez le bouton **Next**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image133.png)

7.  Dans la bo√Æte de dialogue **Choose destination settings,** sous
    **Use automatic settings off** et **Update method,** s√©lectionnez
    **Apprend**, puis cliquez sur le bouton **Save settings**.

![](./media/image134.png)

8.  La destination **Lakehouse** est indiqu√©e sous forme d‚Äôic√¥ne dans la
    **query** dans l'√©diteur Power Query.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image135.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image136.png)

9.  S√©lectionnez **Publish** pour publier le flux de donn√©es. Attendez
    ensuite que le flux de donn√©es **Dataflow 1** soit cr√©√© dans votre
    espace de travail (workspace).

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image137.png)

10. Une fois publi√©, vous pouvez cliquer avec le bouton droit sur le
    flux de donn√©es dans votre espace de travail (workspace),
    s√©lectionner **Propri√©t√©s** et renommer votre flux de donn√©es.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image138.png)

11. Dans la bo√Æte de dialogue **Dataflow1**, saisissez le **Name** sous
    la forme **Gen2_Dataflow** puis cliquez sur le bouton **Save**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image139.png)

![](./media/image140.png)

## T√¢che 3 : Ajouter un flux de donn√©es √† un pipeline

Vous pouvez inclure un flux de donn√©es en tant qu'activit√© dans un
pipeline. Les pipelines sont utilis√©s pour orchestrer les activit√©s
d'ingestion et de traitement des donn√©es, ce qui vous permet de combiner
des flux de donn√©es avec d'autres types d'op√©rations dans un seul
processus planifi√©. Les pipelines peuvent √™tre cr√©√©s dans diff√©rentes
exp√©riences, y compris l'exp√©rience Data Factory.

1.  Sur la page d'accueil de Synapse Data Engineering, sous **le volet
    dp_FabricXX** , s√©lectionnez **+New item** -\> **Data** **Pipeline**

![](./media/image141.png)

2.  Dans la bo√Æte de dialogue **New pipeline**, entrez **Load data** les
    donn√©es dans le champ **Name**, cliquez sur le bouton Cr√©er pour
    ouvrir le nouveau pipeline.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image142.png)

3.  L'√©diteur de pipeline s'ouvre.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image143.png)

> **Astuce** : Si l'assistant de copie de donn√©es s'ouvre
> automatiquement, fermez-le !

4.  S√©lectionnez **Pipeline activity**, puis ajoutez une activit√© de
    **Dataflow** au pipeline.

![](./media/image144.png)

5.  Une fois la nouvelle activit√© **Dataflow1** s√©lectionn√©e, sous
    l'onglet **Settings**, dans la liste d√©roulante Flux de donn√©es,
    s√©lectionnez **Gen2_Dataflow** (le flux de donn√©es que vous avez
    cr√©√© pr√©c√©demment)

![](./media/image145.png)

6.  Sous l'onglet **Home**, enregistrez le pipeline √† l'aide de l'**üñ´**
    ic√¥ne ***(*Save**).

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image146.png)

7.  Utilisez le bouton **‚ñ∑ Run** pour ex√©cuter le pipeline et attendez
    qu'il se termine. Cela peut prendre quelques minutes.

> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image147.png)
>
> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image148.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image149.png)

8.  Dans la barre de menu sur le bord gauche, s√©lectionnez votre espace
    de travail (workspace), c'est-√†-dire **dp_FabricXX**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image150.png)

9.  Dans le volet **Fabric_lakehouse**, s√©lectionnez le
    **Gen2_FabricLakehouse** de type Lakehouse.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image151.png)

10. Dans le volet **Explorer**, s√©lectionnez l'ic√¥ne **...** pour
    **Tables**, s√©lectionnez **Refresh**. D√©veloppez ensuite **Tables**
    et s√©lectionnez la table **orders** qui a √©t√© cr√©√©e par votre flux
    de donn√©es.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image152.png)

![](./media/image153.png)

**Conseil** : Utilisez le connecteur Power BI Desktop *Dataflows* pour
vous connecter directement aux transformations de donn√©es effectu√©es
avec votre flux de donn√©es.

Vous pouvez √©galement effectuer des transformations suppl√©mentaires,
publier en tant que nouveau jeu de donn√©es et distribuer aupr√®s du
public cible des jeux de donn√©es sp√©cialis√©s.

## T√¢che 4 : Nettoyer les ressources

Dans cet exercice, vous allez apprendre √† utiliser Spark pour utiliser
des donn√©es dans Microsoft Fabric.

Si vous avez termin√© d'explorer votre lakehouse, vous pouvez supprimer
l'espace de travail (workspace) que vous avez cr√©√© pour cet exercice.

1.  Dans la barre de gauche, s√©lectionnez l'ic√¥ne de votre espace de
    travail (workspace) pour afficher tous les √©l√©ments qu'il contient.

> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image154.png)

2.  Dans le **...** dans la barre d'outils, s√©lectionnez **Workspace
    setting**.

![](./media/image155.png)

3.  S√©lectionnez **General** et cliquez sur **Remove this workspace.**

![Une capture d'√©cran des param√®tres d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image156.png)

4.  Dans **Delete workspace?** cliquez sur le bouton **Delete**.

> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image157.png)
>
> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image158.png)

**R√©sum√©**

Ce cas d'utilisation vous guide tout au long du processus d'utilisation
de Microsoft Fabric dans Power BI. Il couvre diverses t√¢ches, notamment
la configuration d'un workspace, la cr√©ation d'un lakehouse, le
t√©l√©chargement et la gestion de fichiers de donn√©es et l'utilisation de
carnets de notes pour l'exploration des donn√©es. Les participants
apprendront √† manipuler et √† transformer des donn√©es √† l'aide de
PySpark, √† cr√©er des visualisations, ainsi qu'√† enregistrer et
partitionner des donn√©es pour des requ√™tes efficaces.

Dans ce cas d'utilisation, les participants s'engageront dans une s√©rie
de t√¢ches ax√©es sur l'utilisation des tables delta dans Microsoft
Fabric. Les t√¢ches comprennent le t√©l√©chargement et l'exploration de
donn√©es, la cr√©ation de tables delta g√©r√©es et externes, la comparaison
de leurs propri√©t√©s, l'introduction de fonctionnalit√©s SQL pour la
gestion des donn√©es structur√©es et la fourniture d'informations sur la
visualisation des donn√©es √† l'aide de biblioth√®ques Python telles que
matplotlib et seaborn. Les exercices visent √† fournir une compr√©hension
compl√®te de l'utilisation de Microsoft Fabric pour l'analyse des donn√©es
et de l'int√©gration de tables delta pour la diffusion de donn√©es en
continu dans un contexte IoT.

Ce cas d'utilisation vous guide tout au long du processus de
configuration d'un workspace Fabric, de cr√©ation d'un data lakehouse et
d'ingestion de donn√©es √† des fins d'analyse. Il montre comment d√©finir
un flux de donn√©es pour g√©rer les op√©rations ETL et configurer les
destinations de donn√©es pour le stockage des donn√©es transform√©es. En
outre, vous apprendrez √† int√©grer le flux de donn√©es dans un pipeline
pour un traitement automatis√©. Enfin, vous recevrez des instructions
pour nettoyer les ressources une fois l'exercice termin√©.

Cet atelier vous permet d'acqu√©rir des comp√©tences essentielles pour
travailler avec Fabric, ce qui vous permet de cr√©er et de g√©rer des
espaces de travail, d'√©tablir des lakehouses de donn√©es et d'effectuer
efficacement des transformations de donn√©es. En int√©grant des flux de
donn√©es dans des pipelines, vous apprendrez √† automatiser les t√¢ches de
traitement des donn√©es, √† rationaliser votre flux de travail et √†
am√©liorer la productivit√© dans des sc√©narios r√©els. Les instructions de
nettoyage vous permettent de ne laisser aucune ressource inutile, ce qui
favorise une approche de gestion de l'espace de travail (workspace)
organis√©e et efficace.
