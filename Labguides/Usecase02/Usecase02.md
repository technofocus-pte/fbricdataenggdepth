**ä»‹ç´¹**

Apache Spark
æ˜¯ä¸€å€‹ç”¨æ–¼åˆ†å¸ƒå¼æ•¸æ“šè™•ç†çš„é–‹æºå¼•æ“ï¼Œå»£æ³›ç”¨æ–¼æ¢ç´¢ã€è™•ç†å’Œåˆ†æ Data Lake
Storage ä¸­çš„å¤§é‡æ•¸æ“šã€‚Spark åœ¨è¨±å¤šæ•¸æ“šå¹³è‡ºç”¢å“ä¸­ä½œç‚ºè™•ç†é¸é …æä¾›ï¼ŒåŒ…æ‹¬
Azure HDInsightã€Azure Databricksã€Azure Synapse Analytics å’Œ Microsoft
Fabricã€‚Spark çš„å¥½è™•ä¹‹ä¸€æ˜¯æ”¯æŒå¤šç¨®ç·¨ç¨‹èªè¨€ï¼ŒåŒ…æ‹¬ Javaã€Scalaã€Python å’Œ
SQL;ä½¿ Spark
æˆç‚ºæ•¸æ“šè™•ç†å·¥ä½œè² è¼‰çš„éå¸¸éˆæ´»çš„è§£æ±ºæ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ•¸æ“šæ¸…ç†å’Œä½œã€çµ±è¨ˆåˆ†æå’Œæ©Ÿå™¨å­¸ç¿’ä»¥åŠæ•¸æ“šåˆ†æå’Œå¯è¦–åŒ–ã€‚

Microsoft Fabric Lakehouse ä¸­çš„è¡¨åŸºæ–¼ Apache Spark çš„é–‹æº *Delta Lake*
æ ¼å¼ã€‚Delta Lake å¢åŠ äº†å°æ‰¹è™•ç†å’Œæµæ•¸æ“šä½œçš„é—œä¿‚èªç¾©çš„æ”¯æŒï¼Œä¸¦æ”¯æŒå‰µå»º
Lakehouse é«”ç³»çµæ§‹ï¼Œåœ¨è©²é«”ç³»çµæ§‹ä¸­ï¼ŒApache Spark
å¯ç”¨æ–¼è™•ç†å’ŒæŸ¥è©¢åŸºæ–¼æ•¸æ“šæ¹–ä¸­åŸºç¤æ–‡ä»¶çš„è¡¨ä¸­çš„æ•¸æ“šã€‚

åœ¨ Microsoft Fabric ä¸­ï¼Œæ•¸æ“šæµ ï¼ˆGen2ï¼‰ é€£æ¥åˆ°å„ç¨®æ•¸æ“šæºï¼Œä¸¦åœ¨ Power
Query Online ä¸­åŸ·è¡Œè½‰æ›ã€‚ç„¶å¾Œï¼Œå¯ä»¥åœ¨ Data Pipelines
ä¸­ä½¿ç”¨å®ƒå€‘å°‡æ•¸æ“šå¼•å…¥ Lakehouse æˆ–å…¶ä»–åˆ†æå­˜å„²ï¼Œæˆ–ç‚º Power BI
å ±è¡¨å®šç¾©æ•¸æ“šé›†ã€‚

æ­¤å¯¦é©—å®¤æ—¨åœ¨ä»‹ç´¹ Dataflows ï¼ˆGen2ï¼‰
çš„ä¸åŒå…ƒç´ ï¼Œè€Œä¸æ˜¯å‰µå»ºä¼æ¥­ä¸­å¯èƒ½å­˜åœ¨çš„è¤‡é›œè§£æ±ºæ–¹æ¡ˆã€‚

**ç›®æ¨™ï¼š**

- åœ¨ Microsoft Fabric ä¸­å‰µå»ºå·¥ä½œå€ï¼Œä¸¦å•Ÿç”¨ Fabric è©¦ç”¨ç‰ˆã€‚

- å»ºç«‹æ¹–å€‰ä¸€é«”ç’°å¢ƒä¸¦ä¸Šå‚³æ•¸æ“šæ–‡ä»¶é€²è¡Œåˆ†æã€‚

- ç”Ÿæˆç”¨æ–¼äº¤äº’å¼æ•¸æ“šæ¢ç´¢å’Œåˆ†æçš„ç­†è¨˜æœ¬ã€‚

- å°‡æ•¸æ“šåŠ è¼‰åˆ° DataFrame ä¸­ï¼Œä»¥ä¾¿é€²ä¸€æ­¥è™•ç†å’Œå¯è¦–åŒ–ã€‚

- ä½¿ç”¨ PySpark å°æ•¸æ“šæ‡‰ç”¨è½‰æ›ã€‚

- ä¿å­˜è½‰æ›å¾Œçš„æ•¸æ“šä¸¦å°å…¶é€²è¡Œåˆ†å€ï¼Œä»¥å¯¦ç¾å„ªåŒ–æŸ¥è©¢ã€‚

- åœ¨ Spark å…ƒå­˜å„²ä¸­å‰µå»ºè¡¨ä»¥é€²è¡Œçµæ§‹åŒ–æ•¸æ“šç®¡ç†

&nbsp;

- å°‡ DataFrame å¦å­˜ç‚ºåç‚º â€œsalesordersâ€ çš„è¨—ç®¡å¢é‡è¡¨ã€‚

- å°‡ DataFrame å¦å­˜ç‚ºå…·æœ‰æŒ‡å®šè·¯å¾‘çš„åç‚º â€œexternal_salesorderâ€ çš„å¤–éƒ¨
  delta è¡¨ã€‚

- æè¿°å’Œæ¯”è¼ƒè¨—ç®¡è¡¨å’Œå¤–éƒ¨è¡¨çš„å±¬æ€§ã€‚

- å°è¡¨åŸ·è¡Œ SQL æŸ¥è©¢ä»¥é€²è¡Œåˆ†æå’Œå ±å‘Šã€‚

- ä½¿ç”¨ Python åº«ï¼ˆå¦‚ matplotlib å’Œ seabornï¼‰å¯è¦–åŒ–æ•¸æ“šã€‚

- åœ¨æ•¸æ“šå·¥ç¨‹é«”é©—ä¸­å»ºç«‹æ•¸æ“šæ¹–å€‰ä¸€é«”ï¼Œä¸¦æ”å–ç›¸é—œæ•¸æ“šä»¥ä¾›å¾ŒçºŒåˆ†æã€‚

- å®šç¾©ç”¨æ–¼æå–ã€è½‰æ›æ•¸æ“šä¸¦å°‡å…¶åŠ è¼‰åˆ° Lakehouse ä¸­çš„æ•¸æ“šæµã€‚

- åœ¨ Power Query ä¸­é…ç½®æ•¸æ“šç›®æ¨™ï¼Œä»¥å°‡è½‰æ›å¾Œçš„æ•¸æ“šå­˜å„²åœ¨ Lakehouse ä¸­ã€‚

- å°‡æ•¸æ“šæµåˆä½µåˆ°ç®¡é“ä¸­ï¼Œä»¥å•Ÿç”¨è¨ˆåŠƒçš„æ•¸æ“šè™•ç†å’Œæ”å–ã€‚

- åˆªé™¤å·¥ä½œå€å’Œé—œè¯çš„å…ƒç´ ä»¥çµæŸç·´ç¿’ã€‚

# ç·´ç¿’ 1ï¼šå‰µå»º workspaceã€Lakehouseã€Notebook ä¸¦å°‡æ•¸æ“šåŠ è¼‰åˆ° DataFrame ä¸­ 

## ä»»å‹™ 1ï¼šå‰µå»ºå·¥ä½œå€ 

åœ¨ Fabric ä¸­è™•ç†æ•¸æ“šä¹‹å‰ï¼Œè«‹å‰µå»ºä¸€å€‹å•Ÿç”¨äº† Fabric è©¦ç”¨ç‰ˆçš„å·¥ä½œå€ã€‚

1.  æ‰“é–‹ç€è¦½å™¨ï¼Œå°èˆªåˆ°åœ°å€æ¬„ï¼Œç„¶å¾Œéµå…¥æˆ–ç²˜è²¼ä»¥ä¸‹
    URLï¼š<https://app.fabric.microsoft.com/> ç„¶å¾ŒæŒ‰ **Enter** æŒ‰éˆ•ã€‚

> **æ³¨æ„**ï¼šå¦‚æœæ‚¨è¢«å®šå‘åˆ° Microsoft Fabric ä¸»é ï¼Œè«‹è·³éå¾ \#2 åˆ° \#4
> çš„æ­¥é©Ÿã€‚
>
> ![](./media/image1.png)

2.  åœ¨ **Microsoft Fabric** çª—å£ä¸­ï¼Œè¼¸å…¥æ‚¨çš„æ†‘æ“šï¼Œç„¶å¾Œå–®æ“Š **Submit**
    æŒ‰éˆ•ã€‚

> ![](./media/image2.png)

3.  ç„¶å¾Œï¼Œåœ¨ **Microsoft** çª—å£ä¸­è¼¸å…¥å¯†ç¢¼ä¸¦å–®æ“Š **Sign in** æŒ‰éˆ•**ã€‚**

> ![A login screen with a red box and blue text Description
> automatically generated](./media/image3.png)

4.  åœ¨ **Stay signed in?** çª—å£ä¸­ï¼Œå–®æ“Š **Yes** æŒ‰éˆ•ã€‚

> ![A screenshot of a computer error Description automatically
> generated](./media/image4.png)

5.  Fabric ä¸»é ï¼Œé¸æ“‡ **+New workspace tile**ã€‚

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image5.png)

6.  åœ¨ **Create a workspace tab** ä¸­ï¼Œè¼¸å…¥ä»¥ä¸‹è©³ç´°ä¿¡æ¯ï¼Œç„¶å¾Œå–®æ“Š
    **Apply** æŒ‰éˆ•ã€‚

[TABLE]

> ![A screenshot of a computer Description automatically
> generated](./media/image6.png)![](./media/image7.png)![](./media/image8.png)![](./media/image9.png)

7.  ç­‰å¾…éƒ¨ç½²å®Œæˆã€‚å®Œæˆéœ€è¦ 2-3 åˆ†é˜ã€‚ç•¶æ‚¨çš„æ–° workspace
    æ‰“é–‹æ™‚ï¼Œå®ƒæ‡‰è©²æ˜¯ç©ºçš„ã€‚

## ä»»å‹™ 2ï¼šå‰µå»º Lakehouse ä¸¦ä¸Šå‚³æ–‡ä»¶

ç¾åœ¨ï¼Œä½ å·²æ“æœ‰å·¥ä½œå€ï¼Œå¯ä»¥åˆ‡æ›åˆ°é–€æˆ¶ä¸­çš„ *Data engineering*
é«”é©—ï¼Œä¸¦ç‚ºè¦åˆ†æçš„æ•¸æ“šæ–‡ä»¶å‰µå»ºæ•¸æ“šæ¹–å€‰ä¸€é«”ã€‚

1.  é€šéå–®æ“Šå°èˆªæ¬„ä¸­çš„ **+New item** æŒ‰éˆ•å‰µå»ºæ–°çš„ Eventhouseã€‚

![A screenshot of a browser AI-generated content may be
incorrect.](./media/image10.png)

2.  é»æ“Šâ€œ**Lakehouseâ€**ç£è²¼ã€‚

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image11.png)

3.  åœ¨ **New lakehouse** å°è©±æ¡†ä¸­ï¼Œ åœ¨ **Name** å­—æ®µä¸­è¼¸å…¥
    **+++Fabric_lakehouse+++Â **ï¼Œå–®æ“Š **Create** æŒ‰éˆ•ä¸¦æ‰“é–‹æ–°çš„
    Lakehouseã€‚

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image12.png)

4.  å¤§ç´„ä¸€åˆ†é˜å¾Œï¼Œå°‡å‰µå»ºä¸€å€‹æ–°çš„ç©º
    Lakehouseã€‚æ‚¨éœ€è¦å°‡ä¸€äº›æ•¸æ“šæå–åˆ°æ•¸æ“šæ¹–å€‰ä¸€é«”ä¸­é€²è¡Œåˆ†æã€‚

![A screenshot of a computer Description automatically
generated](./media/image13.png)

5.  æ‚¨å°‡çœ‹åˆ°ä¸€æ¢é€šçŸ¥ï¼ŒæŒ‡å‡º **Successfully created SQL endpoint**ã€‚

> ![](./media/image14.png)

6.  åœ¨ **Explorer** éƒ¨åˆ†çš„ **fabric_lakehouse** ä¸‹ï¼Œå°‡é¼ æ¨™æ‡¸åœåœ¨ **Files
    folder** æ—é‚Šï¼Œç„¶å¾Œå–®æ“Šæ°´å¹³çœç•¥è™Ÿ **ï¼ˆ...ï¼‰** èœå–®ã€‚å°èˆªä¸¦å–®æ“Š
    **Upload**ï¼Œç„¶å¾Œå–®æ“Š **Upload folder**ï¼Œå¦‚ä¸‹åœ–æ‰€ç¤ºã€‚

![](./media/image15.png)

7.  åœ¨å³å´é¡¯ç¤ºçš„ **Upload folder** çª—æ ¼ä¸­ï¼Œé¸æ“‡ **Files/** ä¸‹çš„ **folder
    icon**ï¼Œç„¶å¾Œç€è¦½åˆ° **Cï¼š\LabFiles**ï¼Œç„¶å¾Œé¸æ“‡ **orders**
    æ–‡ä»¶å¤¾ä¸¦å–®æ“Š Upload æŒ‰éˆ•ã€‚

![](./media/image16.png)

8.  å¦‚æœä¸Šå‚³ **Upload 3 files to this site?** å°è©±æ¡†ï¼Œç„¶å¾Œå–®æ“Š
    **Upload** æŒ‰éˆ•ã€‚

![](./media/image17.png)

9.  åœ¨ Upload folder çª—æ ¼ä¸­ï¼Œå–®æ“Š **Upload** æŒ‰éˆ•ã€‚

> ![](./media/image18.png)

10. ä¸Šå‚³æ–‡ä»¶å¾Œï¼Œ**é—œé–‰ Upload folder** çª—æ ¼ã€‚

> ![](./media/image19.png)

11. å±•é–‹ **Files** ä¸¦é¸æ“‡ **orders** æ–‡ä»¶å¤¾ï¼Œä¸¦é©—è­‰ CSV æ–‡ä»¶æ˜¯å¦å·²ä¸Šå‚³ã€‚

![](./media/image20.png)

## ä»»å‹™ 3ï¼šå‰µå»º notebook

è¦åœ¨ Apache Spark
ä¸­è™•ç†æ•¸æ“šï¼Œæ‚¨å¯ä»¥å‰µå»ºä¸€å€‹*ç­†è¨˜æœ¬*ã€‚ç­†è¨˜æœ¬æä¾›äº†ä¸€å€‹äº¤äº’å¼ç’°å¢ƒï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­ç·¨å¯«å’Œé‹è¡Œä»£ç¢¼ï¼ˆä»¥å¤šç¨®èªè¨€ï¼‰ï¼Œä¸¦æ·»åŠ æ³¨é‡‹ä»¥è¨˜éŒ„ä»£ç¢¼ã€‚

1.  åœ¨ **Home** ä¸Šï¼ŒæŸ¥çœ‹æ•¸æ“šæ¹–ä¸­ **orders** æ–‡ä»¶å¤¾çš„å…§å®¹æ™‚ï¼Œåœ¨ **Open
    notebook** èœå–®ä¸­ï¼Œé¸æ“‡ **New notebook**ã€‚

![](./media/image21.png)

2.  å¹¾ç§’é˜å¾Œï¼Œå°‡æ‰“é–‹ä¸€å€‹åŒ…å«å–®å€‹å–®å…ƒæ ¼çš„æ–°ç­†è¨˜æœ¬
    ã€‚ç­†è¨˜æœ¬ç”±ä¸€å€‹æˆ–å¤šå€‹å–®å…ƒæ ¼çµ„æˆï¼Œé€™äº›å–®å…ƒæ ¼å¯ä»¥åŒ…å«*ä»£ç¢¼*æˆ–
    *Markdown*ï¼ˆæ ¼å¼åŒ–æ–‡æœ¬ï¼‰ã€‚

![](./media/image22.png)

3.  é¸æ“‡ç¬¬ä¸€å€‹å–®å…ƒæ ¼ï¼ˆç•¶å‰ç‚º*ä»£ç¢¼*å–®å…ƒæ ¼ï¼‰ï¼Œç„¶å¾Œåœ¨å…¶å³ä¸Šè§’çš„å‹•æ…‹å·¥å…·æ¬„ä¸­ï¼Œä½¿ç”¨
    **Mâ†“** æŒ‰éˆ• **convert the cell to aÂ markdownÂ cell**ã€‚

![](./media/image23.png)

4.  ç•¶å–®å…ƒæ ¼æ›´æ”¹ç‚º Markdown å–®å…ƒæ ¼æ™‚ï¼Œå°‡å‘ˆç¾å®ƒåŒ…å«çš„æ–‡æœ¬ã€‚

![](./media/image24.png)

5.  **ğŸ–‰** ä½¿ç”¨ ï¼ˆEditï¼‰
    æŒ‰éˆ•å°‡å–®å…ƒæ ¼åˆ‡æ›åˆ°ç·¨è¼¯æ¨¡å¼ï¼Œæ›¿æ›æ‰€æœ‰æ–‡æœ¬ï¼Œç„¶å¾ŒæŒ‰å¦‚ä¸‹æ–¹å¼ä¿®æ”¹
    markdown:

> CodeCopy
>
> \# Sales order data exploration
>
> Use the code in this notebook to explore sales order data.

![](./media/image25.png)

![A screenshot of a computer Description automatically
generated](./media/image26.png)

6.  å–®æ“Šç­†è¨˜æœ¬ä¸­å–®å…ƒæ ¼å¤–éƒ¨çš„ä»»æ„ä½ç½®å¯åœæ­¢ç·¨è¼¯å®ƒä¸¦æŸ¥çœ‹å‘ˆç¾çš„ Markdownã€‚

![A screenshot of a computer Description automatically
generated](./media/image27.png)

## ä»»å‹™ 4ï¼šå°‡æ•¸æ“šåŠ è¼‰åˆ° DataFrame ä¸­

ç¾åœ¨ï¼Œæ‚¨å¯ä»¥é‹è¡Œå°‡æ•¸æ“šåŠ è¼‰åˆ° *DataFrame* ä¸­çš„ä»£ç¢¼ã€‚Spark
ä¸­çš„æ•¸æ“šå¹€é¡ä¼¼æ–¼ Python ä¸­çš„ Pandas
æ•¸æ“šå¹€ï¼Œä¸¦æä¾›ä¸€ç¨®é€šç”¨çµæ§‹ä¾†è™•ç†è¡Œå’Œåˆ—ä¸­çš„æ•¸æ“šã€‚

**æ³¨æ„**ï¼šSpark æ”¯æŒå¤šç¨®ç·¨ç¢¼èªè¨€ï¼ŒåŒ…æ‹¬ Scalaã€Java
ç­‰ã€‚åœ¨æœ¬ç·´ç¿’ä¸­ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ *PySpark*ï¼Œå®ƒæ˜¯ Spark å„ªåŒ–çš„ Python
è®Šé«”ã€‚PySpark æ˜¯ Spark ä¸Šæœ€å¸¸ç”¨çš„èªè¨€ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ Fabric
ç­†è¨˜æœ¬ä¸­çš„é»˜èªèªè¨€ã€‚

1.  åœ¨ç­†è¨˜æœ¬å¯è¦‹çš„æƒ…æ³ä¸‹ï¼Œå±•é–‹ **Files** åˆ—è¡¨ä¸¦é¸æ“‡ **orders**
    æ–‡ä»¶å¤¾ï¼Œä»¥ä¾¿ CSV æ–‡ä»¶åˆ—åœ¨ç­†è¨˜æœ¬ç·¨è¼¯å™¨æ—é‚Šã€‚

![](./media/image28.png)

2.  ç¾åœ¨ï¼Œæ‚¨çš„é¼ æ¨™2019.csvæ–‡ä»¶ã€‚å–®æ“Šæ°´å¹³çœç•¥è™Ÿ **ï¼ˆ...ï¼‰**
    é™¤äº†2019.csvã€‚å°èˆªä¸¦å–®æ“Š **Load data**ï¼Œç„¶å¾Œé¸æ“‡
    **Spark**ã€‚åŒ…å«ä»¥ä¸‹ä»£ç¢¼çš„æ–°ä»£ç¢¼å–®å…ƒæ ¼å°‡æ·»åŠ åˆ°ç­†è¨˜æœ¬ä¸­ï¼š

> CodeCopy
>
> df =
> spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
>
> \# df now is a Spark DataFrame containing CSV data from
> "Files/orders/2019.csv".
>
> display(df)

![](./media/image29.png)

![](./media/image30.png)

**æç¤º**ï¼š æ‚¨å¯ä»¥ä½¿ç”¨å·¦å´çš„ Â« åœ–æ¨™ä¾†éš±è—å·¦å´çš„ Lakehouse Explorer çª—æ ¼
ã€‚è¡Œç‚º

æ‰€ä»¥å°‡å¹«åŠ©æ‚¨å°ˆæ³¨æ–¼ç­†è¨˜æœ¬ã€‚

3.  ä½¿ç”¨å–®å…ƒæ ¼å·¦å´çš„ **â–· Run cell** æŒ‰éˆ•ä¾†é‹è¡Œå®ƒã€‚

![](./media/image31.png)

**æ³¨æ„**ï¼šç”±æ–¼é€™æ˜¯æ‚¨ç¬¬ä¸€æ¬¡é‹è¡Œä»»ä½• Spark ä»£ç¢¼ï¼Œå› æ­¤å¿…é ˆå•Ÿå‹• Spark
æœƒè©±ã€‚é€™æ„å‘³è‘—æœƒè©±ä¸­çš„ç¬¬ä¸€æ¬¡é‹è¡Œå¯èƒ½éœ€è¦ä¸€åˆ†é˜å·¦å³æ‰èƒ½å®Œæˆã€‚å¾ŒçºŒé‹è¡Œæœƒæ›´å¿«ã€‚

4.  cell å‘½ä»¤å®Œæˆå¾Œï¼ŒæŸ¥çœ‹å–®å…ƒæ ¼ä¸‹æ–¹çš„è¼¸å‡ºï¼Œè©²è¼¸å‡ºæ‡‰é¡ä¼¼æ–¼ä»¥ä¸‹å…§å®¹ï¼š

![](./media/image32.png)

5.  è¼¸å‡ºé¡¯ç¤º 2019.csv
    æ–‡ä»¶ä¸­æ•¸æ“šçš„è¡Œå’Œåˆ—ã€‚ä½†æ˜¯ï¼Œè«‹æ³¨æ„ï¼Œåˆ—æ¨™é¡Œçœ‹èµ·ä¾†ä¸æ­£ç¢ºã€‚ç”¨æ–¼å°‡æ•¸æ“šåŠ è¼‰åˆ°
    DataFrame çš„é»˜èªä»£ç¢¼å‡å®š CSV
    æ–‡ä»¶åœ¨ç¬¬ä¸€è¡Œä¸­åŒ…å«åˆ—åï¼Œä½†åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼ŒCSV
    æ–‡ä»¶åƒ…åŒ…å«æ•¸æ“šï¼Œè€Œä¸åŒ…å«æ¨™é¡Œä¿¡æ¯ã€‚

6.  ä¿®æ”¹ä»£ç¢¼ä»¥å°‡ **header** é¸é …è¨­ç½®ç‚º **false**ã€‚å°‡å–®å…ƒæ ¼ä¸­çš„æ‰€æœ‰ä»£ç¢¼
    æ›¿æ›ç‚ºä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Š **â–· Run cell** æŒ‰éˆ•ä¸¦æŸ¥çœ‹è¼¸å‡º

> CodeCopy
>
> df =
> spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
>
> \# df now is a Spark DataFrame containing CSV data from
> "Files/orders/2019.csv".
>
> display(df)

![](./media/image33.png)

7.  ç¾åœ¨ï¼Œdataframe
    æ­£ç¢ºåœ°åŒ…å«ç¬¬ä¸€è¡Œä½œç‚ºæ•¸æ“šå€¼ï¼Œä½†åˆ—åæ˜¯è‡ªå‹•ç”Ÿæˆçš„ï¼Œä¸æ˜¯å¾ˆæœ‰å¹«åŠ©ã€‚è¦ç†è§£æ•¸æ“šï¼Œæ‚¨éœ€è¦ç‚ºæ–‡ä»¶ä¸­çš„æ•¸æ“šå€¼é¡¯å¼å®šç¾©æ­£ç¢ºçš„æ¶æ§‹å’Œæ•¸æ“šé¡å‹ã€‚

8.  å°‡ **cell** ä¸­çš„æ‰€æœ‰ä»£ç¢¼ æ›¿æ›ç‚ºä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Š **â–· Run cell**
    æŒ‰éˆ•ä¸¦æŸ¥çœ‹è¼¸å‡º

> CodeCopy
>
> from pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> StructField("SalesOrderNumber", StringType()),
>
> StructField("SalesOrderLineNumber", IntegerType()),
>
> StructField("OrderDate", DateType()),
>
> StructField("CustomerName", StringType()),
>
> StructField("Email", StringType()),
>
> StructField("Item", StringType()),
>
> StructField("Quantity", IntegerType()),
>
> StructField("UnitPrice", FloatType()),
>
> StructField("Tax", FloatType())
>
> \])
>
> df =
> spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")
>
> display(df)

![](./media/image34.png)

![](./media/image35.png)

9.  ç¾åœ¨ DataFrame åŒ…å«æ­£ç¢ºçš„åˆ—åï¼ˆé™¤äº† **Indexï¼Œ**å®ƒæ˜¯æ‰€æœ‰ DataFrame
    ä¸­çš„å…§ç½®åˆ—ï¼ŒåŸºæ–¼æ¯è¡Œçš„åºè™Ÿä½ç½®ï¼‰ã€‚åˆ—çš„æ•¸æ“šé¡å‹æ˜¯ä½¿ç”¨ Spark SQL
    åº«ä¸­å®šç¾©çš„ä¸€çµ„æ¨™æº–é¡å‹æŒ‡å®šçš„ï¼Œé€™äº›é¡å‹æ˜¯åœ¨å–®å…ƒæ ¼çš„é–‹é ­å°å…¥çš„ã€‚

10. é€šéæŸ¥çœ‹æ•¸æ“šå¹€ç¢ºèªæ‚¨çš„æ›´æ”¹å·²æ‡‰ç”¨æ–¼æ•¸æ“šã€‚

11. ä½¿ç”¨ å–®å…ƒæ ¼è¼¸å‡ºä¸‹æ–¹çš„ **+ Code**
    åœ–æ¨™å°‡æ–°çš„ä»£ç¢¼å–®å…ƒæ ¼æ·»åŠ åˆ°ç­†è¨˜æœ¬ä¸­ï¼Œç„¶å¾Œåœ¨å…¶ä¸­è¼¸å…¥ä»¥ä¸‹ä»£ç¢¼ã€‚é»æ“Š **â–·
    Run cell** æŒ‰éˆ•ä¸¦æŸ¥çœ‹è¼¸å‡º

> CodeCopy
>
> display(df)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image36.png)

12. æ•¸æ“šå¹€åƒ…åŒ…å« **2019.csv** æ–‡ä»¶ä¸­çš„æ•¸æ“šã€‚ä¿®æ”¹ä»£ç¢¼ï¼Œä»¥ä¾¿æ–‡ä»¶è·¯å¾‘ä½¿ç”¨
    \* é€šé…ç¬¦å¾ orders æ–‡ä»¶å¤¾ä¸­çš„æ‰€æœ‰æ–‡ä»¶ä¸­è®€å–éŠ·å”®è¨‚å–®æ•¸æ“š

13. ä½¿ç”¨ å–®å…ƒæ ¼è¼¸å‡ºä¸‹æ–¹çš„ **+ Code**
    åœ–æ¨™å°‡æ–°çš„ä»£ç¢¼å–®å…ƒæ ¼æ·»åŠ åˆ°ç­†è¨˜æœ¬ä¸­ï¼Œç„¶å¾Œåœ¨å…¶ä¸­è¼¸å…¥ä»¥ä¸‹ä»£ç¢¼ã€‚

CodeCopy

> from pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> Â  Â  StructField("SalesOrderNumber", StringType()),
>
> Â  Â  StructField("SalesOrderLineNumber", IntegerType()),
>
> Â  Â  StructField("OrderDate", DateType()),
>
> Â  Â  StructField("CustomerName", StringType()),
>
> Â  Â  StructField("Email", StringType()),
>
> Â  Â  StructField("Item", StringType()),
>
> Â  Â  StructField("Quantity", IntegerType()),
>
> Â  Â  StructField("UnitPrice", FloatType()),
>
> Â  Â  StructField("Tax", FloatType())
>
> Â  Â  \])
>
> df =
> spark.read.format("csv").schema(orderSchema).load("Files/orders/\*.csv")
>
> display(df)

![](./media/image37.png)

14. é‹è¡Œä¿®æ”¹å¾Œçš„ä»£ç¢¼å–®å…ƒä¸¦æŸ¥çœ‹è¼¸å‡ºï¼Œç¾åœ¨æ‡‰åŒ…æ‹¬ 2019 å¹´ã€2020 å¹´å’Œ 2021
    å¹´çš„éŠ·å”®é¡ã€‚

![](./media/image38.png)

**æ³¨æ„**ï¼šåƒ…é¡¯ç¤ºè¡Œçš„å­é›†ï¼Œå› æ­¤æ‚¨å¯èƒ½ç„¡æ³•çœ‹åˆ°æ‰€æœ‰å¹´ä»½çš„ç¤ºä¾‹ã€‚

# ç·´ç¿’ 2ï¼šç€è¦½ dataframe ä¸­çš„æ•¸æ“š

dataframe
å°è±¡åŒ…å«å„ç¨®å‡½æ•¸ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨é€™äº›å‡½æ•¸ä¾†ç¯©é¸ã€åˆ†çµ„å’Œä»¥å…¶ä»–æ–¹å¼ä½œå®ƒæ‰€åŒ…å«çš„æ•¸æ“šã€‚

## ä»»å‹™ 1ï¼šç¯©é¸ dataframe

1.  ä½¿ç”¨ å–®å…ƒæ ¼è¼¸å‡ºä¸‹æ–¹çš„ **+ Code**
    åœ–æ¨™å°‡æ–°çš„ä»£ç¢¼å–®å…ƒæ ¼æ·»åŠ åˆ°ç­†è¨˜æœ¬ä¸­ï¼Œç„¶å¾Œåœ¨å…¶ä¸­è¼¸å…¥ä»¥ä¸‹ä»£ç¢¼ã€‚

**CodeCopy**

> customers = df\['CustomerName', 'Email'\]
>
> print(customers.count())
>
> print(customers.distinct().count())
>
> display(customers.distinct())
>
> ![](./media/image39.png)

2.  **Run** æ–°çš„ä»£ç¢¼å–®å…ƒï¼Œä¸¦æŸ¥çœ‹çµæœã€‚è«‹æ³¨æ„ä»¥ä¸‹è©³ç´°ä¿¡æ¯:

    - ç•¶æ‚¨å° DataFrame åŸ·è¡Œä½œæ™‚ï¼Œçµæœæ˜¯ä¸€å€‹æ–°çš„
      DataFrameï¼ˆåœ¨æœ¬ä¾‹ä¸­ï¼Œé€šéå¾ **df** DataFrame
      ä¸­é¸æ“‡ç‰¹å®šçš„åˆ—å­é›†ä¾†å‰µå»ºæ–°çš„ **customers** DataFrameï¼‰

    &nbsp;

    - Dataframes æä¾› **count** å’Œ **distinct**
      ç­‰å‡½æ•¸ï¼Œå¯ç”¨æ–¼åŒ¯ç¸½å’Œç¯©é¸å®ƒå€‘åŒ…å«çš„æ•¸æ“šã€‚

    &nbsp;

    - dataframe\['Field1'ï¼Œ 'Field2'ï¼Œ ...\]
      èªæ³•æ˜¯å®šç¾©åˆ—å­é›†çš„ç°¡å¯«æ–¹æ³•ã€‚æ‚¨é‚„å¯ä»¥ä½¿ç”¨ **select**
      æ–¹æ³•ï¼Œå› æ­¤ä¸Šé¢ä»£ç¢¼çš„ç¬¬ä¸€è¡Œå¯ä»¥å¯«æˆ customers =
      df.selectï¼ˆâ€œCustomerNameâ€ï¼Œ â€œEmailâ€ï¼‰

> ![](./media/image40.png)

3.  ä¿®æ”¹ä»£ç¢¼ï¼Œå°‡ **cell** ä¸­çš„æ‰€æœ‰ä»£ç¢¼æ›¿æ›ç‚º ä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Š **â–· Run
    cell** æŒ‰éˆ•ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

> CodeCopy
>
> customers = df.select("CustomerName",
> "Email").where(df\['Item'\]=='Road-250 Red, 52')
>
> print(customers.count())
>
> print(customers.distinct().count())
>
> display(customers.distinct())

4.  **Runé‹è¡Œ**ä¿®æ”¹å¾Œçš„ä»£ç¢¼ä»¥æŸ¥çœ‹å·²è³¼è²· ***Road-250 Redï¼Œ 52* product**
    çš„å®¢æˆ¶ã€‚è«‹æ³¨æ„ï¼Œæ‚¨å¯ä»¥å°‡å¤šå€‹å‡½æ•¸ **â€œchainâ€**
    åœ¨ä¸€èµ·ï¼Œä»¥ä¾¿ä¸€å€‹å‡½æ•¸çš„è¼¸å‡ºæˆç‚ºä¸‹ä¸€å€‹å‡½æ•¸çš„è¼¸å…¥ -
    åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œ**select** æ–¹æ³•å‰µå»ºçš„æ•¸æ“šå¹€æ˜¯ç”¨æ–¼æ‡‰ç”¨ç¯©é¸æ¢ä»¶çš„
    **where** æ–¹æ³•çš„æºæ•¸æ“šå¹€ã€‚

> ![](./media/image41.png)

## ä»»å‹™ 2ï¼šåœ¨ dataframe ä¸­èšåˆå’Œåˆ†çµ„æ•¸æ“š

1.  å–®æ“Š **+ Code** **+** ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Š **Run cell**
    æŒ‰éˆ•ã€‚

CodeCopy

> productSales = df.select("Item", "Quantity").groupBy("Item").sum()
>
> display(productSales)
>
> ![](./media/image42.png)

2.  è«‹æ³¨æ„ï¼Œçµæœé¡¯ç¤ºæŒ‰ç”¢å“åˆ†çµ„çš„è¨‚å–®æ•¸é‡ç¸½å’Œã€‚**groupBy** æ–¹æ³•æŒ‰ Item
    å°è¡Œé€²è¡Œåˆ†çµ„ï¼Œéš¨å¾Œçš„ **sum**
    èšåˆå‡½æ•¸å°‡æ‡‰ç”¨æ–¼æ‰€æœ‰å‰©é¤˜çš„æ•¸å­—åˆ—ï¼ˆåœ¨æœ¬ä¾‹ä¸­ç‚º *Quantity*ï¼‰

![](./media/image43.png)

3.  å–®æ“Š **+ Code** ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Š **Run cell** æŒ‰éˆ•ã€‚

> **CodeCopy**
>
> from pyspark.sql.functions import \*
>
> yearlySales =
> df.select(year("OrderDate").alias("Year")).groupBy("Year").count().orderBy("Year")
>
> display(yearlySales)

![](./media/image44.png)

4.  è«‹æ³¨æ„ï¼Œçµæœé¡¯ç¤ºæ¯å¹´çš„éŠ·å”®è¨‚å–®æ•¸é‡ã€‚è«‹æ³¨æ„ï¼Œ**select** æ–¹æ³•åŒ…æ‹¬ä¸€å€‹
    SQL **year** å‡½æ•¸ï¼Œç”¨æ–¼æå– *OrderDate*
    å­—æ®µçš„å¹´ä»½éƒ¨åˆ†ï¼ˆé€™å°±æ˜¯ä»£ç¢¼åŒ…å«ä¸€å€‹ **import** èªå¥ä»¥å¾ Spark SQL
    åº«å°å…¥å‡½æ•¸çš„åŸå› ï¼‰ã€‚ç„¶å¾Œï¼Œå®ƒä½¿ç”¨**Â alias**
    æ–¹æ³•ç‚ºæå–çš„å¹´ä»½å€¼åˆ†é…åˆ—åç¨±ã€‚ç„¶å¾Œï¼ŒæŒ‰æ´¾ç”Ÿçš„ Year åˆ—å°æ•¸æ“šé€²è¡Œåˆ†çµ„
    ï¼Œä¸¦è¨ˆç®—æ¯å€‹çµ„ä¸­çš„è¡Œæ•¸ï¼Œæœ€å¾Œä½¿ç”¨ **orderBy**
    æ–¹æ³•å°ç”Ÿæˆçš„æ•¸æ“šå¹€é€²è¡Œæ’åºã€‚

![](./media/image45.png)

# ç·´ç¿’ 3ï¼šä½¿ç”¨ Spark è½‰æ›æ•¸æ“šæ–‡ä»¶

æ•¸æ“šå·¥ç¨‹å¸«çš„ä¸€é …å¸¸è¦‹ä»»å‹™æ˜¯ä»¥ç‰¹å®šæ ¼å¼æˆ–çµæ§‹æ”å–æ•¸æ“šï¼Œä¸¦å°å…¶é€²è¡Œè½‰æ›ä»¥é€²è¡Œé€²ä¸€æ­¥çš„ä¸‹æ¸¸è™•ç†æˆ–åˆ†æã€‚

## ä»»å‹™ 1ï¼šä½¿ç”¨ DataFrame æ–¹æ³•å’Œå‡½æ•¸è½‰æ›æ•¸æ“š

1.  å–®æ“Š + Code ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼

**CodeCopy**

> from pyspark.sql.functions import \*
>
> \## Create Year and Month columns
>
> transformed_df = df.withColumn("Year",
> year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))
>
> \# Create the new FirstName and LastName fields
>
> transformed_df = transformed_df.withColumn("FirstName",
> split(col("CustomerName"), " ").getItem(0)).withColumn("LastName",
> split(col("CustomerName"), " ").getItem(1))
>
> \# Filter and reorder columns
>
> transformed_df = transformed_df\["SalesOrderNumber",
> "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName",
> "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"\]
>
> \# Display the first five orders
>
> display(transformed_df.limit(5))

![](./media/image46.png)

1.  **Run** ä»£ç¢¼ï¼Œé€šéä»¥ä¸‹è½‰æ›å¾åŸå§‹è¨‚å–®æ•¸æ“šå‰µå»ºæ–°çš„ DataFrame:

    - æ ¹æ“š **OrderDate** åˆ—æ·»åŠ  **Year** å’Œ **Month** åˆ—ã€‚

    - æ ¹æ“š **CustomerName** åˆ—æ·»åŠ  **FirstName** å’Œ **LastName** åˆ—ã€‚

    - ç¯©é¸ä¸¦é‡æ–°æ’åºåˆ—ï¼Œåˆªé™¤ **CustomerName** åˆ—ã€‚

![](./media/image47.png)

2.  æŸ¥çœ‹è¼¸å‡ºä¸¦é©—è­‰æ˜¯å¦å·²å°æ•¸æ“šé€²è¡Œè½‰æ›ã€‚

![](./media/image48.png)

æ‚¨å¯ä»¥ä½¿ç”¨ Spark SQL
åº«çš„å…¨éƒ¨åŠŸèƒ½ï¼Œé€šéç¯©é¸è¡Œã€æ´¾ç”Ÿã€åˆªé™¤ã€é‡å‘½ååˆ—ä»¥åŠæ‡‰ç”¨ä»»ä½•å…¶ä»–æ‰€éœ€çš„æ•¸æ“šä¿®æ”¹ä¾†è½‰æ›æ•¸æ“šã€‚

**æç¤º**ï¼šè«‹åƒé–± [*Spark DataFrame
documentation*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)ï¼Œç­è§£æœ‰é—œ
Dataframe å°è±¡æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

## ä»»å‹™ 2ï¼šä¿å­˜è½‰æ›å¾Œçš„æ•¸æ“š

1.  ä½¿ç”¨ä»¥ä¸‹ä»£ç¢¼ **Add a new cell**ï¼Œä»¥ Parquet
    æ ¼å¼ä¿å­˜è½‰æ›å¾Œçš„æ•¸æ“šå¹€ï¼ˆå¦‚æœæ•¸æ“šå·²å­˜åœ¨ï¼Œå‰‡è¦†è“‹æ•¸æ“šï¼‰ã€‚**Run**
    å–®å…ƒæ ¼ä¸¦ç­‰å¾…æ•¸æ“šå·²ä¿å­˜çš„æ¶ˆæ¯ã€‚

> CodeCopy
>
> transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')
>
> print ("Transformed data saved!")
>
> **æ³¨æ„**ï¼šé€šå¸¸ï¼Œ*Parquet*
> æ ¼å¼æ˜¯ç”¨æ–¼é€²ä¸€æ­¥åˆ†ææˆ–å¼•å…¥åˆ†æå­˜å„²çš„æ•¸æ“šæ–‡ä»¶çš„é¦–é¸æ ¼å¼ã€‚Parquet
> æ˜¯ä¸€ç¨®éå¸¸æœ‰æ•ˆçš„æ ¼å¼ï¼Œå¤§å¤šæ•¸å¤§å‹æ•¸æ“šåˆ†æç³»çµ±éƒ½æ”¯æŒå®ƒã€‚äº‹å¯¦ä¸Šï¼Œæœ‰æ™‚æ‚¨çš„æ•¸æ“šè½‰æ›è¦æ±‚å¯èƒ½åªæ˜¯å°‡æ•¸æ“šå¾å…¶ä»–æ ¼å¼ï¼ˆä¾‹å¦‚
> CSVï¼‰è½‰æ›ç‚º Parquetï¼

![](./media/image49.png)

![](./media/image50.png)

2.  ç„¶å¾Œï¼Œåœ¨å·¦å´çš„ **Lakehouse explorer**Â çª—æ ¼ä¸­ï¼Œåœ¨ **Files** ç¯€é»çš„
    ... èœå–®ä¸­ï¼Œé¸æ“‡**Refresh**ã€‚

> ![](./media/image51.png)

3.  å–®æ“Š **transformed_data** æ–‡ä»¶å¤¾ä»¥é©—è­‰å®ƒæ˜¯å¦åŒ…å«åç‚º **orders**
    çš„æ–°æ–‡ä»¶å¤¾ï¼Œè€Œè©²æ–‡ä»¶å¤¾åˆåŒ…å«ä¸€å€‹æˆ–å¤šå€‹ **Parquet files**ã€‚

![](./media/image52.png)

4.  å–®æ“Š **+ Code** following code**ï¼Œ**å¾ **transformed_data -\>
    orders** æ–‡ä»¶å¤¾ä¸­çš„ parquet æ–‡ä»¶åŠ è¼‰æ–°çš„æ•¸æ“šå¹€ :

> **CodeCopy**
>
> orders_df =
> spark.read.format("parquet").load("Files/transformed_data/orders")
>
> display(orders_df)
>
> ![](./media/image53.png)

5.  **Run** å–®å…ƒæ ¼ä¸¦é©—è­‰çµæœæ˜¯å¦é¡¯ç¤ºå·²å¾ parquet æ–‡ä»¶åŠ è¼‰çš„è¨‚å–®æ•¸æ“šã€‚

> ![](./media/image54.png)

## ä»»å‹™ 3ï¼šå°‡æ•¸æ“šä¿å­˜åœ¨åˆ†å€æ–‡ä»¶ä¸­

1.  æ·»åŠ ä¸€å€‹æ–°å–®å…ƒæ ¼ï¼Œå–®æ“Šä½¿ç”¨ä»¥ä¸‹ä»£ç¢¼çš„ **+ Code** ;é€™å°‡ä¿å­˜
    DataFrameï¼Œä¸¦æŒ‰Â **Year** å’Œ**Month** å°æ•¸æ“šé€²è¡Œåˆ†å€ã€‚ **Run**
    å–®å…ƒæ ¼ä¸¦ç­‰å¾…æ•¸æ“šå·²ä¿å­˜çš„æ¶ˆæ¯

> CodeCopy
>
> orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")
>
> print ("Transformed data saved!")
>
> ![](./media/image55.png)
>
> ![](./media/image56.png)

2.  ç„¶å¾Œï¼Œåœ¨å·¦å´çš„ **Lakehouse** è³‡æºç®¡ç†å™¨çª—æ ¼ä¸­ï¼Œåœ¨ **Files** ç¯€é»çš„
    ... èœå–®ä¸­ï¼Œé¸æ“‡åˆ·æ–°**Refresh**ã€‚

![](./media/image57.png)

![](./media/image58.png)

3.  å±•é–‹ **partitioned_orders** æ–‡ä»¶å¤¾ä»¥é©—è­‰å®ƒæ˜¯å¦åŒ…å«åç‚º **Year=xxxx**
    çš„æ–‡ä»¶å¤¾å±¤æ¬¡çµæ§‹ï¼Œæ¯å€‹æ–‡ä»¶å¤¾éƒ½åŒ…å«åç‚º **Month=xxxx**
    çš„æ–‡ä»¶å¤¾ã€‚æ¯å€‹æœˆä»½æ–‡ä»¶å¤¾éƒ½åŒ…å«ä¸€å€‹ parquet
    æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«è©²æœˆçš„è¨‚å–®ã€‚

![](./media/image59.png)

![](./media/image60.png)

> åœ¨è™•ç†å¤§é‡æ•¸æ“šæ™‚ï¼Œå°æ•¸æ“šæ–‡ä»¶é€²è¡Œåˆ†å€æ˜¯å„ªåŒ–æ€§èƒ½çš„å¸¸ç”¨æ–¹æ³•ã€‚æ­¤æŠ€è¡“å¯ä»¥é¡¯è‘—æé«˜æ€§èƒ½ï¼Œä¸¦ä½¿å…¶æ›´æ˜“æ–¼ç¯©é¸æ•¸æ“šã€‚

4.  æ·»åŠ æ–°å–®å…ƒæ ¼ï¼Œå–®æ“ŠåŒ…å«ä»¥ä¸‹ä»£ç¢¼çš„ **+ Codeï¼Œ**å¾ **orders.parquet**
    æ–‡ä»¶åŠ è¼‰æ–°æ•¸æ“šå¹€ï¼š

> CodeCopy
>
> orders_2021_df =
> spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=\*")
>
> display(orders_2021_df)

![](./media/image61.png)

5.  **Run** å–®å…ƒæ ¼ä¸¦é©—è­‰çµæœæ˜¯å¦é¡¯ç¤º 2021
    å¹´éŠ·å”®é¡çš„è¨‚å–®æ•¸æ“šã€‚è«‹æ³¨æ„ï¼Œåœ¨è·¯å¾‘ä¸­æŒ‡å®šçš„åˆ†å€åˆ—ï¼ˆ**Year** å’Œ
    **Month**ï¼‰ä¸åŒ…å«åœ¨æ•¸æ“šå¹€ä¸­ã€‚

![](./media/image62.png)

# **ç·´ç¿’ 3ï¼šä½¿ç”¨** tables **å’Œ SQL**

æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼ŒDataFrame
å°è±¡çš„æœ¬æ©Ÿæ–¹æ³•ä½¿æ‚¨èƒ½å¤ éå¸¸æœ‰æ•ˆåœ°æŸ¥è©¢å’Œåˆ†ææ–‡ä»¶ä¸­çš„æ•¸æ“šã€‚ä½†æ˜¯ï¼Œè¨±å¤šæ•¸æ“šåˆ†æå¸«æ›´ç¿’æ…£æ–¼ä½¿ç”¨ä»–å€‘å¯ä»¥ä½¿ç”¨
SQL èªæ³•æŸ¥è©¢çš„è¡¨ã€‚Spark
æä¾›äº†ä¸€å€‹*metastoreÂ *ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­å®šç¾©é—œä¿‚è¡¨ã€‚æä¾› DataFrame å°è±¡çš„
Spark SQL åº«é‚„æ”¯æŒä½¿ç”¨ SQL èªå¥ä¾†æŸ¥è©¢å…ƒå­˜å„²ä¸­çš„è¡¨ã€‚é€šéä½¿ç”¨ Spark
çš„é€™äº›åŠŸèƒ½ï¼Œæ‚¨å¯ä»¥å°‡æ•¸æ“šæ¹–çš„éˆæ´»æ€§èˆ‡é—œä¿‚æ•¸æ“šå€‰åº«çš„çµæ§‹åŒ–æ•¸æ“šæ¶æ§‹å’ŒåŸºæ–¼
SQL çš„æŸ¥è©¢ç›¸çµåˆï¼Œå› æ­¤ç¨±ç‚ºâ€œæ•¸æ“šæ¹–å€‰ä¸€é«”â€ã€‚

## ä»»å‹™ 1ï¼šå‰µå»ºè¨—ç®¡ table

Spark
å…ƒå­˜å„²ä¸­çš„è¡¨æ˜¯å°æ•¸æ“šæ¹–ä¸­æ–‡ä»¶çš„é—œä¿‚æŠ½è±¡ã€‚è¡¨å¯ä»¥æ˜¯*è¨—ç®¡*çš„ï¼ˆåœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæ–‡ä»¶ç”±å…ƒå­˜å„²ç®¡ç†ï¼‰æˆ–*å¤–éƒ¨*çš„ï¼ˆåœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œè¡¨å¼•ç”¨æ•¸æ“šæ¹–ä¸­æ‚¨ç¨ç«‹æ–¼å…ƒå­˜å„²ç®¡ç†çš„æ–‡ä»¶ä½ç½®ï¼‰ã€‚

1.  æ·»åŠ æ–°ä»£ç¢¼ï¼Œå–®æ“Šç­†è¨˜æœ¬çš„ **+ Code** cell
    ä¸¦è¼¸å…¥ä»¥ä¸‹ä»£ç¢¼ï¼Œè©²ä»£ç¢¼å°‡éŠ·å”®è¨‚å–®æ•¸æ“šçš„æ•¸æ“šå¹€ä¿å­˜ç‚ºåç‚º
    **salesorders** çš„è¡¨ï¼š

> CodeCopy
>
> \# Create a new table
>
> df.write.format("delta").saveAsTable("salesorders")
>
> \# Get the table description
>
> spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)

![A screenshot of a computer Description automatically
generated](./media/image63.png)

**æ³¨æ„**ï¼šå€¼å¾—æ³¨æ„çš„æ˜¯æ­¤ç¤ºä¾‹çš„å¹¾é»ã€‚é¦–å…ˆï¼Œæ²’æœ‰æä¾›é¡¯å¼è·¯å¾‘ï¼Œå› æ­¤è¡¨çš„æ–‡ä»¶å°‡ç”±
metastore ç®¡ç†ã€‚å…¶æ¬¡ï¼Œè©²è¡¨ä»¥ **delta**
æ ¼å¼ä¿å­˜ã€‚æ‚¨å¯ä»¥åŸºæ–¼å¤šç¨®æ–‡ä»¶æ ¼å¼ï¼ˆåŒ…æ‹¬ CSVã€Parquetã€Avro ç­‰ï¼‰å‰µå»ºè¡¨ï¼Œä½†
*delta lake* æ˜¯ä¸€ç¨® Spark
æŠ€è¡“ï¼Œå¯å‘è¡¨æ·»åŠ é—œç³»æ•¸æ“šåº«åŠŸèƒ½;åŒ…æ‹¬å°äº‹å‹™ã€è¡Œç‰ˆæœ¬æ§åˆ¶å’Œå…¶ä»–æœ‰ç”¨åŠŸèƒ½çš„æ”¯æŒã€‚å°æ–¼
Fabric ä¸­çš„æ•¸æ“šæ¹–å€‰ä¸€é«”ï¼Œæœ€å¥½ä»¥ delta æ ¼å¼å‰µå»ºè¡¨ã€‚

2.  **Run** ä»£ç¢¼å–®å…ƒä¸¦æŸ¥çœ‹è¼¸å‡ºï¼Œå…¶ä¸­æè¿°äº†æ–°è¡¨çš„å®šç¾©ã€‚

![A screenshot of a computer Description automatically
generated](./media/image64.png)

3.  åœ¨ Â **Lakehouse explorer** çª—æ ¼ä¸­ï¼Œåœ¨ **Tables** æ–‡ä»¶å¤¾çš„ ...
    èœå–®ä¸­ï¼Œé¸æ“‡ **Refresh**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image65.png)

4.  ç„¶å¾Œï¼Œå±•é–‹ **Tables** ç¯€é»ä¸¦é©—è­‰æ˜¯å¦å·²å‰µå»º **salesorders** è¡¨ã€‚

> ![A screenshot of a computer Description automatically
> generated](./media/image66.png)

5.  å°‡é¼ æ¨™æ‡¸åœåœ¨ **salesorders** è¡¨æ—é‚Šï¼Œç„¶å¾Œå–®æ“Šæ°´å¹³çœç•¥è™Ÿ
    ï¼ˆ...ï¼‰ã€‚å°èˆªä¸¦å–®æ“Š **Load data**ï¼Œç„¶å¾Œé¸æ“‡ **Spark**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image67.png)

6.  å–®æ“Š **â–· Run cell** æŒ‰éˆ•ï¼Œè©²æŒ‰éˆ•ä½¿ç”¨ Spark SQL åº«åœ¨ PySpark
    ä»£ç¢¼ä¸­åµŒå…¥é‡å° **salesorder** è¡¨çš„ SQL
    æŸ¥è©¢ï¼Œä¸¦å°‡æŸ¥è©¢çµæœåŠ è¼‰åˆ°æ•¸æ“šå¹€ä¸­ã€‚

> CodeCopy
>
> df = spark.sql("SELECT \* FROM \[your_lakehouse\].salesorders LIMIT
> 1000")
>
> display(df)

![A screenshot of a computer program Description automatically
generated](./media/image68.png)

![](./media/image69.png)

## ä»»å‹™ 2ï¼šå‰µå»º externalÂ table

æ‚¨é‚„å¯ä»¥å‰µå»º*å¤–éƒ¨* tablesï¼Œå…¶ä¸­æ¶æ§‹å…ƒæ•¸æ“šåœ¨ Lakehouse
çš„å…ƒå­˜å„²ä¸­å®šç¾©ï¼Œä½†æ•¸æ“šæ–‡ä»¶å­˜å„²åœ¨å¤–éƒ¨ä½ç½®ã€‚

1.  åœ¨ç¬¬ä¸€å€‹ä»£ç¢¼å–®å…ƒæ ¼è¿”å›çš„çµæœä¸‹ï¼Œä½¿ç”¨ **+ Code**
    æŒ‰éˆ•æ·»åŠ æ–°çš„ä»£ç¢¼å–®å…ƒæ ¼ï¼ˆå¦‚æœå°šä¸å­˜åœ¨ï¼‰ã€‚ç„¶å¾Œåœ¨æ–°å–®å…ƒæ ¼ä¸­è¼¸å…¥ä»¥ä¸‹ä»£ç¢¼ã€‚

CodeCopy

> df.write.format("delta").saveAsTable("external_salesorder",
> path="\<abfs_path\>/external_salesorder")

![A screenshot of a computer Description automatically
generated](./media/image70.png)

2.  åœ¨ **Lakehouse explorer** çª—æ ¼ä¸­çš„ **...** èœå–®ä¸­ ï¼Œé¸æ“‡**Â Files**
    ä¸­çš„ Copy ABFS pathã€‚

> ABFS è·¯å¾‘æ˜¯æŒ‡å‘ **Lakehouse** çš„ OneLake å­˜å„²ä¸­ **Files**
> æ–‡ä»¶å¤¾çš„å®Œå…¨é™å®šè·¯å¾‘ - é¡ä¼¼æ–¼ï¼š

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

![A screenshot of a computer Description automatically
generated](./media/image71.png)

3.  ç¾åœ¨ï¼Œç§»å‹•åˆ°ä»£ç¢¼å–®å…ƒæ ¼ä¸­ï¼Œå°‡ **\<abfs_path\>** æ›¿æ›ç‚º
    æ‚¨è¤‡è£½åˆ°è¨˜äº‹æœ¬çš„ pathï¼Œä»¥ä¾¿ä»£ç¢¼å°‡ DataFrame
    ä¿å­˜ç‚ºå¤–éƒ¨è¡¨ï¼Œå…¶ä¸­åŒ…å«æ•¸æ“šæ–‡ä»¶ï¼Œä½æ–¼ **Files** æ–‡ä»¶å¤¾**path** åç‚º
    **external_salesorder** çš„æ–‡ä»¶å¤¾ä¸­ã€‚å®Œæ•´è·¯å¾‘æ‡‰é¡ä¼¼æ–¼

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

4.  ä½¿ç”¨å–®å…ƒæ ¼å·¦å´çš„ **â–· ï¼ˆ*Run cell*ï¼‰** æŒ‰éˆ•é‹è¡Œå®ƒã€‚

![](./media/image72.png)

5.  åœ¨ **Lakehouse explorer** çª—æ ¼ä¸­ï¼Œåœ¨ **Tables** æ–‡ä»¶å¤¾çš„ ...
    èœå–®ä¸­ï¼Œé¸æ“‡ **Refresh**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image73.png)

6.  ç„¶å¾Œå±•é–‹ **Tables** ç¯€é»ä¸¦é©—è­‰æ˜¯å¦å·²å‰µå»º **external_salesorder**
    è¡¨ã€‚

![A screenshot of a computer Description automatically
generated](./media/image74.png)

7.  åœ¨ **Lakehouse** è³‡æºç®¡ç†å™¨çª—æ ¼ä¸­ï¼Œåœ¨ **Files** æ–‡ä»¶å¤¾çš„ ...
    èœå–®ä¸­ï¼Œé¸æ“‡ **Refres**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image75.png)

8.  ç„¶å¾Œå±•é–‹ **Files** ç¯€é»ä¸¦é©—è­‰ æ˜¯å¦å·²ç‚ºè¡¨çš„æ•¸æ“šæ–‡ä»¶å‰µå»º
    **external_salesorder** æ–‡ä»¶å¤¾ã€‚

![A screenshot of a computer Description automatically
generated](./media/image76.png)

## ä»»å‹™ 3ï¼šæ¯”è¼ƒè¨—ç®¡è¡¨å’Œ externalÂ tables

è®“æˆ‘å€‘æ¢è¨ä¸€ä¸‹è¨—ç®¡è¡¨å’Œå¤–éƒ¨è¡¨ä¹‹é–“çš„å€åˆ¥ã€‚

1.  åœ¨ä»£ç¢¼å–®å…ƒæ ¼è¿”å›çš„çµæœä¸‹ï¼Œä½¿ç”¨ **+ Code**
    æŒ‰éˆ•æ·»åŠ æ–°çš„ä»£ç¢¼å–®å…ƒæ ¼ã€‚å°‡ä¸‹é¢çš„ä»£ç¢¼è¤‡è£½åˆ° Code
    å–®å…ƒæ ¼ä¸­ï¼Œç„¶å¾Œä½¿ç”¨å–®å…ƒæ ¼å·¦å´çš„ **â–· ï¼ˆ*Run cell*ï¼‰** æŒ‰éˆ•é‹è¡Œå®ƒã€‚

> SqlCopy
>
> %%sql
>
> DESCRIBE FORMATTED salesorders;

![A screenshot of a computer Description automatically
generated](./media/image77.png)

![A screenshot of a computer Description automatically
generated](./media/image78.png)

2.  åœ¨çµæœä¸­ï¼ŒæŸ¥çœ‹ è¡¨çš„ **Location** å±¬æ€§ï¼Œè©²å±¬æ€§æ‡‰è©²æ˜¯ä»¥
    **/Tables/salesorders** çµå°¾çš„æ¹–å€‰ä¸€é«”çš„ **OneLake** å­˜å„²çš„è·¯å¾‘
    ï¼ˆæ‚¨å¯èƒ½éœ€è¦æ“´å¤§ **Data type** åˆ—æ‰èƒ½æŸ¥çœ‹å®Œæ•´è·¯å¾‘ï¼‰ã€‚

![A screenshot of a computer Description automatically
generated](./media/image79.png)

3.  ä¿®æ”¹ **DESCRIBE** å‘½ä»¤ä»¥é¡¯ç¤º **external_saleorder**
    è¡¨çš„è©³ç´°ä¿¡æ¯ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

4.  åœ¨ä»£ç¢¼å–®å…ƒæ ¼è¿”å›çš„çµæœä¸‹ï¼Œä½¿ç”¨ **+ Code**
    æŒ‰éˆ•æ·»åŠ æ–°çš„ä»£ç¢¼å–®å…ƒæ ¼ã€‚è¤‡è£½ä¸‹é¢çš„ä»£ç¢¼ï¼Œä¸¦ä½¿ç”¨å–®å…ƒæ ¼å·¦å´çš„ **â–·
    ï¼ˆ*Run cell*ï¼‰** æŒ‰éˆ•é‹è¡Œå®ƒã€‚

> SqlCopy
>
> %%sql
>
> DESCRIBE FORMATTED external_salesorder;

![A screenshot of a email Description automatically
generated](./media/image80.png)

5.  åœ¨çµæœä¸­ï¼ŒæŸ¥çœ‹ è¡¨çš„ **Location** å±¬æ€§ï¼Œè©²å±¬æ€§æ‡‰è©²æ˜¯ä»¥
    **/Files/external_saleorder** çµå°¾çš„æ¹–å€‰ä¸€é«”çš„ OneLake å­˜å„²è·¯å¾‘
    ï¼ˆæ‚¨å¯èƒ½éœ€è¦æ“´å¤§ **Data type** åˆ—æ‰èƒ½æŸ¥çœ‹å®Œæ•´è·¯å¾‘ï¼‰ã€‚

![A screenshot of a computer Description automatically
generated](./media/image81.png)

## ä»»å‹™ 4ï¼šåœ¨å–®å…ƒæ ¼ä¸­é‹è¡Œ SQL ä»£ç¢¼

é›–ç„¶èƒ½å¤ å°‡ SQL èªå¥åµŒå…¥åˆ°åŒ…å« PySpark
ä»£ç¢¼çš„å–®å…ƒæ ¼ä¸­å¾ˆæœ‰ç”¨ï¼Œä½†æ•¸æ“šåˆ†æå¸«é€šå¸¸åªæƒ³ç›´æ¥åœ¨ SQL ä¸­å·¥ä½œã€‚

1.  å–®æ“Š Notebook çš„ **+ Code** å–®å…ƒæ ¼ï¼Œç„¶å¾Œåœ¨å…¶ä¸­è¼¸å…¥ä»¥ä¸‹ä»£ç¢¼ã€‚å–®æ“Š **â–·
    Run cell** æŒ‰éˆ•ä¸¦æŸ¥çœ‹çµæœã€‚è«‹æ³¨æ„:

    - å–®å…ƒæ ¼é–‹é ­çš„ %%sql è¡Œï¼ˆç¨±ç‚º*é­”è¡“*ï¼‰è¡¨ç¤ºæ‡‰ä½¿ç”¨ Spark SQL
      èªè¨€é‹è¡Œæ™‚è€Œä¸æ˜¯ PySpark ä¾†é‹è¡Œæ­¤å–®å…ƒæ ¼ä¸­çš„ä»£ç¢¼ã€‚

    - SQL ä»£ç¢¼å¼•ç”¨ æ‚¨ä¹‹å‰å‰µå»ºçš„ **salesorders** è¡¨ã€‚

    - SQL æŸ¥è©¢çš„è¼¸å‡ºå°‡è‡ªå‹•ä½œç‚ºçµæœé¡¯ç¤ºåœ¨å–®å…ƒæ ¼ä¸‹

> SqlCopy
>
> %%sql
>
> SELECT YEAR(OrderDate) AS OrderYear,
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue
>
> FROM salesorders
>
> GROUP BY YEAR(OrderDate)
>
> ORDER BY OrderYear;

![A screenshot of a computer Description automatically
generated](./media/image82.png)

![](./media/image83.png)

**æ³¨æ„**ï¼šæœ‰é—œ Spark SQL å’Œæ•¸æ“šå¹€çš„æ›´å¤šä¿¡æ¯ï¼Œè«‹åƒé–± [*Spark SQL
documentation*](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html)ã€‚

# ç·´ç¿’ 4ï¼šä½¿ç”¨ Spark å¯è¦–åŒ–æ•¸æ“š

çœ¾æ‰€å‘¨çŸ¥ï¼Œä¸€å¼µåœ–ç‰‡å‹éåƒè¨€è¬èªï¼Œä¸€å¼µåœ–è¡¨é€šå¸¸å‹éä¸€åƒè¡Œæ•¸æ“šã€‚é›–ç„¶ Fabric
ä¸­çš„ç­†è¨˜æœ¬åŒ…æ‹¬ä¸€å€‹å…§ç½®çš„åœ–è¡¨è¦–åœ–ï¼Œç”¨æ–¼å¾æ•¸æ“šå¹€æˆ– Spark SQL
æŸ¥è©¢é¡¯ç¤ºçš„æ•¸æ“šï¼Œä½†å®ƒä¸¦ä¸æ˜¯ç‚ºå…¨é¢çš„åœ–è¡¨è€Œè¨­è¨ˆçš„ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨
**matplotlib** å’Œ **seaborn** ç­‰ Python
åœ–å½¢åº«æ ¹æ“šæ•¸æ“šå¹€ä¸­çš„æ•¸æ“šå‰µå»ºåœ–è¡¨ã€‚

## ä»»å‹™ 1ï¼šä»¥åœ–è¡¨å½¢å¼æŸ¥çœ‹çµæœ

1.  å–®æ“Š Notebook çš„ **+ Code** å–®å…ƒæ ¼ï¼Œç„¶å¾Œåœ¨å…¶ä¸­è¼¸å…¥ä»¥ä¸‹ä»£ç¢¼ã€‚å–®æ“Š **â–·
    Run cell** æŒ‰éˆ•ï¼Œä¸¦è§€å¯Ÿå®ƒå¾ æ‚¨ä¹‹å‰å‰µå»ºçš„ **salesorders**
    è¦–åœ–ä¸­è¿”å›æ•¸æ“šã€‚

> SqlCopy
>
> %%sql
>
> SELECT \* FROM salesorders

![A screenshot of a computer Description automatically
generated](./media/image84.png)

![A screenshot of a computer Description automatically
generated](./media/image85.png)

2.  åœ¨å–®å…ƒæ ¼ä¸‹æ–¹çš„çµæœéƒ¨åˆ†ä¸­ï¼Œå°‡ **View** é¸é …å¾ **Table** æ›´æ”¹ç‚º
    **Chart**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image86.png)

3.  ä½¿ç”¨åœ–è¡¨å³ä¸Šè§’çš„ **View options**
    æŒ‰éˆ•é¡¯ç¤ºåœ–è¡¨çš„é¸é …çª—æ ¼ã€‚ç„¶å¾ŒæŒ‰å¦‚ä¸‹æ–¹å¼è¨­ç½®é¸é …ä¸¦é¸æ“‡ **Apply** :

    - **Chart type**: Bar chart

    - **Key**: Item

    - **Values**: Quantity

    - **Series Group**:Â *leave blank*

    - **Aggregation**: Sum

    - **Stacked**:Â *Unselected*

![A blue barcode on a white background Description automatically
generated](./media/image87.png)

![A screenshot of a graph Description automatically
generated](./media/image88.png)

4.  é©—è­‰åœ–è¡¨æ˜¯å¦èˆ‡æ­¤é¡ä¼¼

![A screenshot of a computer Description automatically
generated](./media/image89.png)

## ä»»å‹™ 2ï¼šmatplotlib å…¥é–€

1.  å–®æ“Š**+ Code** ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼ã€‚ **Run**
    ä»£ç¢¼ä¸¦è§€å¯Ÿå®ƒæ˜¯å¦è¿”å›åŒ…å«å¹´æ”¶å…¥çš„ Spark æ•¸æ“šå¹€ã€‚

> CodeCopy
>
> sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue \\
>
> FROM salesorders \\
>
> GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\
>
> ORDER BY OrderYear"
>
> df_spark = spark.sql(sqlQuery)
>
> df_spark.show()

![A screenshot of a computer Description automatically
generated](./media/image90.png)

![](./media/image91.png)

2.  è¦å°‡æ•¸æ“šå¯è¦–åŒ–ç‚ºåœ–è¡¨ï¼Œæˆ‘å€‘å°‡é¦–å…ˆä½¿ç”¨ **matplotlib** Python
    åº«ã€‚è©²åº«æ˜¯è¨±å¤šå…¶ä»–åº«æ‰€åŸºæ–¼çš„æ ¸å¿ƒç¹ªåœ–åº«ï¼Œä¸¦åœ¨å‰µå»ºåœ–è¡¨æ™‚æä¾›äº†æ¥µå¤§çš„éˆæ´»æ€§ã€‚

3.  å–®æ“Š **+ Code**ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼ã€‚

**CodeCopy**

> from matplotlib import pyplot as plt
>
> \# matplotlib requires a Pandas dataframe, not a Spark one
>
> df_sales = df_spark.toPandas()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\])
>
> \# Display the plot
>
> plt.show()

![A screenshot of a computer Description automatically
generated](./media/image92.png)

5.  å–®æ“Š **Run cell**
    æŒ‰éˆ•ä¸¦æŸ¥çœ‹çµæœï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€å€‹æŸ±å½¢åœ–ï¼Œå…¶ä¸­åŒ…å«æ¯å¹´çš„ç¸½æ”¶å…¥ã€‚è«‹æ³¨æ„ç”¨æ–¼ç”Ÿæˆæ­¤åœ–è¡¨çš„ä»£ç¢¼çš„ä»¥ä¸‹åŠŸèƒ½:

    - **matplotlib** åº«éœ€è¦ *Pandas* æ•¸æ“šå¹€ï¼Œå› æ­¤æ‚¨éœ€è¦å°‡ Spark SQL
      æŸ¥è©¢è¿”å›çš„ Spark æ•¸æ“šå¹€è½‰æ›ç‚ºæ­¤æ ¼å¼ã€‚

    - **matplotlib** åº«çš„æ ¸å¿ƒæ˜¯ **pyplot**
      å°è±¡ã€‚é€™æ˜¯å¤§å¤šæ•¸ç¹ªåœ–åŠŸèƒ½çš„åŸºç¤ã€‚

    - é»˜èªè¨­ç½®æœƒç”Ÿæˆå¯ç”¨çš„åœ–è¡¨ï¼Œä½†æœ‰ç›¸ç•¶å¤§çš„ç©ºé–“å¯ä»¥å°å…¶é€²è¡Œè‡ªå®šç¾©

![A screenshot of a computer screen Description automatically
generated](./media/image93.png)

6.  ä¿®æ”¹ä»£ç¢¼ä»¥ç¹ªè£½åœ–è¡¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œå°‡ **Cell** ä¸­çš„æ‰€æœ‰ä»£ç¢¼æ›¿æ›ç‚º
    ä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Š **â–· Run cell** æŒ‰éˆ•ä¸¦æŸ¥çœ‹è¼¸å‡º

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure
>
> plt.show()

![A screenshot of a graph Description automatically
generated](./media/image94.png)

7.  è©²åœ–è¡¨ç¾åœ¨åŒ…å«æ›´å¤šä¿¡æ¯ã€‚å¾æŠ€è¡“ä¸Šè¬›ï¼Œç¹ªåœ–åŒ…å«åœ¨ **Figure**
    ä¸­ã€‚åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­ï¼Œè©²åœ–æ˜¯ç‚ºæ‚¨éš±å¼å‰µå»ºçš„;ä½†æ‚¨å¯ä»¥é¡¯å¼å‰µå»ºå®ƒã€‚

8.  ä¿®æ”¹ä»£ç¢¼ä»¥ç¹ªè£½åœ–è¡¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œå°‡ **cell** ä¸­çš„æ‰€æœ‰ä»£ç¢¼æ›¿æ›ç‚º
    ä»¥ä¸‹ä»£ç¢¼ã€‚

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a Figure
>
> fig = plt.figure(figsize=(8,3))
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure
>
> plt.show()

![A screenshot of a computer program Description automatically
generated](./media/image95.png)

9.  **Re-run** ä»£ç¢¼å–®å…ƒä¸¦æŸ¥çœ‹çµæœã€‚è©²åœ–ç¢ºå®šåœ–çš„å½¢ç‹€å’Œå¤§å°ã€‚

> ä¸€å€‹åœ–çª—å¯ä»¥åŒ…å«å¤šå€‹å­åœ–ï¼Œæ¯å€‹å­åœ–éƒ½æœ‰è‡ªå·±çš„*axis*ã€‚

![A screenshot of a computer Description automatically
generated](./media/image96.png)

10. ä¿®æ”¹ä»£ç¢¼ä»¥ç¹ªè£½åœ–è¡¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚ **Re-run**
    ä»£ç¢¼å–®å…ƒä¸¦æŸ¥çœ‹çµæœã€‚è©²åœ–çª—åŒ…å«ä»£ç¢¼ä¸­æŒ‡å®šçš„å­åœ–ã€‚

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a figure for 2 subplots (1 row, 2 columns)
>
> fig, ax = plt.subplots(1, 2, figsize = (10,4))
>
> \# Create a bar plot of revenue by year on the first axis
>
> ax\[0\].bar(x=df_sales\['OrderYear'\],
> height=df_sales\['GrossRevenue'\], color='orange')
>
> ax\[0\].set_title('Revenue by Year')
>
> \# Create a pie chart of yearly order counts on the second axis
>
> yearly_counts = df_sales\['OrderYear'\].value_counts()
>
> ax\[1\].pie(yearly_counts)
>
> ax\[1\].set_title('Orders per Year')
>
> ax\[1\].legend(yearly_counts.keys().tolist())
>
> \# Add a title to the Figure
>
> fig.suptitle('Sales Data')
>
> \# Show the figure
>
> plt.show()

![A screenshot of a computer program Description automatically
generated](./media/image97.png)

![](./media/image98.png)

**æ³¨æ„**ï¼šè¦ç­è§£æœ‰é—œä½¿ç”¨ matplotlib ç¹ªåœ–çš„æ›´å¤šä¿¡æ¯ï¼Œè«‹åƒé–± [*matplotlib
documentation*](https://matplotlib.org/).

## ä»»å‹™ 3ï¼šä½¿ç”¨ seaborn åº«

é›–ç„¶ **matplotlib**
ä½¿æ‚¨èƒ½å¤ å‰µå»ºå¤šç¨®é¡å‹çš„è¤‡é›œåœ–è¡¨ï¼Œä½†å®ƒå¯èƒ½éœ€è¦ä¸€äº›è¤‡é›œçš„ä»£ç¢¼æ‰èƒ½ç²å¾—æœ€ä½³çµæœã€‚å‡ºæ–¼é€™å€‹åŸå› ï¼Œå¤šå¹´ä¾†ï¼Œåœ¨
matplotlib
çš„åŸºç¤ä¸Šæ§‹å»ºäº†è¨±å¤šæ–°çš„åº«ï¼Œä»¥æŠ½è±¡å…¶è¤‡é›œæ€§ä¸¦å¢å¼·å…¶åŠŸèƒ½ã€‚**seaborn**
å°±æ˜¯é€™æ¨£ä¸€å€‹åº«ã€‚

1.  å–®æ“Š **+ Code** ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼ã€‚

CodeCopy

> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar chart
>
> ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()

![A screenshot of a graph Description automatically
generated](./media/image99.png)

2.  **Run** ä»£ç¢¼ä¸¦è§€å¯Ÿå®ƒæ˜¯å¦ä½¿ç”¨ seaborn åº«é¡¯ç¤ºæ¢å½¢åœ–ã€‚

![A screenshot of a graph Description automatically
generated](./media/image100.png)

3.  **Modify** ä»£ç¢¼å¦‚ä¸‹ã€‚ **Run** ä¿®æ”¹å¾Œçš„ä»£ç¢¼ï¼Œè«‹æ³¨æ„ï¼Œseaborn
    ä½¿æ‚¨èƒ½å¤ ç‚ºç¹ªåœ–è¨­ç½®ä¸€è‡´çš„é¡è‰²ä¸»é¡Œã€‚

> CodeCopy
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Set the visual theme for seaborn
>
> sns.set_theme(style="whitegrid")
>
> \# Create a bar chart
>
> ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()
>
> ![](./media/image101.png)

4.  å†æ¬¡ **Modify**ä»£ç¢¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚ **Run**
    ä¿®æ”¹å¾Œçš„ä»£ç¢¼ä»¥æŠ˜ç·šåœ–çš„å½¢å¼æŸ¥çœ‹å¹´æ”¶å…¥ã€‚

> CodeCopy
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar chart
>
> ax = sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()

![](./media/image102.png)

**æ³¨**ï¼š è¦ç­è§£æœ‰é—œä½¿ç”¨ seaborn é€²è¡Œæ‰“å°çš„æ›´å¤šä¿¡æ¯ï¼Œè«‹åƒè¦‹ [*seaborn
documentation*](https://seaborn.pydata.org/index.html)ã€‚

## ä»»å‹™ 4ï¼šä½¿ç”¨å¢é‡ tables æµå¼å‚³è¼¸æ•¸æ“š

Delta Lake æ”¯æŒæµå¼è™•ç†æ•¸æ“šã€‚Delta è¡¨å¯ä»¥æ˜¯ ä½¿ç”¨ Spark Structured
Streaming API
å‰µå»ºçš„æ•¸æ“šæµçš„æ¥æ”¶å™¨æˆ–æºã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œä½ å°‡ä½¿ç”¨å¢é‡è¡¨ä½œç‚ºæ¨¡æ“¬ç‰©è¯ç¶²
ï¼ˆIoTï¼‰ å ´æ™¯ä¸­æŸäº›æµæ•¸æ“šçš„æ¥æ”¶å™¨ã€‚

1.  å–®æ“Š **+ Code** ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Š **Run cell** æŒ‰éˆ•ã€‚

CodeCopy

> from notebookutils import mssparkutils
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import \*
>
> \# Create a folder
>
> inputPath = 'Files/data/'
>
> mssparkutils.fs.mkdirs(inputPath)
>
> \# Create a stream that reads data from the folder, using a JSON
> schema
>
> jsonSchema = StructType(\[
>
> StructField("device", StringType(), False),
>
> StructField("status", StringType(), False)
>
> \])
>
> iotstream =
> spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger",
> 1).json(inputPath)
>
> \# Write some event data to the folder
>
> device_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "data.txt", device_data, True)
>
> print("Source stream created...")

![A screenshot of a computer program Description automatically
generated](./media/image103.png)

![A screenshot of a computer Description automatically
generated](./media/image104.png)

2.  ç¢ºä¿æ¶ˆæ¯ ***Source stream created...***
    è¢«æ‰“å°å‡ºä¾†ã€‚æ‚¨å‰›å‰›é‹è¡Œçš„ä»£ç¢¼åŸºæ–¼ä¸€å€‹æ–‡ä»¶å¤¾å‰µå»ºäº†ä¸€å€‹æµæ•¸æ“šæºï¼Œè©²æ–‡ä»¶å¤¾å·²ä¿å­˜äº†ä¸€äº›æ•¸æ“šï¼Œè¡¨ç¤ºä¾†è‡ªå‡è¨­
    IoT è¨­å‚™çš„è®€æ•¸ã€‚

3.  å–®æ“Š **+ Code** ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Šé‹è¡Œ **Run cell**
    æŒ‰éˆ•ã€‚

CodeCopy

> \# Write the stream to a delta table
>
> delta_stream_table_path = 'Tables/iotdevicedata'
>
> checkpointpath = 'Files/delta/checkpoint'
>
> deltastream =
> iotstream.writeStream.format("delta").option("checkpointLocation",
> checkpointpath).start(delta_stream_table_path)
>
> print("Streaming to delta sink...")

![A screenshot of a computer Description automatically
generated](./media/image105.png)

4.  æ­¤ä»£ç¢¼å°‡ delta æ ¼å¼çš„æµå¼è™•ç†è¨­å‚™æ•¸æ“šå¯«å…¥åç‚º **iotdevicedata**
    çš„æ–‡ä»¶å¤¾ã€‚ç”±æ–¼æ–‡ä»¶å¤¾ä½ç½®çš„è·¯å¾‘ä½æ–¼ **Tables**
    æ–‡ä»¶å¤¾ä¸­ï¼Œå› æ­¤å°‡è‡ªå‹•ç‚ºå…¶å‰µå»ºä¸€å€‹è¡¨ã€‚å–®æ“Šè¡¨æ ¼æ—é‚Šçš„æ°´å¹³çœç•¥è™Ÿï¼Œç„¶å¾Œå–®æ“Š
    **Refresh**ã€‚

![](./media/image106.png)

![A screenshot of a computer Description automatically
generated](./media/image107.png)

5.  å–®æ“Š **+ Code** ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Š **Run cell** æŒ‰éˆ•ã€‚

> SqlCopy
>
> %%sql
>
> SELECT \* FROM IotDeviceData;

![A screenshot of a computer Description automatically
generated](./media/image108.png)

6.  æ­¤ä»£ç¢¼æŸ¥è©¢ **IotDeviceData** è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¾†è‡ªæµå¼è™•ç†æºçš„è¨­å‚™æ•¸æ“šã€‚

7.  å–®æ“Š **+ Code** ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Š **Run cell** æŒ‰éˆ•ã€‚

> CodeCopy
>
> \# Add more data to the source stream
>
> more_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)

![A screenshot of a computer Description automatically
generated](./media/image109.png)

8.  æ­¤ä»£ç¢¼å°‡æ›´å¤šå‡è¨­çš„è¨­å‚™æ•¸æ“šå¯«å…¥æµå¼è™•ç†æºã€‚

9.  å–®æ“Š **+ Code** ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Š **Run Cell** æŒ‰éˆ•ã€‚

> SqlCopy
>
> %%sql
>
> SELECT \* FROM IotDeviceData;
>
> ![A screenshot of a computer Description automatically
> generated](./media/image110.png)

10. æ­¤ä»£ç¢¼å†æ¬¡æŸ¥è©¢ **IotDeviceData**
    è¡¨ï¼Œè©²è¡¨ç¾åœ¨æ‡‰åŒ…å«å·²æ·»åŠ åˆ°æµå¼è™•ç†æºçš„å…¶ä»–æ•¸æ“šã€‚

11. å–®æ“Š **+ Code** ä¸¦è¤‡è£½ä¸¦ç²˜è²¼ä»¥ä¸‹ä»£ç¢¼ï¼Œç„¶å¾Œå–®æ“Š **Run cell** æŒ‰éˆ•ã€‚

> CodeCopy
>
> deltastream.stop()

![A screenshot of a computer Description automatically
generated](./media/image111.png)

12. æ­¤ä»£ç¢¼å°‡åœæ­¢æµã€‚

## ä»»å‹™ 5ï¼šä¿å­˜ notebook ä¸¦çµæŸ Spark æœƒè©±

ç¾åœ¨ï¼Œæ‚¨å·²ç¶“å®Œæˆäº†å°æ•¸æ“šçš„è™•ç†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æœ‰æ„ç¾©çš„åç¨±ä¿å­˜ç­†è¨˜æœ¬ä¸¦çµæŸ
Spark æœƒè©±ã€‚

1.  åœ¨ç­†è¨˜æœ¬èœå–®æ¬„ä¸­ï¼Œä½¿ç”¨ âš™ï¸ **Settings** åœ–æ¨™æŸ¥çœ‹ç­†è¨˜æœ¬è¨­ç½®ã€‚

![A screenshot of a computer Description automatically
generated](./media/image112.png)

2.  å°‡ç­†è¨˜æœ¬çš„ **Name** è¨­ç½®ç‚º ++**Explore Sales
    Orders**++ï¼Œç„¶å¾Œé—œé–‰è¨­ç½®çª—æ ¼ã€‚

![A screenshot of a computer Description automatically
generated](./media/image113.png)

3.  åœ¨ç­†è¨˜æœ¬èœå–®ä¸Šï¼Œé¸æ“‡ **Stop session** ä»¥çµæŸ Spark æœƒè©±ã€‚

![A screenshot of a computer Description automatically
generated](./media/image114.png)

![A screenshot of a computer Description automatically
generated](./media/image115.png)

# ç·´ç¿’ 5ï¼šåœ¨ Microsoft Fabric ä¸­å‰µå»ºæ•¸æ“šæµï¼ˆç¬¬ 2 ä»£ï¼‰

åœ¨ Microsoft Fabric ä¸­ï¼Œæ•¸æ“šæµ ï¼ˆGen2ï¼‰ é€£æ¥åˆ°å„ç¨®æ•¸æ“šæºï¼Œä¸¦åœ¨ Power
Query Online ä¸­åŸ·è¡Œè½‰æ›ã€‚ç„¶å¾Œï¼Œå¯ä»¥åœ¨ Data Pipelines
ä¸­ä½¿ç”¨å®ƒå€‘å°‡æ•¸æ“šå¼•å…¥ Lakehouse æˆ–å…¶ä»–åˆ†æå­˜å„²ï¼Œæˆ–ç‚º Power BI
å ±è¡¨å®šç¾©æ•¸æ“šé›†ã€‚

æœ¬ç·´ç¿’æ—¨åœ¨ä»‹ç´¹ Dataflows ï¼ˆGen2ï¼‰
çš„ä¸åŒå…ƒç´ ï¼Œè€Œä¸æ˜¯å‰µå»ºä¼æ¥­ä¸­å¯èƒ½å­˜åœ¨çš„è¤‡é›œè§£æ±ºæ–¹æ¡ˆ

## ä»»å‹™ 1ï¼šå‰µå»º Dataflow ï¼ˆGen2ï¼‰ ä»¥æ”å–æ•¸æ“š

ç¾åœ¨ï¼Œæ‚¨æœ‰ä¸€å€‹
Lakehouseï¼Œæ‚¨éœ€è¦å°‡ä¸€äº›æ•¸æ“šæå–åˆ°å…¶ä¸­ã€‚å¯¦ç¾æ­¤ç›®çš„çš„ä¸€ç¨®æ–¹æ³•æ˜¯å®šç¾©å°è£*æå–ã€è½‰æ›å’ŒåŠ è¼‰*
ï¼ˆETLï¼‰ æµç¨‹çš„æ•¸æ“šæµã€‚

1.  ç¾åœ¨ï¼Œå–®æ“Š å·¦å´å°èˆªçª—æ ¼ä¸­çš„ **Fabric_lakehouse**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image116.png)

2.  åœ¨ **Fabric_lakehouse** ä¸»é ä¸­ï¼Œå–®æ“Š **Get data** ï¼Œç„¶å¾Œé¸æ“‡ **New
    Dataflow Gen2ã€‚** æ­¤æ™‚å°‡æ‰“é–‹æ–°æ•¸æ“šæµçš„ Power Query ç·¨è¼¯å™¨ã€‚

![](./media/image117.png)

3.  åœ¨ **Power Query** çª—æ ¼ä¸­çš„ **Home tab** ä¸‹ï¼Œå–®æ“Š **Import from a
    Text/CSV file**ã€‚

![](./media/image118.png)

4.  åœ¨ **Connect to data source** çª—æ ¼çš„ **Connection settings**
    ä¸‹ï¼Œé¸æ“‡ **Link to file ï¼ˆpreviewï¼‰**å–®é¸æŒ‰éˆ•

- **Link to file**:Â *Selected*

- **File path or
  URL**:Â [https://raw.githubusercontent.com/MicrosoftLearning/dp-
  data/main/orders.csv](https://raw.githubusercontent.com/MicrosoftLearning/dp-%20%20data/main/orders.csv)

![](./media/image119.png)

5.  åœ¨ **Connect to data source** çª—æ ¼çš„ **Connection credentials**
    ä¸‹ï¼Œè¼¸å…¥ä»¥ä¸‹è©³ç´°ä¿¡æ¯ï¼Œç„¶å¾Œå–®æ“Š **Next** æŒ‰éˆ•ã€‚

    - **Connection**: Create new connection

    - **data gateway**: (none)

    - **Authentication kind**: Organizational account

![](./media/image120.png)

6.  åœ¨ **Preview file data** çª—æ ¼ä¸­ï¼Œå–®æ“Š **Create** ä»¥å‰µå»ºæ•¸æ“šæºã€‚ ![A
    screenshot of a computer Description automatically
    generated](./media/image121.png)

7.  **Power Query**
    ç·¨è¼¯å™¨é¡¯ç¤ºæ•¸æ“šæºå’Œä¸€çµ„ç”¨æ–¼è¨­ç½®æ•¸æ“šæ ¼å¼çš„åˆå§‹æŸ¥è©¢æ­¥é©Ÿã€‚

![A screenshot of a computer Description automatically
generated](./media/image122.png)

8.  åœ¨å·¥å…·æ¬„åŠŸèƒ½å€ä¸Šï¼Œé¸æ“‡ **Add column** é¸é …å¡ã€‚ç„¶å¾Œï¼Œé¸æ“‡ **Custom
    columnã€‚**

![A screenshot of a computer Description automatically
generated](./media/image123.png)Â 

9.  å°‡ æ–°åˆ—åç¨± è¨­ç½®ç‚º **MonthNo** ï¼Œå°‡ æ•¸æ“šé¡å‹ è¨­ç½®ç‚º Whole
    Numberï¼Œç„¶å¾Œåœ¨ **Custom column formula** ä¸‹
    æ·»åŠ ä»¥ä¸‹å…¬å¼ï¼š**Date.Month(\[OrderDate\])** ã€‚é¸æ“‡ **OKã€‚**

![A screenshot of a computer Description automatically
generated](./media/image124.png)

10. è«‹æ³¨æ„æ·»åŠ è‡ªå®šç¾©åˆ—çš„æ­¥é©Ÿæ˜¯å¦‚ä½•æ·»åŠ åˆ°æŸ¥è©¢ä¸­çš„ã€‚çµæœåˆ—å°‡é¡¯ç¤ºåœ¨æ•¸æ“šçª—æ ¼ä¸­ã€‚

![A screenshot of a computer Description automatically
generated](./media/image125.png)

**æç¤ºï¼š**åœ¨å³å´çš„ Query Settings çª—æ ¼ä¸­ï¼Œè«‹æ³¨æ„ **Applied Steps**
åŒ…æ‹¬æ¯å€‹è½‰æ›æ­¥é©Ÿã€‚åœ¨åº•éƒ¨ï¼Œæ‚¨é‚„å¯ä»¥åˆ‡æ› **Diagram flow** æŒ‰éˆ•ä»¥æ‰“é–‹æ­¥é©Ÿçš„
Visual Diagramï¼ˆå¯è¦–åŒ–åœ–è¡¨ï¼‰ã€‚

å¯ä»¥å‘ä¸Šæˆ–å‘ä¸‹ç§»å‹•æ­¥é©Ÿï¼Œé€šéé¸æ“‡é½’è¼ªåœ–æ¨™é€²è¡Œç·¨è¼¯ï¼Œä¸¦ä¸”æ‚¨å¯ä»¥é¸æ“‡æ¯å€‹æ­¥é©Ÿä»¥åœ¨é è¦½çª—æ ¼ä¸­æŸ¥çœ‹æ‡‰ç”¨çš„è½‰æ›ã€‚

ä»»å‹™ 2ï¼šç‚º Dataflow æ·»åŠ æ•¸æ“šç›®æ¨™

1.  åœ¨ **Power Query** å·¥å…·æ¬„åŠŸèƒ½å€ä¸Šï¼Œé¸æ“‡ **Home** é¸é …å¡ã€‚ç„¶å¾Œï¼Œåœ¨
    Data destination ä¸‹æ‹‰èœå–®ä¸­ï¼Œé¸æ“‡ **Lakehouse**ï¼ˆå¦‚æœå°šæœªé¸æ“‡ï¼‰ã€‚

![](./media/image126.png)

![A screenshot of a computer Description automatically
generated](./media/image127.png)

**æ³¨æ„ï¼š**å¦‚æœæ­¤é¸é …ç°é¡¯ï¼Œå‰‡æ‚¨å¯èƒ½å·²ç¶“è¨­ç½®äº†æ•¸æ“šç›®æ¨™ã€‚æª¢æŸ¥ Power Query
ç·¨è¼¯å™¨å³å´ Query settings ï¼ˆæŸ¥è©¢è¨­ç½®ï¼‰
çª—æ ¼åº•éƒ¨çš„æ•¸æ“šç›®æ¨™ã€‚å¦‚æœå·²è¨­ç½®ç›®æ¨™ï¼Œå‰‡å¯ä»¥ä½¿ç”¨ é½’è¼ª æ›´æ”¹å®ƒã€‚

2.  å–®æ“Š æ‰€é¸ **Lakehouse** é¸é …æ—é‚Šçš„ **Settings** åœ–æ¨™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image128.png)

3.  åœ¨ **Connect to data destination** å°è©±æ¡†ä¸­ï¼Œé¸æ“‡ **Edit
    connectionã€‚**

![A screenshot of a computer Description automatically
generated](./media/image129.png)

4.  åœ¨ **Connect to data destination** å°è©±æ¡†ä¸­ï¼Œé¸æ“‡ **Sign in**
    ä½¿ç”¨æ‚¨çš„ Power BI çµ„ç¹”å¸³æˆ¶ç™»éŒ„ ,
    ä»¥è¨­ç½®æ•¸æ“šæµç”¨æ–¼è¨ªå•æ¹–å€‰ä¸€é«”çš„èº«ä»½ã€‚

![A screenshot of a computer Description automatically
generated](./media/image130.png)

![A screenshot of a computer Description automatically
generated](./media/image131.png)

5.  åœ¨ Connect to data destination å°è©±æ¡†ä¸­ï¼Œé¸æ“‡ **Next**

![A screenshot of a computer Description automatically
generated](./media/image132.png)

6.  åœ¨ **Connect to data destination** å°è©±æ¡†ä¸­ï¼Œé¸æ“‡ **New
    table**ã€‚å–®æ“Š **Lakehouse folder** ï¼Œé¸æ“‡æ‚¨çš„å·¥ä½œå€ â€“
    **dp_FabricXX** ç„¶å¾Œé¸æ“‡æ‚¨çš„ Lakehouseï¼Œå³ **Fabric_lakehouseã€‚**
    ç„¶å¾Œå°‡ Table name æŒ‡å®šç‚º **orders** ä¸¦é¸æ“‡ **Next** æŒ‰éˆ•ã€‚

![A screenshot of a computer Description automatically
generated](./media/image133.png)

7.  åœ¨ **Choose destination settings** å°è©±æ¡†çš„ **Use automatic settings
    off** å’Œ **Update method** ä¸‹ï¼Œé¸æ“‡ **Append** ï¼Œç„¶å¾Œå–®æ“Š **Save
    settings** æŒ‰éˆ•ã€‚

![](./media/image134.png)

8.  **Lakehouse** ç›®æ¨™åœ¨ **Power Query** ç·¨è¼¯å™¨çš„ **query** ä¸­æŒ‡ç¤ºç‚º
    **iconã€‚**

![A screenshot of a computer Description automatically
generated](./media/image135.png)

![A screenshot of a computer Description automatically
generated](./media/image136.png)

9.  é¸æ“‡ **PublishÂ ** ä»¥ç™¼ä½ˆæ•¸æ“šæµã€‚ç„¶å¾Œç­‰å¾… **Dataflow 1**
    æ•¸æ“šæµåœ¨æ‚¨çš„å·¥ä½œå€ä¸­å‰µå»ºã€‚

![A screenshot of a computer Description automatically
generated](./media/image137.png)

10. ç™¼ä½ˆå¾Œï¼Œæ‚¨å¯ä»¥å³éµå–®æ“Šå·¥ä½œå€ä¸­çš„æ•¸æ“šæµï¼Œé¸æ“‡
    **Properties**ï¼Œç„¶å¾Œé‡å‘½åæ•¸æ“šæµã€‚

![A screenshot of a computer Description automatically
generated](./media/image138.png)

11. åœ¨ **Dataflow1** å°è©±æ¡†ä¸­ï¼Œè¼¸å…¥ **Name** ä½œç‚º **Gen2_Dataflow**
    ç„¶å¾Œå–®æ“Š **Save** æŒ‰éˆ•ã€‚

![A screenshot of a computer Description automatically
generated](./media/image139.png)

![](./media/image140.png)

## ä»»å‹™ 3ï¼šå°‡æ•¸æ“šæµæ·»åŠ åˆ°ç®¡é“

æ‚¨å¯ä»¥å°‡æ•¸æ“šæµä½œç‚ºæ´»å‹•åŒ…å«åœ¨ç®¡é“ä¸­ã€‚ç®¡é“ç”¨æ–¼ç·¨æ’æ•¸æ“šæ”å–å’Œè™•ç†æ´»å‹•ï¼Œä½¿æ‚¨èƒ½å¤ åœ¨å–®å€‹è¨ˆåŠƒæµç¨‹ä¸­å°‡æ•¸æ“šæµèˆ‡å…¶ä»–é¡å‹çš„ä½œç›¸çµåˆã€‚å¯ä»¥åœ¨å¹¾ç¨®ä¸åŒçš„é«”é©—ä¸­å‰µå»ºç®¡é“ï¼ŒåŒ…æ‹¬æ•¸æ“šå·¥å» é«”é©—ã€‚

1.  åœ¨ Synapse æ•¸æ“šå·¥ç¨‹ä¸»é çš„ **dp_FabricXX** çª—æ ¼ä¸‹ï¼Œé¸æ“‡ **+New -\>
    Data pipeline**

![](./media/image141.png)

2.  åœ¨ **New pipeline** å°è©±æ¡†ä¸­ï¼Œåœ¨ **Name** å­—æ®µä¸­è¼¸å…¥ **Load data**
    ï¼Œç„¶å¾Œå–®æ“Š **Create** æŒ‰éˆ•ä»¥æ‰“é–‹æ–°ç®¡é“ã€‚

![A screenshot of a computer Description automatically
generated](./media/image142.png)

3.  ç®¡é“ç·¨è¼¯å™¨éš¨å³æ‰“é–‹ã€‚

![A screenshot of a computer Description automatically
generated](./media/image143.png)

> **æç¤º**ï¼š å¦‚æœ Copy Data åš®å°è‡ªå‹•æ‰“é–‹ï¼Œè«‹å°‡å…¶é—œé–‰ï¼

4.  é¸æ“‡ **Pipeline activityï¼Œ**ç„¶å¾Œå‘ç®¡é“ä¸­æ·»åŠ  **Dataflow** æ´»å‹•ã€‚

![](./media/image144.png)

5.  é¸æ“‡æ–°çš„ **Dataflow1** æ´»å‹•å¾Œï¼Œåœ¨ **Settings** é¸é …å¡çš„ **Dataflow**
    ä¸‹æ‹‰åˆ—è¡¨ä¸­ï¼Œé¸æ“‡ **Gen2_Dataflow** ï¼ˆæ‚¨ä¹‹å‰å‰µå»ºçš„æ•¸æ“šæµï¼‰

![](./media/image145.png)

6.  åœ¨ **Home** é¸é …å¡ä¸Šï¼Œä½¿ç”¨ **ğŸ–« ï¼ˆ*Save*ï¼‰** åœ–æ¨™ä¿å­˜ç®¡é“ã€‚

![A screenshot of a computer Description automatically
generated](./media/image146.png)

7.  ä½¿ç”¨ **â–· Run** æŒ‰éˆ•é‹è¡Œç®¡é“ï¼Œä¸¦ç­‰å¾…å…¶å®Œæˆã€‚é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜æ™‚é–“ã€‚

> ![A screenshot of a computer Description automatically
> generated](./media/image147.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image148.png)

![A screenshot of a computer Description automatically
generated](./media/image149.png)

8.  åœ¨å·¦é‚Šç·£çš„èœå–®æ¬„ä¸­ï¼Œé¸æ“‡æ‚¨çš„å·¥ä½œå€ï¼Œå³ **dp_FabricXX**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image150.png)

9.  åœ¨ **Fabric_lakehouse** çª—æ ¼ä¸­ï¼Œé¸æ“‡ Lakehouse
    é¡å‹çš„**Gen2_FabricLakehouse**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image151.png)

12. åœ¨ **Explorer** çª—æ ¼ä¸­ï¼Œé¸æ“‡ **...** èœå–®ï¼Œé¸æ“‡
    **Refresh**ã€‚ç„¶å¾Œå±•é–‹ **Tables** ä¸¦é¸æ“‡ **orders** è¡¨ï¼Œè©²è¡¨å·²ç”±
    **dataflow**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image152.png)

![](./media/image153.png)

**æç¤º**ï¼š ä½¿ç”¨ Power BI Desktop
*æ•¸æ“šæµé€£æ¥å™¨*ç›´æ¥é€£æ¥åˆ°é€šéæ•¸æ“šæµå®Œæˆçš„æ•¸æ“šè½‰æ›ã€‚

æ‚¨é‚„å¯ä»¥é€²è¡Œå…¶ä»–è½‰æ›ï¼Œç™¼ä½ˆç‚ºæ–°æ•¸æ“šé›†ï¼Œä¸¦èˆ‡å°ˆç”¨æ•¸æ“šé›†çš„ç›®æ¨™å—çœ¾ä¸€èµ·åˆ†ç™¼ã€‚

## ä»»å‹™ 4ï¼šæ¸…ç†è³‡æº

åœ¨æœ¬ç·´ç¿’ä¸­ï¼Œæ‚¨å­¸ç¿’äº†å¦‚ä½•ä½¿ç”¨ Spark è™•ç† Microsoft Fabric ä¸­çš„æ•¸æ“šã€‚

å¦‚æœæ‚¨å·²å®Œæˆå° Lakehouse çš„æ¢ç´¢ï¼Œå‰‡å¯ä»¥åˆªé™¤ç‚ºæœ¬ç·´ç¿’å‰µå»ºçš„å·¥ä½œå€ã€‚

1.  åœ¨å·¦å´çš„æ¬„ä¸­ï¼Œé¸æ“‡å·¥ä½œå€çš„åœ–æ¨™ä»¥æŸ¥çœ‹å…¶åŒ…å«çš„æ‰€æœ‰é …ç›®ã€‚

> ![A screenshot of a computer Description automatically
> generated](./media/image154.png)

2.  åœ¨ **...** èœå–®ä¸­ï¼Œé¸æ“‡ **Workspace settings**ã€‚

![](./media/image155.png)

3.  é¸æ“‡ **General** ä¸¦å–®æ“Š **Remove this workspaceã€‚**

![A screenshot of a computer settings Description automatically
generated](./media/image156.png)

4.  åœ¨ **Delete workspace?** å°è©±æ¡†ä¸­ï¼Œå–®æ“Š **Delete** æŒ‰éˆ•ã€‚

> ![A screenshot of a computer Description automatically
> generated](./media/image157.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image158.png)

**ç¸½çµ**

æ­¤ ç”¨ä¾‹ å°‡æŒ‡å°æ‚¨å®Œæˆåœ¨ Power BI ä¸­ä½¿ç”¨ Microsoft Fabric
çš„éç¨‹ã€‚å®ƒæ¶µè“‹å„ç¨®ä»»å‹™ï¼ŒåŒ…æ‹¬è¨­ç½®å·¥ä½œå€ã€å‰µå»ºæ¹–å€‰ä¸€é«”ã€ä¸Šå‚³å’Œç®¡ç†æ•¸æ“šæ–‡ä»¶ä»¥åŠä½¿ç”¨
Notebook é€²è¡Œæ•¸æ“šæ¢ç´¢ã€‚åƒèˆ‡è€…å°‡å­¸ç¿’å¦‚ä½•ä½¿ç”¨
PySparkä½œå’Œè½‰æ›æ•¸æ“šã€å‰µå»ºå¯è¦–åŒ–ä»¥åŠä¿å­˜å’Œåˆ†å€æ•¸æ“šä»¥å¯¦ç¾é«˜æ•ˆæŸ¥è©¢ã€‚

åœ¨æ­¤ç”¨ä¾‹ä¸­ï¼Œåƒèˆ‡è€…å°‡åƒèˆ‡ä¸€ç³»åˆ—ä»»å‹™ï¼Œé‡é»æ˜¯åœ¨ Microsoft Fabric
ä¸­ä½¿ç”¨å¢é‡è¡¨ã€‚é€™äº›ä»»å‹™åŒ…æ‹¬ä¸Šå‚³å’Œæ¢ç´¢æ•¸æ“šã€å‰µå»ºè¨—ç®¡å’Œå¤–éƒ¨å¢é‡è¡¨ã€æ¯”è¼ƒå®ƒå€‘çš„å±¬æ€§ï¼Œè©²å¯¦é©—å®¤ä»‹ç´¹äº†ç”¨æ–¼ç®¡ç†çµæ§‹åŒ–æ•¸æ“šçš„
SQL åŠŸèƒ½ï¼Œä¸¦ä½¿ç”¨ matplotlib å’Œ seaborn ç­‰ Python
åº«æä¾›æœ‰é—œæ•¸æ“šå¯è¦–åŒ–çš„è¦‹è§£ã€‚é€™äº›ç·´ç¿’æ—¨åœ¨å…¨é¢ç­è§£å¦‚ä½•åˆ©ç”¨ Microsoft
Fabric é€²è¡Œæ•¸æ“šåˆ†æï¼Œä»¥åŠåœ¨ IoT ä¸Šä¸‹æ–‡ä¸­åˆä½µå¢é‡è¡¨ä»¥æµå¼å‚³è¼¸æ•¸æ“šã€‚

æ­¤ä½¿ç”¨æ¡ˆä¾‹å°‡æŒ‡å°æ‚¨å®Œæˆè¨­ç½® Fabric
å·¥ä½œå€ã€å‰µå»ºæ•¸æ“šæ¹–å€‰ä¸€é«”å’Œæ”å–æ•¸æ“šé€²è¡Œåˆ†æçš„éç¨‹ã€‚å®ƒæ¼”ç¤ºäº†å¦‚ä½•å®šç¾©æ•¸æ“šæµä¾†è™•ç†
ETLä½œä¸¦é…ç½®æ•¸æ“šç›®æ¨™ä»¥å­˜å„²è½‰æ›å¾Œçš„æ•¸æ“šã€‚æ­¤å¤–ï¼Œæ‚¨é‚„å°‡å­¸ç¿’å¦‚ä½•å°‡æ•¸æ“šæµé›†æˆåˆ°ç®¡é“ä¸­ä»¥é€²è¡Œè‡ªå‹•åŒ–è™•ç†ã€‚æœ€å¾Œï¼Œæ‚¨å°‡ç²å¾—åœ¨ç·´ç¿’å®Œæˆå¾Œæ¸…ç†è³‡æºçš„èªªæ˜ã€‚

æ­¤å¯¦é©—å®¤ç‚ºæ‚¨æä¾›ä½¿ç”¨ Fabric
çš„åŸºæœ¬æŠ€èƒ½ï¼Œä½¿æ‚¨èƒ½å¤ å‰µå»ºå’Œç®¡ç†å·¥ä½œå€ã€å»ºç«‹æ•¸æ“šæ¹–å€‰ä¸€é«”ä»¥åŠé«˜æ•ˆåŸ·è¡Œæ•¸æ“šè½‰æ›ã€‚é€šéå°‡æ•¸æ“šæµæ•´åˆåˆ°ç®¡é“ä¸­ï¼Œæ‚¨å°‡å­¸ç¿’å¦‚ä½•è‡ªå‹•åŸ·è¡Œæ•¸æ“šè™•ç†ä»»å‹™ã€ç°¡åŒ–å·¥ä½œæµç¨‹ä¸¦æé«˜å¯¦éš›å ´æ™¯ä¸­çš„ç”Ÿç”¢åŠ›ã€‚æ¸…ç†èªªæ˜å¯ç¢ºä¿æ‚¨ä¸æœƒç•™ä¸‹ä¸å¿…è¦çš„è³‡æºï¼Œå¾è€Œä¿ƒé€²æœ‰åºä¸”é«˜æ•ˆçš„å·¥ä½œå€ç®¡ç†æ–¹æ³•ã€‚
