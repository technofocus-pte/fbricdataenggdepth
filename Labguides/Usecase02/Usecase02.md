**ä»‹ç»**

Apache Spark
æ˜¯ä¸€ä¸ªç”¨äºåˆ†å¸ƒå¼æ•°æ®å¤„ç†çš„å¼€æºå¼•æ“ï¼Œå¹¿æ³›ç”¨äºæ¢ç´¢ã€å¤„ç†å’Œåˆ†æ Data Lake
Storage ä¸­çš„å¤§é‡æ•°æ®ã€‚Spark åœ¨è®¸å¤šæ•°æ®å¹³å°äº§å“ä¸­ä½œä¸ºå¤„ç†é€‰é¡¹æä¾›ï¼ŒåŒ…æ‹¬
Azure HDInsightã€Azure Databricksã€Azure Synapse Analytics å’Œ Microsoft
Fabricã€‚Spark çš„å¥½å¤„ä¹‹ä¸€æ˜¯æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬ Javaã€Scalaã€Python å’Œ
SQL;ä½¿ Spark
æˆä¸ºæ•°æ®å¤„ç†å·¥ä½œè´Ÿè½½çš„éå¸¸çµæ´»çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ•°æ®æ¸…ç†å’Œä½œã€ç»Ÿè®¡åˆ†æå’Œæœºå™¨å­¦ä¹ ä»¥åŠæ•°æ®åˆ†æå’Œå¯è§†åŒ–ã€‚

Microsoft Fabric Lakehouse ä¸­çš„è¡¨åŸºäº Apache Spark çš„å¼€æº *Delta Lake*
æ ¼å¼ã€‚Delta Lake å¢åŠ äº†å¯¹æ‰¹å¤„ç†å’Œæµæ•°æ®ä½œçš„å…³ç³»è¯­ä¹‰çš„æ”¯æŒï¼Œå¹¶æ”¯æŒåˆ›å»º
Lakehouse ä½“ç³»ç»“æ„ï¼Œåœ¨è¯¥ä½“ç³»ç»“æ„ä¸­ï¼ŒApache Spark
å¯ç”¨äºå¤„ç†å’ŒæŸ¥è¯¢åŸºäºæ•°æ®æ¹–ä¸­åŸºç¡€æ–‡ä»¶çš„è¡¨ä¸­çš„æ•°æ®ã€‚

åœ¨ Microsoft Fabric ä¸­ï¼Œæ•°æ®æµ ï¼ˆGen2ï¼‰ è¿æ¥åˆ°å„ç§æ•°æ®æºï¼Œå¹¶åœ¨ Power
Query Online ä¸­æ‰§è¡Œè½¬æ¢ã€‚ç„¶åï¼Œå¯ä»¥åœ¨ Data Pipelines
ä¸­ä½¿ç”¨å®ƒä»¬å°†æ•°æ®å¼•å…¥ Lakehouse æˆ–å…¶ä»–åˆ†æå­˜å‚¨ï¼Œæˆ–ä¸º Power BI
æŠ¥è¡¨å®šä¹‰æ•°æ®é›†ã€‚

æ­¤å®éªŒå®¤æ—¨åœ¨ä»‹ç» Dataflows ï¼ˆGen2ï¼‰
çš„ä¸åŒå…ƒç´ ï¼Œè€Œä¸æ˜¯åˆ›å»ºä¼ä¸šä¸­å¯èƒ½å­˜åœ¨çš„å¤æ‚è§£å†³æ–¹æ¡ˆã€‚

**ç›®æ ‡ï¼š**

- åœ¨ Microsoft Fabric ä¸­åˆ›å»ºå·¥ä½œåŒºï¼Œå¹¶å¯ç”¨ Fabric è¯•ç”¨ç‰ˆã€‚

- å»ºç«‹æ¹–ä»“ä¸€ä½“ç¯å¢ƒå¹¶ä¸Šä¼ æ•°æ®æ–‡ä»¶è¿›è¡Œåˆ†æã€‚

- ç”Ÿæˆç”¨äºäº¤äº’å¼æ•°æ®æ¢ç´¢å’Œåˆ†æçš„ç¬”è®°æœ¬ã€‚

- å°†æ•°æ®åŠ è½½åˆ° DataFrame ä¸­ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥å¤„ç†å’Œå¯è§†åŒ–ã€‚

- ä½¿ç”¨ PySpark å¯¹æ•°æ®åº”ç”¨è½¬æ¢ã€‚

- ä¿å­˜è½¬æ¢åçš„æ•°æ®å¹¶å¯¹å…¶è¿›è¡Œåˆ†åŒºï¼Œä»¥å®ç°ä¼˜åŒ–æŸ¥è¯¢ã€‚

- åœ¨ Spark å…ƒå­˜å‚¨ä¸­åˆ›å»ºè¡¨ä»¥è¿›è¡Œç»“æ„åŒ–æ•°æ®ç®¡ç†

&nbsp;

- å°† DataFrame å¦å­˜ä¸ºåä¸º â€œsalesordersâ€ çš„æ‰˜ç®¡å¢é‡è¡¨ã€‚

- å°† DataFrame å¦å­˜ä¸ºå…·æœ‰æŒ‡å®šè·¯å¾„çš„åä¸º â€œexternal_salesorderâ€ çš„å¤–éƒ¨
  delta è¡¨ã€‚

- æè¿°å’Œæ¯”è¾ƒæ‰˜ç®¡è¡¨å’Œå¤–éƒ¨è¡¨çš„å±æ€§ã€‚

- å¯¹è¡¨æ‰§è¡Œ SQL æŸ¥è¯¢ä»¥è¿›è¡Œåˆ†æå’ŒæŠ¥å‘Šã€‚

- ä½¿ç”¨ Python åº“ï¼ˆå¦‚ matplotlib å’Œ seabornï¼‰å¯è§†åŒ–æ•°æ®ã€‚

- åœ¨æ•°æ®å·¥ç¨‹ä½“éªŒä¸­å»ºç«‹æ•°æ®æ¹–ä»“ä¸€ä½“ï¼Œå¹¶æ‘„å–ç›¸å…³æ•°æ®ä»¥ä¾›åç»­åˆ†æã€‚

- å®šä¹‰ç”¨äºæå–ã€è½¬æ¢æ•°æ®å¹¶å°†å…¶åŠ è½½åˆ° Lakehouse ä¸­çš„æ•°æ®æµã€‚

- åœ¨ Power Query ä¸­é…ç½®æ•°æ®ç›®æ ‡ï¼Œä»¥å°†è½¬æ¢åçš„æ•°æ®å­˜å‚¨åœ¨ Lakehouse ä¸­ã€‚

- å°†æ•°æ®æµåˆå¹¶åˆ°ç®¡é“ä¸­ï¼Œä»¥å¯ç”¨è®¡åˆ’çš„æ•°æ®å¤„ç†å’Œæ‘„å–ã€‚

- åˆ é™¤å·¥ä½œåŒºå’Œå…³è”çš„å…ƒç´ ä»¥ç»“æŸç»ƒä¹ ã€‚

# ç»ƒä¹  1ï¼šåˆ›å»º workspaceã€Lakehouseã€Notebook å¹¶å°†æ•°æ®åŠ è½½åˆ° DataFrame ä¸­ 

## ä»»åŠ¡ 1ï¼šåˆ›å»ºå·¥ä½œåŒº 

åœ¨ Fabric ä¸­å¤„ç†æ•°æ®ä¹‹å‰ï¼Œè¯·åˆ›å»ºä¸€ä¸ªå¯ç”¨äº† Fabric è¯•ç”¨ç‰ˆçš„å·¥ä½œåŒºã€‚

1.  æ‰“å¼€æµè§ˆå™¨ï¼Œå¯¼èˆªåˆ°åœ°å€æ ï¼Œç„¶åé”®å…¥æˆ–ç²˜è´´ä»¥ä¸‹
    URLï¼š<https://app.fabric.microsoft.com/> ç„¶åæŒ‰ **Enter** æŒ‰é’®ã€‚

> **æ³¨æ„**ï¼šå¦‚æœæ‚¨è¢«å®šå‘åˆ° Microsoft Fabric ä¸»é¡µï¼Œè¯·è·³è¿‡ä» \#2 åˆ° \#4
> çš„æ­¥éª¤ã€‚
>
> ![](./media/image1.png)

2.  åœ¨ **Microsoft Fabric** çª—å£ä¸­ï¼Œè¾“å…¥æ‚¨çš„å‡­æ®ï¼Œç„¶åå•å‡» **Submit**
    æŒ‰é’®ã€‚

> ![](./media/image2.png)

3.  ç„¶åï¼Œåœ¨ **Microsoft** çª—å£ä¸­è¾“å…¥å¯†ç å¹¶å•å‡» **Sign in** æŒ‰é’®**ã€‚**

> ![A login screen with a red box and blue text Description
> automatically generated](./media/image3.png)

4.  åœ¨ **Stay signed in?** çª—å£ä¸­ï¼Œå•å‡» **Yes** æŒ‰é’®ã€‚

> ![A screenshot of a computer error Description automatically
> generated](./media/image4.png)

5.  Fabric ä¸»é¡µï¼Œé€‰æ‹© **+New workspace tile**ã€‚

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image5.png)

6.  åœ¨ **Create a workspace tab** ä¸­ï¼Œè¾“å…¥ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯ï¼Œç„¶åå•å‡»
    **Apply** æŒ‰é’®ã€‚

[TABLE]

> ![A screenshot of a computer Description automatically
> generated](./media/image6.png)![](./media/image7.png)![](./media/image8.png)![](./media/image9.png)

7.  ç­‰å¾…éƒ¨ç½²å®Œæˆã€‚å®Œæˆéœ€è¦ 2-3 åˆ†é’Ÿã€‚å½“æ‚¨çš„æ–° workspace
    æ‰“å¼€æ—¶ï¼Œå®ƒåº”è¯¥æ˜¯ç©ºçš„ã€‚

## ä»»åŠ¡ 2ï¼šåˆ›å»º Lakehouse å¹¶ä¸Šä¼ æ–‡ä»¶

ç°åœ¨ï¼Œä½ å·²æ‹¥æœ‰å·¥ä½œåŒºï¼Œå¯ä»¥åˆ‡æ¢åˆ°é—¨æˆ·ä¸­çš„ *Data engineering*
ä½“éªŒï¼Œå¹¶ä¸ºè¦åˆ†æçš„æ•°æ®æ–‡ä»¶åˆ›å»ºæ•°æ®æ¹–ä»“ä¸€ä½“ã€‚

1.  é€šè¿‡å•å‡»å¯¼èˆªæ ä¸­çš„ **+New item** æŒ‰é’®åˆ›å»ºæ–°çš„ Eventhouseã€‚

![A screenshot of a browser AI-generated content may be
incorrect.](./media/image10.png)

2.  ç‚¹å‡»â€œ**Lakehouseâ€**ç£è´´ã€‚

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image11.png)

3.  åœ¨ **New lakehouse** å¯¹è¯æ¡†ä¸­ï¼Œ åœ¨ **Name** å­—æ®µä¸­è¾“å…¥
    **+++Fabric_lakehouse+++Â **ï¼Œå•å‡» **Create** æŒ‰é’®å¹¶æ‰“å¼€æ–°çš„
    Lakehouseã€‚

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image12.png)

4.  å¤§çº¦ä¸€åˆ†é’Ÿåï¼Œå°†åˆ›å»ºä¸€ä¸ªæ–°çš„ç©º
    Lakehouseã€‚æ‚¨éœ€è¦å°†ä¸€äº›æ•°æ®æå–åˆ°æ•°æ®æ¹–ä»“ä¸€ä½“ä¸­è¿›è¡Œåˆ†æã€‚

![A screenshot of a computer Description automatically
generated](./media/image13.png)

5.  æ‚¨å°†çœ‹åˆ°ä¸€æ¡é€šçŸ¥ï¼ŒæŒ‡å‡º **Successfully created SQL endpoint**ã€‚

> ![](./media/image14.png)

6.  åœ¨ **Explorer** éƒ¨åˆ†çš„ **fabric_lakehouse** ä¸‹ï¼Œå°†é¼ æ ‡æ‚¬åœåœ¨ **Files
    folder** æ—è¾¹ï¼Œç„¶åå•å‡»æ°´å¹³çœç•¥å· **ï¼ˆ...ï¼‰** èœå•ã€‚å¯¼èˆªå¹¶å•å‡»
    **Upload**ï¼Œç„¶åå•å‡» **Upload folder**ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](./media/image15.png)

7.  åœ¨å³ä¾§æ˜¾ç¤ºçš„ **Upload folder** çª—æ ¼ä¸­ï¼Œé€‰æ‹© **Files/** ä¸‹çš„ **folder
    icon**ï¼Œç„¶åæµè§ˆåˆ° **Cï¼š\LabFiles**ï¼Œç„¶åé€‰æ‹© **orders**
    æ–‡ä»¶å¤¹å¹¶å•å‡» Upload æŒ‰é’®ã€‚

![](./media/image16.png)

8.  å¦‚æœä¸Šä¼  **Upload 3 files to this site?** å¯¹è¯æ¡†ï¼Œç„¶åå•å‡»
    **Upload** æŒ‰é’®ã€‚

![](./media/image17.png)

9.  åœ¨ Upload folder çª—æ ¼ä¸­ï¼Œå•å‡» **Upload** æŒ‰é’®ã€‚

> ![](./media/image18.png)

10. ä¸Šä¼ æ–‡ä»¶åï¼Œ**å…³é—­ Upload folder** çª—æ ¼ã€‚

> ![](./media/image19.png)

11. å±•å¼€ **Files** å¹¶é€‰æ‹© **orders** æ–‡ä»¶å¤¹ï¼Œå¹¶éªŒè¯ CSV æ–‡ä»¶æ˜¯å¦å·²ä¸Šä¼ ã€‚

![](./media/image20.png)

## ä»»åŠ¡ 3ï¼šåˆ›å»º notebook

è¦åœ¨ Apache Spark
ä¸­å¤„ç†æ•°æ®ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ª*ç¬”è®°æœ¬*ã€‚ç¬”è®°æœ¬æä¾›äº†ä¸€ä¸ªäº¤äº’å¼ç¯å¢ƒï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­ç¼–å†™å’Œè¿è¡Œä»£ç ï¼ˆä»¥å¤šç§è¯­è¨€ï¼‰ï¼Œå¹¶æ·»åŠ æ³¨é‡Šä»¥è®°å½•ä»£ç ã€‚

1.  åœ¨ **Home** ä¸Šï¼ŒæŸ¥çœ‹æ•°æ®æ¹–ä¸­ **orders** æ–‡ä»¶å¤¹çš„å†…å®¹æ—¶ï¼Œåœ¨ **Open
    notebook** èœå•ä¸­ï¼Œé€‰æ‹© **New notebook**ã€‚

![](./media/image21.png)

2.  å‡ ç§’é’Ÿåï¼Œå°†æ‰“å¼€ä¸€ä¸ªåŒ…å«å•ä¸ªå•å…ƒæ ¼çš„æ–°ç¬”è®°æœ¬
    ã€‚ç¬”è®°æœ¬ç”±ä¸€ä¸ªæˆ–å¤šä¸ªå•å…ƒæ ¼ç»„æˆï¼Œè¿™äº›å•å…ƒæ ¼å¯ä»¥åŒ…å«*ä»£ç *æˆ–
    *Markdown*ï¼ˆæ ¼å¼åŒ–æ–‡æœ¬ï¼‰ã€‚

![](./media/image22.png)

3.  é€‰æ‹©ç¬¬ä¸€ä¸ªå•å…ƒæ ¼ï¼ˆå½“å‰ä¸º*ä»£ç *å•å…ƒæ ¼ï¼‰ï¼Œç„¶ååœ¨å…¶å³ä¸Šè§’çš„åŠ¨æ€å·¥å…·æ ä¸­ï¼Œä½¿ç”¨
    **Mâ†“** æŒ‰é’® **convert the cell to aÂ markdownÂ cell**ã€‚

![](./media/image23.png)

4.  å½“å•å…ƒæ ¼æ›´æ”¹ä¸º Markdown å•å…ƒæ ¼æ—¶ï¼Œå°†å‘ˆç°å®ƒåŒ…å«çš„æ–‡æœ¬ã€‚

![](./media/image24.png)

5.  **ğŸ–‰** ä½¿ç”¨ ï¼ˆEditï¼‰
    æŒ‰é’®å°†å•å…ƒæ ¼åˆ‡æ¢åˆ°ç¼–è¾‘æ¨¡å¼ï¼Œæ›¿æ¢æ‰€æœ‰æ–‡æœ¬ï¼Œç„¶åæŒ‰å¦‚ä¸‹æ–¹å¼ä¿®æ”¹
    markdown:

> CodeCopy
>
> \# Sales order data exploration
>
> Use the code in this notebook to explore sales order data.

![](./media/image25.png)

![A screenshot of a computer Description automatically
generated](./media/image26.png)

6.  å•å‡»ç¬”è®°æœ¬ä¸­å•å…ƒæ ¼å¤–éƒ¨çš„ä»»æ„ä½ç½®å¯åœæ­¢ç¼–è¾‘å®ƒå¹¶æŸ¥çœ‹å‘ˆç°çš„ Markdownã€‚

![A screenshot of a computer Description automatically
generated](./media/image27.png)

## ä»»åŠ¡ 4ï¼šå°†æ•°æ®åŠ è½½åˆ° DataFrame ä¸­

ç°åœ¨ï¼Œæ‚¨å¯ä»¥è¿è¡Œå°†æ•°æ®åŠ è½½åˆ° *DataFrame* ä¸­çš„ä»£ç ã€‚Spark
ä¸­çš„æ•°æ®å¸§ç±»ä¼¼äº Python ä¸­çš„ Pandas
æ•°æ®å¸§ï¼Œå¹¶æä¾›ä¸€ç§é€šç”¨ç»“æ„æ¥å¤„ç†è¡Œå’Œåˆ—ä¸­çš„æ•°æ®ã€‚

**æ³¨æ„**ï¼šSpark æ”¯æŒå¤šç§ç¼–ç è¯­è¨€ï¼ŒåŒ…æ‹¬ Scalaã€Java
ç­‰ã€‚åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ *PySpark*ï¼Œå®ƒæ˜¯ Spark ä¼˜åŒ–çš„ Python
å˜ä½“ã€‚PySpark æ˜¯ Spark ä¸Šæœ€å¸¸ç”¨çš„è¯­è¨€ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ Fabric
ç¬”è®°æœ¬ä¸­çš„é»˜è®¤è¯­è¨€ã€‚

1.  åœ¨ç¬”è®°æœ¬å¯è§çš„æƒ…å†µä¸‹ï¼Œå±•å¼€ **Files** åˆ—è¡¨å¹¶é€‰æ‹© **orders**
    æ–‡ä»¶å¤¹ï¼Œä»¥ä¾¿ CSV æ–‡ä»¶åˆ—åœ¨ç¬”è®°æœ¬ç¼–è¾‘å™¨æ—è¾¹ã€‚

![](./media/image28.png)

2.  ç°åœ¨ï¼Œæ‚¨çš„é¼ æ ‡2019.csvæ–‡ä»¶ã€‚å•å‡»æ°´å¹³çœç•¥å· **ï¼ˆ...ï¼‰**
    é™¤äº†2019.csvã€‚å¯¼èˆªå¹¶å•å‡» **Load data**ï¼Œç„¶åé€‰æ‹©
    **Spark**ã€‚åŒ…å«ä»¥ä¸‹ä»£ç çš„æ–°ä»£ç å•å…ƒæ ¼å°†æ·»åŠ åˆ°ç¬”è®°æœ¬ä¸­ï¼š

> CodeCopy
>
> df =
> spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
>
> \# df now is a Spark DataFrame containing CSV data from
> "Files/orders/2019.csv".
>
> display(df)

![](./media/image29.png)

![](./media/image30.png)

**æç¤º**ï¼š æ‚¨å¯ä»¥ä½¿ç”¨å·¦ä¾§çš„ Â« å›¾æ ‡æ¥éšè—å·¦ä¾§çš„ Lakehouse Explorer çª—æ ¼
ã€‚è¡Œä¸º

æ‰€ä»¥å°†å¸®åŠ©æ‚¨ä¸“æ³¨äºç¬”è®°æœ¬ã€‚

3.  ä½¿ç”¨å•å…ƒæ ¼å·¦ä¾§çš„ **â–· Run cell** æŒ‰é’®æ¥è¿è¡Œå®ƒã€‚

![](./media/image31.png)

**æ³¨æ„**ï¼šç”±äºè¿™æ˜¯æ‚¨ç¬¬ä¸€æ¬¡è¿è¡Œä»»ä½• Spark ä»£ç ï¼Œå› æ­¤å¿…é¡»å¯åŠ¨ Spark
ä¼šè¯ã€‚è¿™æ„å‘³ç€ä¼šè¯ä¸­çš„ç¬¬ä¸€æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸€åˆ†é’Ÿå·¦å³æ‰èƒ½å®Œæˆã€‚åç»­è¿è¡Œä¼šæ›´å¿«ã€‚

4.  cell å‘½ä»¤å®Œæˆåï¼ŒæŸ¥çœ‹å•å…ƒæ ¼ä¸‹æ–¹çš„è¾“å‡ºï¼Œè¯¥è¾“å‡ºåº”ç±»ä¼¼äºä»¥ä¸‹å†…å®¹ï¼š

![](./media/image32.png)

5.  è¾“å‡ºæ˜¾ç¤º 2019.csv
    æ–‡ä»¶ä¸­æ•°æ®çš„è¡Œå’Œåˆ—ã€‚ä½†æ˜¯ï¼Œè¯·æ³¨æ„ï¼Œåˆ—æ ‡é¢˜çœ‹èµ·æ¥ä¸æ­£ç¡®ã€‚ç”¨äºå°†æ•°æ®åŠ è½½åˆ°
    DataFrame çš„é»˜è®¤ä»£ç å‡å®š CSV
    æ–‡ä»¶åœ¨ç¬¬ä¸€è¡Œä¸­åŒ…å«åˆ—åï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒCSV
    æ–‡ä»¶ä»…åŒ…å«æ•°æ®ï¼Œè€Œä¸åŒ…å«æ ‡é¢˜ä¿¡æ¯ã€‚

6.  ä¿®æ”¹ä»£ç ä»¥å°† **header** é€‰é¡¹è®¾ç½®ä¸º **false**ã€‚å°†å•å…ƒæ ¼ä¸­çš„æ‰€æœ‰ä»£ç 
    æ›¿æ¢ä¸ºä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **â–· Run cell** æŒ‰é’®å¹¶æŸ¥çœ‹è¾“å‡º

> CodeCopy
>
> df =
> spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
>
> \# df now is a Spark DataFrame containing CSV data from
> "Files/orders/2019.csv".
>
> display(df)

![](./media/image33.png)

7.  ç°åœ¨ï¼Œdataframe
    æ­£ç¡®åœ°åŒ…å«ç¬¬ä¸€è¡Œä½œä¸ºæ•°æ®å€¼ï¼Œä½†åˆ—åæ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œä¸æ˜¯å¾ˆæœ‰å¸®åŠ©ã€‚è¦ç†è§£æ•°æ®ï¼Œæ‚¨éœ€è¦ä¸ºæ–‡ä»¶ä¸­çš„æ•°æ®å€¼æ˜¾å¼å®šä¹‰æ­£ç¡®çš„æ¶æ„å’Œæ•°æ®ç±»å‹ã€‚

8.  å°† **cell** ä¸­çš„æ‰€æœ‰ä»£ç  æ›¿æ¢ä¸ºä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **â–· Run cell**
    æŒ‰é’®å¹¶æŸ¥çœ‹è¾“å‡º

> CodeCopy
>
> from pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> StructField("SalesOrderNumber", StringType()),
>
> StructField("SalesOrderLineNumber", IntegerType()),
>
> StructField("OrderDate", DateType()),
>
> StructField("CustomerName", StringType()),
>
> StructField("Email", StringType()),
>
> StructField("Item", StringType()),
>
> StructField("Quantity", IntegerType()),
>
> StructField("UnitPrice", FloatType()),
>
> StructField("Tax", FloatType())
>
> \])
>
> df =
> spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")
>
> display(df)

![](./media/image34.png)

![](./media/image35.png)

9.  ç°åœ¨ DataFrame åŒ…å«æ­£ç¡®çš„åˆ—åï¼ˆé™¤äº† **Indexï¼Œ**å®ƒæ˜¯æ‰€æœ‰ DataFrame
    ä¸­çš„å†…ç½®åˆ—ï¼ŒåŸºäºæ¯è¡Œçš„åºå·ä½ç½®ï¼‰ã€‚åˆ—çš„æ•°æ®ç±»å‹æ˜¯ä½¿ç”¨ Spark SQL
    åº“ä¸­å®šä¹‰çš„ä¸€ç»„æ ‡å‡†ç±»å‹æŒ‡å®šçš„ï¼Œè¿™äº›ç±»å‹æ˜¯åœ¨å•å…ƒæ ¼çš„å¼€å¤´å¯¼å…¥çš„ã€‚

10. é€šè¿‡æŸ¥çœ‹æ•°æ®å¸§ç¡®è®¤æ‚¨çš„æ›´æ”¹å·²åº”ç”¨äºæ•°æ®ã€‚

11. ä½¿ç”¨ å•å…ƒæ ¼è¾“å‡ºä¸‹æ–¹çš„ **+ Code**
    å›¾æ ‡å°†æ–°çš„ä»£ç å•å…ƒæ ¼æ·»åŠ åˆ°ç¬”è®°æœ¬ä¸­ï¼Œç„¶ååœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹ä»£ç ã€‚ç‚¹å‡» **â–·
    Run cell** æŒ‰é’®å¹¶æŸ¥çœ‹è¾“å‡º

> CodeCopy
>
> display(df)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image36.png)

12. æ•°æ®å¸§ä»…åŒ…å« **2019.csv** æ–‡ä»¶ä¸­çš„æ•°æ®ã€‚ä¿®æ”¹ä»£ç ï¼Œä»¥ä¾¿æ–‡ä»¶è·¯å¾„ä½¿ç”¨
    \* é€šé…ç¬¦ä» orders æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶ä¸­è¯»å–é”€å”®è®¢å•æ•°æ®

13. ä½¿ç”¨ å•å…ƒæ ¼è¾“å‡ºä¸‹æ–¹çš„ **+ Code**
    å›¾æ ‡å°†æ–°çš„ä»£ç å•å…ƒæ ¼æ·»åŠ åˆ°ç¬”è®°æœ¬ä¸­ï¼Œç„¶ååœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹ä»£ç ã€‚

CodeCopy

> from pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> Â  Â  StructField("SalesOrderNumber", StringType()),
>
> Â  Â  StructField("SalesOrderLineNumber", IntegerType()),
>
> Â  Â  StructField("OrderDate", DateType()),
>
> Â  Â  StructField("CustomerName", StringType()),
>
> Â  Â  StructField("Email", StringType()),
>
> Â  Â  StructField("Item", StringType()),
>
> Â  Â  StructField("Quantity", IntegerType()),
>
> Â  Â  StructField("UnitPrice", FloatType()),
>
> Â  Â  StructField("Tax", FloatType())
>
> Â  Â  \])
>
> df =
> spark.read.format("csv").schema(orderSchema).load("Files/orders/\*.csv")
>
> display(df)

![](./media/image37.png)

14. è¿è¡Œä¿®æ”¹åçš„ä»£ç å•å…ƒå¹¶æŸ¥çœ‹è¾“å‡ºï¼Œç°åœ¨åº”åŒ…æ‹¬ 2019 å¹´ã€2020 å¹´å’Œ 2021
    å¹´çš„é”€å”®é¢ã€‚

![](./media/image38.png)

**æ³¨æ„**ï¼šä»…æ˜¾ç¤ºè¡Œçš„å­é›†ï¼Œå› æ­¤æ‚¨å¯èƒ½æ— æ³•çœ‹åˆ°æ‰€æœ‰å¹´ä»½çš„ç¤ºä¾‹ã€‚

# ç»ƒä¹  2ï¼šæµè§ˆ dataframe ä¸­çš„æ•°æ®

dataframe
å¯¹è±¡åŒ…å«å„ç§å‡½æ•°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›å‡½æ•°æ¥ç­›é€‰ã€åˆ†ç»„å’Œä»¥å…¶ä»–æ–¹å¼ä½œå®ƒæ‰€åŒ…å«çš„æ•°æ®ã€‚

## ä»»åŠ¡ 1ï¼šç­›é€‰ dataframe

1.  ä½¿ç”¨ å•å…ƒæ ¼è¾“å‡ºä¸‹æ–¹çš„ **+ Code**
    å›¾æ ‡å°†æ–°çš„ä»£ç å•å…ƒæ ¼æ·»åŠ åˆ°ç¬”è®°æœ¬ä¸­ï¼Œç„¶ååœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹ä»£ç ã€‚

**CodeCopy**

> customers = df\['CustomerName', 'Email'\]
>
> print(customers.count())
>
> print(customers.distinct().count())
>
> display(customers.distinct())
>
> ![](./media/image39.png)

2.  **Run** æ–°çš„ä»£ç å•å…ƒï¼Œå¹¶æŸ¥çœ‹ç»“æœã€‚è¯·æ³¨æ„ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯:

    - å½“æ‚¨å¯¹ DataFrame æ‰§è¡Œä½œæ—¶ï¼Œç»“æœæ˜¯ä¸€ä¸ªæ–°çš„
      DataFrameï¼ˆåœ¨æœ¬ä¾‹ä¸­ï¼Œé€šè¿‡ä» **df** DataFrame
      ä¸­é€‰æ‹©ç‰¹å®šçš„åˆ—å­é›†æ¥åˆ›å»ºæ–°çš„ **customers** DataFrameï¼‰

    &nbsp;

    - Dataframes æä¾› **count** å’Œ **distinct**
      ç­‰å‡½æ•°ï¼Œå¯ç”¨äºæ±‡æ€»å’Œç­›é€‰å®ƒä»¬åŒ…å«çš„æ•°æ®ã€‚

    &nbsp;

    - dataframe\['Field1'ï¼Œ 'Field2'ï¼Œ ...\]
      è¯­æ³•æ˜¯å®šä¹‰åˆ—å­é›†çš„ç®€å†™æ–¹æ³•ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ **select**
      æ–¹æ³•ï¼Œå› æ­¤ä¸Šé¢ä»£ç çš„ç¬¬ä¸€è¡Œå¯ä»¥å†™æˆ customers =
      df.selectï¼ˆâ€œCustomerNameâ€ï¼Œ â€œEmailâ€ï¼‰

> ![](./media/image40.png)

3.  ä¿®æ”¹ä»£ç ï¼Œå°† **cell** ä¸­çš„æ‰€æœ‰ä»£ç æ›¿æ¢ä¸º ä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **â–· Run
    cell** æŒ‰é’®ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

> CodeCopy
>
> customers = df.select("CustomerName",
> "Email").where(df\['Item'\]=='Road-250 Red, 52')
>
> print(customers.count())
>
> print(customers.distinct().count())
>
> display(customers.distinct())

4.  **Runè¿è¡Œ**ä¿®æ”¹åçš„ä»£ç ä»¥æŸ¥çœ‹å·²è´­ä¹° ***Road-250 Redï¼Œ 52* product**
    çš„å®¢æˆ·ã€‚è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥å°†å¤šä¸ªå‡½æ•° **â€œchainâ€**
    åœ¨ä¸€èµ·ï¼Œä»¥ä¾¿ä¸€ä¸ªå‡½æ•°çš„è¾“å‡ºæˆä¸ºä¸‹ä¸€ä¸ªå‡½æ•°çš„è¾“å…¥ -
    åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ**select** æ–¹æ³•åˆ›å»ºçš„æ•°æ®å¸§æ˜¯ç”¨äºåº”ç”¨ç­›é€‰æ¡ä»¶çš„
    **where** æ–¹æ³•çš„æºæ•°æ®å¸§ã€‚

> ![](./media/image41.png)

## ä»»åŠ¡ 2ï¼šåœ¨ dataframe ä¸­èšåˆå’Œåˆ†ç»„æ•°æ®

1.  å•å‡» **+ Code** **+** å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **Run cell**
    æŒ‰é’®ã€‚

CodeCopy

> productSales = df.select("Item", "Quantity").groupBy("Item").sum()
>
> display(productSales)
>
> ![](./media/image42.png)

2.  è¯·æ³¨æ„ï¼Œç»“æœæ˜¾ç¤ºæŒ‰äº§å“åˆ†ç»„çš„è®¢å•æ•°é‡æ€»å’Œã€‚**groupBy** æ–¹æ³•æŒ‰ Item
    å¯¹è¡Œè¿›è¡Œåˆ†ç»„ï¼Œéšåçš„ **sum**
    èšåˆå‡½æ•°å°†åº”ç”¨äºæ‰€æœ‰å‰©ä½™çš„æ•°å­—åˆ—ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸º *Quantity*ï¼‰

![](./media/image43.png)

3.  å•å‡» **+ Code** å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **Run cell** æŒ‰é’®ã€‚

> **CodeCopy**
>
> from pyspark.sql.functions import \*
>
> yearlySales =
> df.select(year("OrderDate").alias("Year")).groupBy("Year").count().orderBy("Year")
>
> display(yearlySales)

![](./media/image44.png)

4.  è¯·æ³¨æ„ï¼Œç»“æœæ˜¾ç¤ºæ¯å¹´çš„é”€å”®è®¢å•æ•°é‡ã€‚è¯·æ³¨æ„ï¼Œ**select** æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ª
    SQL **year** å‡½æ•°ï¼Œç”¨äºæå– *OrderDate*
    å­—æ®µçš„å¹´ä»½éƒ¨åˆ†ï¼ˆè¿™å°±æ˜¯ä»£ç åŒ…å«ä¸€ä¸ª **import** è¯­å¥ä»¥ä» Spark SQL
    åº“å¯¼å…¥å‡½æ•°çš„åŸå› ï¼‰ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨**Â alias**
    æ–¹æ³•ä¸ºæå–çš„å¹´ä»½å€¼åˆ†é…åˆ—åç§°ã€‚ç„¶åï¼ŒæŒ‰æ´¾ç”Ÿçš„ Year åˆ—å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„
    ï¼Œå¹¶è®¡ç®—æ¯ä¸ªç»„ä¸­çš„è¡Œæ•°ï¼Œæœ€åä½¿ç”¨ **orderBy**
    æ–¹æ³•å¯¹ç”Ÿæˆçš„æ•°æ®å¸§è¿›è¡Œæ’åºã€‚

![](./media/image45.png)

# ç»ƒä¹  3ï¼šä½¿ç”¨ Spark è½¬æ¢æ•°æ®æ–‡ä»¶

æ•°æ®å·¥ç¨‹å¸ˆçš„ä¸€é¡¹å¸¸è§ä»»åŠ¡æ˜¯ä»¥ç‰¹å®šæ ¼å¼æˆ–ç»“æ„æ‘„å–æ•°æ®ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè½¬æ¢ä»¥è¿›è¡Œè¿›ä¸€æ­¥çš„ä¸‹æ¸¸å¤„ç†æˆ–åˆ†æã€‚

## ä»»åŠ¡ 1ï¼šä½¿ç”¨ DataFrame æ–¹æ³•å’Œå‡½æ•°è½¬æ¢æ•°æ®

1.  å•å‡» + Code å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç 

**CodeCopy**

> from pyspark.sql.functions import \*
>
> \## Create Year and Month columns
>
> transformed_df = df.withColumn("Year",
> year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))
>
> \# Create the new FirstName and LastName fields
>
> transformed_df = transformed_df.withColumn("FirstName",
> split(col("CustomerName"), " ").getItem(0)).withColumn("LastName",
> split(col("CustomerName"), " ").getItem(1))
>
> \# Filter and reorder columns
>
> transformed_df = transformed_df\["SalesOrderNumber",
> "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName",
> "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"\]
>
> \# Display the first five orders
>
> display(transformed_df.limit(5))

![](./media/image46.png)

1.  **Run** ä»£ç ï¼Œé€šè¿‡ä»¥ä¸‹è½¬æ¢ä»åŸå§‹è®¢å•æ•°æ®åˆ›å»ºæ–°çš„ DataFrame:

    - æ ¹æ® **OrderDate** åˆ—æ·»åŠ  **Year** å’Œ **Month** åˆ—ã€‚

    - æ ¹æ® **CustomerName** åˆ—æ·»åŠ  **FirstName** å’Œ **LastName** åˆ—ã€‚

    - ç­›é€‰å¹¶é‡æ–°æ’åºåˆ—ï¼Œåˆ é™¤ **CustomerName** åˆ—ã€‚

![](./media/image47.png)

2.  æŸ¥çœ‹è¾“å‡ºå¹¶éªŒè¯æ˜¯å¦å·²å¯¹æ•°æ®è¿›è¡Œè½¬æ¢ã€‚

![](./media/image48.png)

æ‚¨å¯ä»¥ä½¿ç”¨ Spark SQL
åº“çš„å…¨éƒ¨åŠŸèƒ½ï¼Œé€šè¿‡ç­›é€‰è¡Œã€æ´¾ç”Ÿã€åˆ é™¤ã€é‡å‘½ååˆ—ä»¥åŠåº”ç”¨ä»»ä½•å…¶ä»–æ‰€éœ€çš„æ•°æ®ä¿®æ”¹æ¥è½¬æ¢æ•°æ®ã€‚

**æç¤º**ï¼šè¯·å‚é˜… [*Spark DataFrame
documentation*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)ï¼Œäº†è§£æœ‰å…³
Dataframe å¯¹è±¡æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

## ä»»åŠ¡ 2ï¼šä¿å­˜è½¬æ¢åçš„æ•°æ®

1.  ä½¿ç”¨ä»¥ä¸‹ä»£ç  **Add a new cell**ï¼Œä»¥ Parquet
    æ ¼å¼ä¿å­˜è½¬æ¢åçš„æ•°æ®å¸§ï¼ˆå¦‚æœæ•°æ®å·²å­˜åœ¨ï¼Œåˆ™è¦†ç›–æ•°æ®ï¼‰ã€‚**Run**
    å•å…ƒæ ¼å¹¶ç­‰å¾…æ•°æ®å·²ä¿å­˜çš„æ¶ˆæ¯ã€‚

> CodeCopy
>
> transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')
>
> print ("Transformed data saved!")
>
> **æ³¨æ„**ï¼šé€šå¸¸ï¼Œ*Parquet*
> æ ¼å¼æ˜¯ç”¨äºè¿›ä¸€æ­¥åˆ†ææˆ–å¼•å…¥åˆ†æå­˜å‚¨çš„æ•°æ®æ–‡ä»¶çš„é¦–é€‰æ ¼å¼ã€‚Parquet
> æ˜¯ä¸€ç§éå¸¸æœ‰æ•ˆçš„æ ¼å¼ï¼Œå¤§å¤šæ•°å¤§å‹æ•°æ®åˆ†æç³»ç»Ÿéƒ½æ”¯æŒå®ƒã€‚äº‹å®ä¸Šï¼Œæœ‰æ—¶æ‚¨çš„æ•°æ®è½¬æ¢è¦æ±‚å¯èƒ½åªæ˜¯å°†æ•°æ®ä»å…¶ä»–æ ¼å¼ï¼ˆä¾‹å¦‚
> CSVï¼‰è½¬æ¢ä¸º Parquetï¼

![](./media/image49.png)

![](./media/image50.png)

2.  ç„¶åï¼Œåœ¨å·¦ä¾§çš„ **Lakehouse explorer**Â çª—æ ¼ä¸­ï¼Œåœ¨ **Files** èŠ‚ç‚¹çš„
    ... èœå•ä¸­ï¼Œé€‰æ‹©**Refresh**ã€‚

> ![](./media/image51.png)

3.  å•å‡» **transformed_data** æ–‡ä»¶å¤¹ä»¥éªŒè¯å®ƒæ˜¯å¦åŒ…å«åä¸º **orders**
    çš„æ–°æ–‡ä»¶å¤¹ï¼Œè€Œè¯¥æ–‡ä»¶å¤¹åˆåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ª **Parquet files**ã€‚

![](./media/image52.png)

4.  å•å‡» **+ Code** following code**ï¼Œ**ä» **transformed_data -\>
    orders** æ–‡ä»¶å¤¹ä¸­çš„ parquet æ–‡ä»¶åŠ è½½æ–°çš„æ•°æ®å¸§ :

> **CodeCopy**
>
> orders_df =
> spark.read.format("parquet").load("Files/transformed_data/orders")
>
> display(orders_df)
>
> ![](./media/image53.png)

5.  **Run** å•å…ƒæ ¼å¹¶éªŒè¯ç»“æœæ˜¯å¦æ˜¾ç¤ºå·²ä» parquet æ–‡ä»¶åŠ è½½çš„è®¢å•æ•°æ®ã€‚

> ![](./media/image54.png)

## ä»»åŠ¡ 3ï¼šå°†æ•°æ®ä¿å­˜åœ¨åˆ†åŒºæ–‡ä»¶ä¸­

1.  æ·»åŠ ä¸€ä¸ªæ–°å•å…ƒæ ¼ï¼Œå•å‡»ä½¿ç”¨ä»¥ä¸‹ä»£ç çš„ **+ Code** ;è¿™å°†ä¿å­˜
    DataFrameï¼Œå¹¶æŒ‰Â **Year** å’Œ**Month** å¯¹æ•°æ®è¿›è¡Œåˆ†åŒºã€‚ **Run**
    å•å…ƒæ ¼å¹¶ç­‰å¾…æ•°æ®å·²ä¿å­˜çš„æ¶ˆæ¯

> CodeCopy
>
> orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")
>
> print ("Transformed data saved!")
>
> ![](./media/image55.png)
>
> ![](./media/image56.png)

2.  ç„¶åï¼Œåœ¨å·¦ä¾§çš„ **Lakehouse** èµ„æºç®¡ç†å™¨çª—æ ¼ä¸­ï¼Œåœ¨ **Files** èŠ‚ç‚¹çš„
    ... èœå•ä¸­ï¼Œé€‰æ‹©åˆ·æ–°**Refresh**ã€‚

![](./media/image57.png)

![](./media/image58.png)

3.  å±•å¼€ **partitioned_orders** æ–‡ä»¶å¤¹ä»¥éªŒè¯å®ƒæ˜¯å¦åŒ…å«åä¸º **Year=xxxx**
    çš„æ–‡ä»¶å¤¹å±‚æ¬¡ç»“æ„ï¼Œæ¯ä¸ªæ–‡ä»¶å¤¹éƒ½åŒ…å«åä¸º **Month=xxxx**
    çš„æ–‡ä»¶å¤¹ã€‚æ¯ä¸ªæœˆä»½æ–‡ä»¶å¤¹éƒ½åŒ…å«ä¸€ä¸ª parquet
    æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«è¯¥æœˆçš„è®¢å•ã€‚

![](./media/image59.png)

![](./media/image60.png)

> åœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶ï¼Œå¯¹æ•°æ®æ–‡ä»¶è¿›è¡Œåˆ†åŒºæ˜¯ä¼˜åŒ–æ€§èƒ½çš„å¸¸ç”¨æ–¹æ³•ã€‚æ­¤æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œå¹¶ä½¿å…¶æ›´æ˜“äºç­›é€‰æ•°æ®ã€‚

4.  æ·»åŠ æ–°å•å…ƒæ ¼ï¼Œå•å‡»åŒ…å«ä»¥ä¸‹ä»£ç çš„ **+ Codeï¼Œ**ä» **orders.parquet**
    æ–‡ä»¶åŠ è½½æ–°æ•°æ®å¸§ï¼š

> CodeCopy
>
> orders_2021_df =
> spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=\*")
>
> display(orders_2021_df)

![](./media/image61.png)

5.  **Run** å•å…ƒæ ¼å¹¶éªŒè¯ç»“æœæ˜¯å¦æ˜¾ç¤º 2021
    å¹´é”€å”®é¢çš„è®¢å•æ•°æ®ã€‚è¯·æ³¨æ„ï¼Œåœ¨è·¯å¾„ä¸­æŒ‡å®šçš„åˆ†åŒºåˆ—ï¼ˆ**Year** å’Œ
    **Month**ï¼‰ä¸åŒ…å«åœ¨æ•°æ®å¸§ä¸­ã€‚

![](./media/image62.png)

# **ç»ƒä¹  3ï¼šä½¿ç”¨** tables **å’Œ SQL**

æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼ŒDataFrame
å¯¹è±¡çš„æœ¬æœºæ–¹æ³•ä½¿æ‚¨èƒ½å¤Ÿéå¸¸æœ‰æ•ˆåœ°æŸ¥è¯¢å’Œåˆ†ææ–‡ä»¶ä¸­çš„æ•°æ®ã€‚ä½†æ˜¯ï¼Œè®¸å¤šæ•°æ®åˆ†æå¸ˆæ›´ä¹ æƒ¯äºä½¿ç”¨ä»–ä»¬å¯ä»¥ä½¿ç”¨
SQL è¯­æ³•æŸ¥è¯¢çš„è¡¨ã€‚Spark
æä¾›äº†ä¸€ä¸ª*metastoreÂ *ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­å®šä¹‰å…³ç³»è¡¨ã€‚æä¾› DataFrame å¯¹è±¡çš„
Spark SQL åº“è¿˜æ”¯æŒä½¿ç”¨ SQL è¯­å¥æ¥æŸ¥è¯¢å…ƒå­˜å‚¨ä¸­çš„è¡¨ã€‚é€šè¿‡ä½¿ç”¨ Spark
çš„è¿™äº›åŠŸèƒ½ï¼Œæ‚¨å¯ä»¥å°†æ•°æ®æ¹–çš„çµæ´»æ€§ä¸å…³ç³»æ•°æ®ä»“åº“çš„ç»“æ„åŒ–æ•°æ®æ¶æ„å’ŒåŸºäº
SQL çš„æŸ¥è¯¢ç›¸ç»“åˆï¼Œå› æ­¤ç§°ä¸ºâ€œæ•°æ®æ¹–ä»“ä¸€ä½“â€ã€‚

## ä»»åŠ¡ 1ï¼šåˆ›å»ºæ‰˜ç®¡ table

Spark
å…ƒå­˜å‚¨ä¸­çš„è¡¨æ˜¯å¯¹æ•°æ®æ¹–ä¸­æ–‡ä»¶çš„å…³ç³»æŠ½è±¡ã€‚è¡¨å¯ä»¥æ˜¯*æ‰˜ç®¡*çš„ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ–‡ä»¶ç”±å…ƒå­˜å‚¨ç®¡ç†ï¼‰æˆ–*å¤–éƒ¨*çš„ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¡¨å¼•ç”¨æ•°æ®æ¹–ä¸­æ‚¨ç‹¬ç«‹äºå…ƒå­˜å‚¨ç®¡ç†çš„æ–‡ä»¶ä½ç½®ï¼‰ã€‚

1.  æ·»åŠ æ–°ä»£ç ï¼Œå•å‡»ç¬”è®°æœ¬çš„ **+ Code** cell
    å¹¶è¾“å…¥ä»¥ä¸‹ä»£ç ï¼Œè¯¥ä»£ç å°†é”€å”®è®¢å•æ•°æ®çš„æ•°æ®å¸§ä¿å­˜ä¸ºåä¸º
    **salesorders** çš„è¡¨ï¼š

> CodeCopy
>
> \# Create a new table
>
> df.write.format("delta").saveAsTable("salesorders")
>
> \# Get the table description
>
> spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)

![A screenshot of a computer Description automatically
generated](./media/image63.png)

**æ³¨æ„**ï¼šå€¼å¾—æ³¨æ„çš„æ˜¯æ­¤ç¤ºä¾‹çš„å‡ ç‚¹ã€‚é¦–å…ˆï¼Œæ²¡æœ‰æä¾›æ˜¾å¼è·¯å¾„ï¼Œå› æ­¤è¡¨çš„æ–‡ä»¶å°†ç”±
metastore ç®¡ç†ã€‚å…¶æ¬¡ï¼Œè¯¥è¡¨ä»¥ **delta**
æ ¼å¼ä¿å­˜ã€‚æ‚¨å¯ä»¥åŸºäºå¤šç§æ–‡ä»¶æ ¼å¼ï¼ˆåŒ…æ‹¬ CSVã€Parquetã€Avro ç­‰ï¼‰åˆ›å»ºè¡¨ï¼Œä½†
*delta lake* æ˜¯ä¸€ç§ Spark
æŠ€æœ¯ï¼Œå¯å‘è¡¨æ·»åŠ å…³ç³»æ•°æ®åº“åŠŸèƒ½;åŒ…æ‹¬å¯¹äº‹åŠ¡ã€è¡Œç‰ˆæœ¬æ§åˆ¶å’Œå…¶ä»–æœ‰ç”¨åŠŸèƒ½çš„æ”¯æŒã€‚å¯¹äº
Fabric ä¸­çš„æ•°æ®æ¹–ä»“ä¸€ä½“ï¼Œæœ€å¥½ä»¥ delta æ ¼å¼åˆ›å»ºè¡¨ã€‚

2.  **Run** ä»£ç å•å…ƒå¹¶æŸ¥çœ‹è¾“å‡ºï¼Œå…¶ä¸­æè¿°äº†æ–°è¡¨çš„å®šä¹‰ã€‚

![A screenshot of a computer Description automatically
generated](./media/image64.png)

3.  åœ¨ Â **Lakehouse explorer** çª—æ ¼ä¸­ï¼Œåœ¨ **Tables** æ–‡ä»¶å¤¹çš„ ...
    èœå•ä¸­ï¼Œé€‰æ‹© **Refresh**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image65.png)

4.  ç„¶åï¼Œå±•å¼€ **Tables** èŠ‚ç‚¹å¹¶éªŒè¯æ˜¯å¦å·²åˆ›å»º **salesorders** è¡¨ã€‚

> ![A screenshot of a computer Description automatically
> generated](./media/image66.png)

5.  å°†é¼ æ ‡æ‚¬åœåœ¨ **salesorders** è¡¨æ—è¾¹ï¼Œç„¶åå•å‡»æ°´å¹³çœç•¥å·
    ï¼ˆ...ï¼‰ã€‚å¯¼èˆªå¹¶å•å‡» **Load data**ï¼Œç„¶åé€‰æ‹© **Spark**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image67.png)

6.  å•å‡» **â–· Run cell** æŒ‰é’®ï¼Œè¯¥æŒ‰é’®ä½¿ç”¨ Spark SQL åº“åœ¨ PySpark
    ä»£ç ä¸­åµŒå…¥é’ˆå¯¹ **salesorder** è¡¨çš„ SQL
    æŸ¥è¯¢ï¼Œå¹¶å°†æŸ¥è¯¢ç»“æœåŠ è½½åˆ°æ•°æ®å¸§ä¸­ã€‚

> CodeCopy
>
> df = spark.sql("SELECT \* FROM \[your_lakehouse\].salesorders LIMIT
> 1000")
>
> display(df)

![A screenshot of a computer program Description automatically
generated](./media/image68.png)

![](./media/image69.png)

## ä»»åŠ¡ 2ï¼šåˆ›å»º externalÂ table

æ‚¨è¿˜å¯ä»¥åˆ›å»º*å¤–éƒ¨* tablesï¼Œå…¶ä¸­æ¶æ„å…ƒæ•°æ®åœ¨ Lakehouse
çš„å…ƒå­˜å‚¨ä¸­å®šä¹‰ï¼Œä½†æ•°æ®æ–‡ä»¶å­˜å‚¨åœ¨å¤–éƒ¨ä½ç½®ã€‚

1.  åœ¨ç¬¬ä¸€ä¸ªä»£ç å•å…ƒæ ¼è¿”å›çš„ç»“æœä¸‹ï¼Œä½¿ç”¨ **+ Code**
    æŒ‰é’®æ·»åŠ æ–°çš„ä»£ç å•å…ƒæ ¼ï¼ˆå¦‚æœå°šä¸å­˜åœ¨ï¼‰ã€‚ç„¶ååœ¨æ–°å•å…ƒæ ¼ä¸­è¾“å…¥ä»¥ä¸‹ä»£ç ã€‚

CodeCopy

> df.write.format("delta").saveAsTable("external_salesorder",
> path="\<abfs_path\>/external_salesorder")

![A screenshot of a computer Description automatically
generated](./media/image70.png)

2.  åœ¨ **Lakehouse explorer** çª—æ ¼ä¸­çš„ **...** èœå•ä¸­ ï¼Œé€‰æ‹©**Â Files**
    ä¸­çš„ Copy ABFS pathã€‚

> ABFS è·¯å¾„æ˜¯æŒ‡å‘ **Lakehouse** çš„ OneLake å­˜å‚¨ä¸­ **Files**
> æ–‡ä»¶å¤¹çš„å®Œå…¨é™å®šè·¯å¾„ - ç±»ä¼¼äºï¼š

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

![A screenshot of a computer Description automatically
generated](./media/image71.png)

3.  ç°åœ¨ï¼Œç§»åŠ¨åˆ°ä»£ç å•å…ƒæ ¼ä¸­ï¼Œå°† **\<abfs_path\>** æ›¿æ¢ä¸º
    æ‚¨å¤åˆ¶åˆ°è®°äº‹æœ¬çš„ pathï¼Œä»¥ä¾¿ä»£ç å°† DataFrame
    ä¿å­˜ä¸ºå¤–éƒ¨è¡¨ï¼Œå…¶ä¸­åŒ…å«æ•°æ®æ–‡ä»¶ï¼Œä½äº **Files** æ–‡ä»¶å¤¹**path** åä¸º
    **external_salesorder** çš„æ–‡ä»¶å¤¹ä¸­ã€‚å®Œæ•´è·¯å¾„åº”ç±»ä¼¼äº

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

4.  ä½¿ç”¨å•å…ƒæ ¼å·¦ä¾§çš„ **â–· ï¼ˆ*Run cell*ï¼‰** æŒ‰é’®è¿è¡Œå®ƒã€‚

![](./media/image72.png)

5.  åœ¨ **Lakehouse explorer** çª—æ ¼ä¸­ï¼Œåœ¨ **Tables** æ–‡ä»¶å¤¹çš„ ...
    èœå•ä¸­ï¼Œé€‰æ‹© **Refresh**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image73.png)

6.  ç„¶åå±•å¼€ **Tables** èŠ‚ç‚¹å¹¶éªŒè¯æ˜¯å¦å·²åˆ›å»º **external_salesorder**
    è¡¨ã€‚

![A screenshot of a computer Description automatically
generated](./media/image74.png)

7.  åœ¨ **Lakehouse** èµ„æºç®¡ç†å™¨çª—æ ¼ä¸­ï¼Œåœ¨ **Files** æ–‡ä»¶å¤¹çš„ ...
    èœå•ä¸­ï¼Œé€‰æ‹© **Refres**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image75.png)

8.  ç„¶åå±•å¼€ **Files** èŠ‚ç‚¹å¹¶éªŒè¯ æ˜¯å¦å·²ä¸ºè¡¨çš„æ•°æ®æ–‡ä»¶åˆ›å»º
    **external_salesorder** æ–‡ä»¶å¤¹ã€‚

![A screenshot of a computer Description automatically
generated](./media/image76.png)

## ä»»åŠ¡ 3ï¼šæ¯”è¾ƒæ‰˜ç®¡è¡¨å’Œ externalÂ tables

è®©æˆ‘ä»¬æ¢è®¨ä¸€ä¸‹æ‰˜ç®¡è¡¨å’Œå¤–éƒ¨è¡¨ä¹‹é—´çš„åŒºåˆ«ã€‚

1.  åœ¨ä»£ç å•å…ƒæ ¼è¿”å›çš„ç»“æœä¸‹ï¼Œä½¿ç”¨ **+ Code**
    æŒ‰é’®æ·»åŠ æ–°çš„ä»£ç å•å…ƒæ ¼ã€‚å°†ä¸‹é¢çš„ä»£ç å¤åˆ¶åˆ° Code
    å•å…ƒæ ¼ä¸­ï¼Œç„¶åä½¿ç”¨å•å…ƒæ ¼å·¦ä¾§çš„ **â–· ï¼ˆ*Run cell*ï¼‰** æŒ‰é’®è¿è¡Œå®ƒã€‚

> SqlCopy
>
> %%sql
>
> DESCRIBE FORMATTED salesorders;

![A screenshot of a computer Description automatically
generated](./media/image77.png)

![A screenshot of a computer Description automatically
generated](./media/image78.png)

2.  åœ¨ç»“æœä¸­ï¼ŒæŸ¥çœ‹ è¡¨çš„ **Location** å±æ€§ï¼Œè¯¥å±æ€§åº”è¯¥æ˜¯ä»¥
    **/Tables/salesorders** ç»“å°¾çš„æ¹–ä»“ä¸€ä½“çš„ **OneLake** å­˜å‚¨çš„è·¯å¾„
    ï¼ˆæ‚¨å¯èƒ½éœ€è¦æ‰©å¤§ **Data type** åˆ—æ‰èƒ½æŸ¥çœ‹å®Œæ•´è·¯å¾„ï¼‰ã€‚

![A screenshot of a computer Description automatically
generated](./media/image79.png)

3.  ä¿®æ”¹ **DESCRIBE** å‘½ä»¤ä»¥æ˜¾ç¤º **external_saleorder**
    è¡¨çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

4.  åœ¨ä»£ç å•å…ƒæ ¼è¿”å›çš„ç»“æœä¸‹ï¼Œä½¿ç”¨ **+ Code**
    æŒ‰é’®æ·»åŠ æ–°çš„ä»£ç å•å…ƒæ ¼ã€‚å¤åˆ¶ä¸‹é¢çš„ä»£ç ï¼Œå¹¶ä½¿ç”¨å•å…ƒæ ¼å·¦ä¾§çš„ **â–·
    ï¼ˆ*Run cell*ï¼‰** æŒ‰é’®è¿è¡Œå®ƒã€‚

> SqlCopy
>
> %%sql
>
> DESCRIBE FORMATTED external_salesorder;

![A screenshot of a email Description automatically
generated](./media/image80.png)

5.  åœ¨ç»“æœä¸­ï¼ŒæŸ¥çœ‹ è¡¨çš„ **Location** å±æ€§ï¼Œè¯¥å±æ€§åº”è¯¥æ˜¯ä»¥
    **/Files/external_saleorder** ç»“å°¾çš„æ¹–ä»“ä¸€ä½“çš„ OneLake å­˜å‚¨è·¯å¾„
    ï¼ˆæ‚¨å¯èƒ½éœ€è¦æ‰©å¤§ **Data type** åˆ—æ‰èƒ½æŸ¥çœ‹å®Œæ•´è·¯å¾„ï¼‰ã€‚

![A screenshot of a computer Description automatically
generated](./media/image81.png)

## ä»»åŠ¡ 4ï¼šåœ¨å•å…ƒæ ¼ä¸­è¿è¡Œ SQL ä»£ç 

è™½ç„¶èƒ½å¤Ÿå°† SQL è¯­å¥åµŒå…¥åˆ°åŒ…å« PySpark
ä»£ç çš„å•å…ƒæ ¼ä¸­å¾ˆæœ‰ç”¨ï¼Œä½†æ•°æ®åˆ†æå¸ˆé€šå¸¸åªæƒ³ç›´æ¥åœ¨ SQL ä¸­å·¥ä½œã€‚

1.  å•å‡» Notebook çš„ **+ Code** å•å…ƒæ ¼ï¼Œç„¶ååœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹ä»£ç ã€‚å•å‡» **â–·
    Run cell** æŒ‰é’®å¹¶æŸ¥çœ‹ç»“æœã€‚è¯·æ³¨æ„:

    - å•å…ƒæ ¼å¼€å¤´çš„ %%sql è¡Œï¼ˆç§°ä¸º*é­”æœ¯*ï¼‰è¡¨ç¤ºåº”ä½¿ç”¨ Spark SQL
      è¯­è¨€è¿è¡Œæ—¶è€Œä¸æ˜¯ PySpark æ¥è¿è¡Œæ­¤å•å…ƒæ ¼ä¸­çš„ä»£ç ã€‚

    - SQL ä»£ç å¼•ç”¨ æ‚¨ä¹‹å‰åˆ›å»ºçš„ **salesorders** è¡¨ã€‚

    - SQL æŸ¥è¯¢çš„è¾“å‡ºå°†è‡ªåŠ¨ä½œä¸ºç»“æœæ˜¾ç¤ºåœ¨å•å…ƒæ ¼ä¸‹

> SqlCopy
>
> %%sql
>
> SELECT YEAR(OrderDate) AS OrderYear,
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue
>
> FROM salesorders
>
> GROUP BY YEAR(OrderDate)
>
> ORDER BY OrderYear;

![A screenshot of a computer Description automatically
generated](./media/image82.png)

![](./media/image83.png)

**æ³¨æ„**ï¼šæœ‰å…³ Spark SQL å’Œæ•°æ®å¸§çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [*Spark SQL
documentation*](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html)ã€‚

# ç»ƒä¹  4ï¼šä½¿ç”¨ Spark å¯è§†åŒ–æ•°æ®

ä¼—æ‰€å‘¨çŸ¥ï¼Œä¸€å¼ å›¾ç‰‡èƒœè¿‡åƒè¨€ä¸‡è¯­ï¼Œä¸€å¼ å›¾è¡¨é€šå¸¸èƒœè¿‡ä¸€åƒè¡Œæ•°æ®ã€‚è™½ç„¶ Fabric
ä¸­çš„ç¬”è®°æœ¬åŒ…æ‹¬ä¸€ä¸ªå†…ç½®çš„å›¾è¡¨è§†å›¾ï¼Œç”¨äºä»æ•°æ®å¸§æˆ– Spark SQL
æŸ¥è¯¢æ˜¾ç¤ºçš„æ•°æ®ï¼Œä½†å®ƒå¹¶ä¸æ˜¯ä¸ºå…¨é¢çš„å›¾è¡¨è€Œè®¾è®¡çš„ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨
**matplotlib** å’Œ **seaborn** ç­‰ Python
å›¾å½¢åº“æ ¹æ®æ•°æ®å¸§ä¸­çš„æ•°æ®åˆ›å»ºå›¾è¡¨ã€‚

## ä»»åŠ¡ 1ï¼šä»¥å›¾è¡¨å½¢å¼æŸ¥çœ‹ç»“æœ

1.  å•å‡» Notebook çš„ **+ Code** å•å…ƒæ ¼ï¼Œç„¶ååœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹ä»£ç ã€‚å•å‡» **â–·
    Run cell** æŒ‰é’®ï¼Œå¹¶è§‚å¯Ÿå®ƒä» æ‚¨ä¹‹å‰åˆ›å»ºçš„ **salesorders**
    è§†å›¾ä¸­è¿”å›æ•°æ®ã€‚

> SqlCopy
>
> %%sql
>
> SELECT \* FROM salesorders

![A screenshot of a computer Description automatically
generated](./media/image84.png)

![A screenshot of a computer Description automatically
generated](./media/image85.png)

2.  åœ¨å•å…ƒæ ¼ä¸‹æ–¹çš„ç»“æœéƒ¨åˆ†ä¸­ï¼Œå°† **View** é€‰é¡¹ä» **Table** æ›´æ”¹ä¸º
    **Chart**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image86.png)

3.  ä½¿ç”¨å›¾è¡¨å³ä¸Šè§’çš„ **View options**
    æŒ‰é’®æ˜¾ç¤ºå›¾è¡¨çš„é€‰é¡¹çª—æ ¼ã€‚ç„¶åæŒ‰å¦‚ä¸‹æ–¹å¼è®¾ç½®é€‰é¡¹å¹¶é€‰æ‹© **Apply** :

    - **Chart type**: Bar chart

    - **Key**: Item

    - **Values**: Quantity

    - **Series Group**:Â *leave blank*

    - **Aggregation**: Sum

    - **Stacked**:Â *Unselected*

![A blue barcode on a white background Description automatically
generated](./media/image87.png)

![A screenshot of a graph Description automatically
generated](./media/image88.png)

4.  éªŒè¯å›¾è¡¨æ˜¯å¦ä¸æ­¤ç±»ä¼¼

![A screenshot of a computer Description automatically
generated](./media/image89.png)

## ä»»åŠ¡ 2ï¼šmatplotlib å…¥é—¨

1.  å•å‡»**+ Code** å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ã€‚ **Run**
    ä»£ç å¹¶è§‚å¯Ÿå®ƒæ˜¯å¦è¿”å›åŒ…å«å¹´æ”¶å…¥çš„ Spark æ•°æ®å¸§ã€‚

> CodeCopy
>
> sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue \\
>
> FROM salesorders \\
>
> GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\
>
> ORDER BY OrderYear"
>
> df_spark = spark.sql(sqlQuery)
>
> df_spark.show()

![A screenshot of a computer Description automatically
generated](./media/image90.png)

![](./media/image91.png)

2.  è¦å°†æ•°æ®å¯è§†åŒ–ä¸ºå›¾è¡¨ï¼Œæˆ‘ä»¬å°†é¦–å…ˆä½¿ç”¨ **matplotlib** Python
    åº“ã€‚è¯¥åº“æ˜¯è®¸å¤šå…¶ä»–åº“æ‰€åŸºäºçš„æ ¸å¿ƒç»˜å›¾åº“ï¼Œå¹¶åœ¨åˆ›å»ºå›¾è¡¨æ—¶æä¾›äº†æå¤§çš„çµæ´»æ€§ã€‚

3.  å•å‡» **+ Code**å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ã€‚

**CodeCopy**

> from matplotlib import pyplot as plt
>
> \# matplotlib requires a Pandas dataframe, not a Spark one
>
> df_sales = df_spark.toPandas()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\])
>
> \# Display the plot
>
> plt.show()

![A screenshot of a computer Description automatically
generated](./media/image92.png)

5.  å•å‡» **Run cell**
    æŒ‰é’®å¹¶æŸ¥çœ‹ç»“æœï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªæŸ±å½¢å›¾ï¼Œå…¶ä¸­åŒ…å«æ¯å¹´çš„æ€»æ”¶å…¥ã€‚è¯·æ³¨æ„ç”¨äºç”Ÿæˆæ­¤å›¾è¡¨çš„ä»£ç çš„ä»¥ä¸‹åŠŸèƒ½:

    - **matplotlib** åº“éœ€è¦ *Pandas* æ•°æ®å¸§ï¼Œå› æ­¤æ‚¨éœ€è¦å°† Spark SQL
      æŸ¥è¯¢è¿”å›çš„ Spark æ•°æ®å¸§è½¬æ¢ä¸ºæ­¤æ ¼å¼ã€‚

    - **matplotlib** åº“çš„æ ¸å¿ƒæ˜¯ **pyplot**
      å¯¹è±¡ã€‚è¿™æ˜¯å¤§å¤šæ•°ç»˜å›¾åŠŸèƒ½çš„åŸºç¡€ã€‚

    - é»˜è®¤è®¾ç½®ä¼šç”Ÿæˆå¯ç”¨çš„å›¾è¡¨ï¼Œä½†æœ‰ç›¸å½“å¤§çš„ç©ºé—´å¯ä»¥å¯¹å…¶è¿›è¡Œè‡ªå®šä¹‰

![A screenshot of a computer screen Description automatically
generated](./media/image93.png)

6.  ä¿®æ”¹ä»£ç ä»¥ç»˜åˆ¶å›¾è¡¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œå°† **Cell** ä¸­çš„æ‰€æœ‰ä»£ç æ›¿æ¢ä¸º
    ä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **â–· Run cell** æŒ‰é’®å¹¶æŸ¥çœ‹è¾“å‡º

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure
>
> plt.show()

![A screenshot of a graph Description automatically
generated](./media/image94.png)

7.  è¯¥å›¾è¡¨ç°åœ¨åŒ…å«æ›´å¤šä¿¡æ¯ã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼Œç»˜å›¾åŒ…å«åœ¨ **Figure**
    ä¸­ã€‚åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­ï¼Œè¯¥å›¾æ˜¯ä¸ºæ‚¨éšå¼åˆ›å»ºçš„;ä½†æ‚¨å¯ä»¥æ˜¾å¼åˆ›å»ºå®ƒã€‚

8.  ä¿®æ”¹ä»£ç ä»¥ç»˜åˆ¶å›¾è¡¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œå°† **cell** ä¸­çš„æ‰€æœ‰ä»£ç æ›¿æ¢ä¸º
    ä»¥ä¸‹ä»£ç ã€‚

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a Figure
>
> fig = plt.figure(figsize=(8,3))
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure
>
> plt.show()

![A screenshot of a computer program Description automatically
generated](./media/image95.png)

9.  **Re-run** ä»£ç å•å…ƒå¹¶æŸ¥çœ‹ç»“æœã€‚è¯¥å›¾ç¡®å®šå›¾çš„å½¢çŠ¶å’Œå¤§å°ã€‚

> ä¸€ä¸ªå›¾çª—å¯ä»¥åŒ…å«å¤šä¸ªå­å›¾ï¼Œæ¯ä¸ªå­å›¾éƒ½æœ‰è‡ªå·±çš„*axis*ã€‚

![A screenshot of a computer Description automatically
generated](./media/image96.png)

10. ä¿®æ”¹ä»£ç ä»¥ç»˜åˆ¶å›¾è¡¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚ **Re-run**
    ä»£ç å•å…ƒå¹¶æŸ¥çœ‹ç»“æœã€‚è¯¥å›¾çª—åŒ…å«ä»£ç ä¸­æŒ‡å®šçš„å­å›¾ã€‚

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a figure for 2 subplots (1 row, 2 columns)
>
> fig, ax = plt.subplots(1, 2, figsize = (10,4))
>
> \# Create a bar plot of revenue by year on the first axis
>
> ax\[0\].bar(x=df_sales\['OrderYear'\],
> height=df_sales\['GrossRevenue'\], color='orange')
>
> ax\[0\].set_title('Revenue by Year')
>
> \# Create a pie chart of yearly order counts on the second axis
>
> yearly_counts = df_sales\['OrderYear'\].value_counts()
>
> ax\[1\].pie(yearly_counts)
>
> ax\[1\].set_title('Orders per Year')
>
> ax\[1\].legend(yearly_counts.keys().tolist())
>
> \# Add a title to the Figure
>
> fig.suptitle('Sales Data')
>
> \# Show the figure
>
> plt.show()

![A screenshot of a computer program Description automatically
generated](./media/image97.png)

![](./media/image98.png)

**æ³¨æ„**ï¼šè¦äº†è§£æœ‰å…³ä½¿ç”¨ matplotlib ç»˜å›¾çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [*matplotlib
documentation*](https://matplotlib.org/).

## ä»»åŠ¡ 3ï¼šä½¿ç”¨ seaborn åº“

è™½ç„¶ **matplotlib**
ä½¿æ‚¨èƒ½å¤Ÿåˆ›å»ºå¤šç§ç±»å‹çš„å¤æ‚å›¾è¡¨ï¼Œä½†å®ƒå¯èƒ½éœ€è¦ä¸€äº›å¤æ‚çš„ä»£ç æ‰èƒ½è·å¾—æœ€ä½³ç»“æœã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œå¤šå¹´æ¥ï¼Œåœ¨
matplotlib
çš„åŸºç¡€ä¸Šæ„å»ºäº†è®¸å¤šæ–°çš„åº“ï¼Œä»¥æŠ½è±¡å…¶å¤æ‚æ€§å¹¶å¢å¼ºå…¶åŠŸèƒ½ã€‚**seaborn**
å°±æ˜¯è¿™æ ·ä¸€ä¸ªåº“ã€‚

1.  å•å‡» **+ Code** å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ã€‚

CodeCopy

> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar chart
>
> ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()

![A screenshot of a graph Description automatically
generated](./media/image99.png)

2.  **Run** ä»£ç å¹¶è§‚å¯Ÿå®ƒæ˜¯å¦ä½¿ç”¨ seaborn åº“æ˜¾ç¤ºæ¡å½¢å›¾ã€‚

![A screenshot of a graph Description automatically
generated](./media/image100.png)

3.  **Modify** ä»£ç å¦‚ä¸‹ã€‚ **Run** ä¿®æ”¹åçš„ä»£ç ï¼Œè¯·æ³¨æ„ï¼Œseaborn
    ä½¿æ‚¨èƒ½å¤Ÿä¸ºç»˜å›¾è®¾ç½®ä¸€è‡´çš„é¢œè‰²ä¸»é¢˜ã€‚

> CodeCopy
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Set the visual theme for seaborn
>
> sns.set_theme(style="whitegrid")
>
> \# Create a bar chart
>
> ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()
>
> ![](./media/image101.png)

4.  å†æ¬¡ **Modify**ä»£ç ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚ **Run**
    ä¿®æ”¹åçš„ä»£ç ä»¥æŠ˜çº¿å›¾çš„å½¢å¼æŸ¥çœ‹å¹´æ”¶å…¥ã€‚

> CodeCopy
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar chart
>
> ax = sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()

![](./media/image102.png)

**æ³¨**ï¼š è¦äº†è§£æœ‰å…³ä½¿ç”¨ seaborn è¿›è¡Œæ‰“å°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§ [*seaborn
documentation*](https://seaborn.pydata.org/index.html)ã€‚

## ä»»åŠ¡ 4ï¼šä½¿ç”¨å¢é‡ tables æµå¼ä¼ è¾“æ•°æ®

Delta Lake æ”¯æŒæµå¼å¤„ç†æ•°æ®ã€‚Delta è¡¨å¯ä»¥æ˜¯ ä½¿ç”¨ Spark Structured
Streaming API
åˆ›å»ºçš„æ•°æ®æµçš„æ¥æ”¶å™¨æˆ–æºã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œä½ å°†ä½¿ç”¨å¢é‡è¡¨ä½œä¸ºæ¨¡æ‹Ÿç‰©è”ç½‘
ï¼ˆIoTï¼‰ åœºæ™¯ä¸­æŸäº›æµæ•°æ®çš„æ¥æ”¶å™¨ã€‚

1.  å•å‡» **+ Code** å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **Run cell** æŒ‰é’®ã€‚

CodeCopy

> from notebookutils import mssparkutils
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import \*
>
> \# Create a folder
>
> inputPath = 'Files/data/'
>
> mssparkutils.fs.mkdirs(inputPath)
>
> \# Create a stream that reads data from the folder, using a JSON
> schema
>
> jsonSchema = StructType(\[
>
> StructField("device", StringType(), False),
>
> StructField("status", StringType(), False)
>
> \])
>
> iotstream =
> spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger",
> 1).json(inputPath)
>
> \# Write some event data to the folder
>
> device_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "data.txt", device_data, True)
>
> print("Source stream created...")

![A screenshot of a computer program Description automatically
generated](./media/image103.png)

![A screenshot of a computer Description automatically
generated](./media/image104.png)

2.  ç¡®ä¿æ¶ˆæ¯ ***Source stream created...***
    è¢«æ‰“å°å‡ºæ¥ã€‚æ‚¨åˆšåˆšè¿è¡Œçš„ä»£ç åŸºäºä¸€ä¸ªæ–‡ä»¶å¤¹åˆ›å»ºäº†ä¸€ä¸ªæµæ•°æ®æºï¼Œè¯¥æ–‡ä»¶å¤¹å·²ä¿å­˜äº†ä¸€äº›æ•°æ®ï¼Œè¡¨ç¤ºæ¥è‡ªå‡è®¾
    IoT è®¾å¤‡çš„è¯»æ•°ã€‚

3.  å•å‡» **+ Code** å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡»è¿è¡Œ **Run cell**
    æŒ‰é’®ã€‚

CodeCopy

> \# Write the stream to a delta table
>
> delta_stream_table_path = 'Tables/iotdevicedata'
>
> checkpointpath = 'Files/delta/checkpoint'
>
> deltastream =
> iotstream.writeStream.format("delta").option("checkpointLocation",
> checkpointpath).start(delta_stream_table_path)
>
> print("Streaming to delta sink...")

![A screenshot of a computer Description automatically
generated](./media/image105.png)

4.  æ­¤ä»£ç å°† delta æ ¼å¼çš„æµå¼å¤„ç†è®¾å¤‡æ•°æ®å†™å…¥åä¸º **iotdevicedata**
    çš„æ–‡ä»¶å¤¹ã€‚ç”±äºæ–‡ä»¶å¤¹ä½ç½®çš„è·¯å¾„ä½äº **Tables**
    æ–‡ä»¶å¤¹ä¸­ï¼Œå› æ­¤å°†è‡ªåŠ¨ä¸ºå…¶åˆ›å»ºä¸€ä¸ªè¡¨ã€‚å•å‡»è¡¨æ ¼æ—è¾¹çš„æ°´å¹³çœç•¥å·ï¼Œç„¶åå•å‡»
    **Refresh**ã€‚

![](./media/image106.png)

![A screenshot of a computer Description automatically
generated](./media/image107.png)

5.  å•å‡» **+ Code** å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **Run cell** æŒ‰é’®ã€‚

> SqlCopy
>
> %%sql
>
> SELECT \* FROM IotDeviceData;

![A screenshot of a computer Description automatically
generated](./media/image108.png)

6.  æ­¤ä»£ç æŸ¥è¯¢ **IotDeviceData** è¡¨ï¼Œå…¶ä¸­åŒ…å«æ¥è‡ªæµå¼å¤„ç†æºçš„è®¾å¤‡æ•°æ®ã€‚

7.  å•å‡» **+ Code** å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **Run cell** æŒ‰é’®ã€‚

> CodeCopy
>
> \# Add more data to the source stream
>
> more_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)

![A screenshot of a computer Description automatically
generated](./media/image109.png)

8.  æ­¤ä»£ç å°†æ›´å¤šå‡è®¾çš„è®¾å¤‡æ•°æ®å†™å…¥æµå¼å¤„ç†æºã€‚

9.  å•å‡» **+ Code** å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **Run Cell** æŒ‰é’®ã€‚

> SqlCopy
>
> %%sql
>
> SELECT \* FROM IotDeviceData;
>
> ![A screenshot of a computer Description automatically
> generated](./media/image110.png)

10. æ­¤ä»£ç å†æ¬¡æŸ¥è¯¢ **IotDeviceData**
    è¡¨ï¼Œè¯¥è¡¨ç°åœ¨åº”åŒ…å«å·²æ·»åŠ åˆ°æµå¼å¤„ç†æºçš„å…¶ä»–æ•°æ®ã€‚

11. å•å‡» **+ Code** å¹¶å¤åˆ¶å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **Run cell** æŒ‰é’®ã€‚

> CodeCopy
>
> deltastream.stop()

![A screenshot of a computer Description automatically
generated](./media/image111.png)

12. æ­¤ä»£ç å°†åœæ­¢æµã€‚

## ä»»åŠ¡ 5ï¼šä¿å­˜ notebook å¹¶ç»“æŸ Spark ä¼šè¯

ç°åœ¨ï¼Œæ‚¨å·²ç»å®Œæˆäº†å¯¹æ•°æ®çš„å¤„ç†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æœ‰æ„ä¹‰çš„åç§°ä¿å­˜ç¬”è®°æœ¬å¹¶ç»“æŸ
Spark ä¼šè¯ã€‚

1.  åœ¨ç¬”è®°æœ¬èœå•æ ä¸­ï¼Œä½¿ç”¨ âš™ï¸ **Settings** å›¾æ ‡æŸ¥çœ‹ç¬”è®°æœ¬è®¾ç½®ã€‚

![A screenshot of a computer Description automatically
generated](./media/image112.png)

2.  å°†ç¬”è®°æœ¬çš„ **Name** è®¾ç½®ä¸º ++**Explore Sales
    Orders**++ï¼Œç„¶åå…³é—­è®¾ç½®çª—æ ¼ã€‚

![A screenshot of a computer Description automatically
generated](./media/image113.png)

3.  åœ¨ç¬”è®°æœ¬èœå•ä¸Šï¼Œé€‰æ‹© **Stop session** ä»¥ç»“æŸ Spark ä¼šè¯ã€‚

![A screenshot of a computer Description automatically
generated](./media/image114.png)

![A screenshot of a computer Description automatically
generated](./media/image115.png)

# ç»ƒä¹  5ï¼šåœ¨ Microsoft Fabric ä¸­åˆ›å»ºæ•°æ®æµï¼ˆç¬¬ 2 ä»£ï¼‰

åœ¨ Microsoft Fabric ä¸­ï¼Œæ•°æ®æµ ï¼ˆGen2ï¼‰ è¿æ¥åˆ°å„ç§æ•°æ®æºï¼Œå¹¶åœ¨ Power
Query Online ä¸­æ‰§è¡Œè½¬æ¢ã€‚ç„¶åï¼Œå¯ä»¥åœ¨ Data Pipelines
ä¸­ä½¿ç”¨å®ƒä»¬å°†æ•°æ®å¼•å…¥ Lakehouse æˆ–å…¶ä»–åˆ†æå­˜å‚¨ï¼Œæˆ–ä¸º Power BI
æŠ¥è¡¨å®šä¹‰æ•°æ®é›†ã€‚

æœ¬ç»ƒä¹ æ—¨åœ¨ä»‹ç» Dataflows ï¼ˆGen2ï¼‰
çš„ä¸åŒå…ƒç´ ï¼Œè€Œä¸æ˜¯åˆ›å»ºä¼ä¸šä¸­å¯èƒ½å­˜åœ¨çš„å¤æ‚è§£å†³æ–¹æ¡ˆ

## ä»»åŠ¡ 1ï¼šåˆ›å»º Dataflow ï¼ˆGen2ï¼‰ ä»¥æ‘„å–æ•°æ®

ç°åœ¨ï¼Œæ‚¨æœ‰ä¸€ä¸ª
Lakehouseï¼Œæ‚¨éœ€è¦å°†ä¸€äº›æ•°æ®æå–åˆ°å…¶ä¸­ã€‚å®ç°æ­¤ç›®çš„çš„ä¸€ç§æ–¹æ³•æ˜¯å®šä¹‰å°è£…*æå–ã€è½¬æ¢å’ŒåŠ è½½*
ï¼ˆETLï¼‰ æµç¨‹çš„æ•°æ®æµã€‚

1.  ç°åœ¨ï¼Œå•å‡» å·¦ä¾§å¯¼èˆªçª—æ ¼ä¸­çš„ **Fabric_lakehouse**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image116.png)

2.  åœ¨ **Fabric_lakehouse** ä¸»é¡µä¸­ï¼Œå•å‡» **Get data** ï¼Œç„¶åé€‰æ‹© **New
    Dataflow Gen2ã€‚** æ­¤æ—¶å°†æ‰“å¼€æ–°æ•°æ®æµçš„ Power Query ç¼–è¾‘å™¨ã€‚

![](./media/image117.png)

3.  åœ¨ **Power Query** çª—æ ¼ä¸­çš„ **Home tab** ä¸‹ï¼Œå•å‡» **Import from a
    Text/CSV file**ã€‚

![](./media/image118.png)

4.  åœ¨ **Connect to data source** çª—æ ¼çš„ **Connection settings**
    ä¸‹ï¼Œé€‰æ‹© **Link to file ï¼ˆpreviewï¼‰**å•é€‰æŒ‰é’®

- **Link to file**:Â *Selected*

- **File path or
  URL**:Â [https://raw.githubusercontent.com/MicrosoftLearning/dp-
  data/main/orders.csv](https://raw.githubusercontent.com/MicrosoftLearning/dp-%20%20data/main/orders.csv)

![](./media/image119.png)

5.  åœ¨ **Connect to data source** çª—æ ¼çš„ **Connection credentials**
    ä¸‹ï¼Œè¾“å…¥ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯ï¼Œç„¶åå•å‡» **Next** æŒ‰é’®ã€‚

    - **Connection**: Create new connection

    - **data gateway**: (none)

    - **Authentication kind**: Organizational account

![](./media/image120.png)

6.  åœ¨ **Preview file data** çª—æ ¼ä¸­ï¼Œå•å‡» **Create** ä»¥åˆ›å»ºæ•°æ®æºã€‚ ![A
    screenshot of a computer Description automatically
    generated](./media/image121.png)

7.  **Power Query**
    ç¼–è¾‘å™¨æ˜¾ç¤ºæ•°æ®æºå’Œä¸€ç»„ç”¨äºè®¾ç½®æ•°æ®æ ¼å¼çš„åˆå§‹æŸ¥è¯¢æ­¥éª¤ã€‚

![A screenshot of a computer Description automatically
generated](./media/image122.png)

8.  åœ¨å·¥å…·æ åŠŸèƒ½åŒºä¸Šï¼Œé€‰æ‹© **Add column** é€‰é¡¹å¡ã€‚ç„¶åï¼Œé€‰æ‹© **Custom
    columnã€‚**

![A screenshot of a computer Description automatically
generated](./media/image123.png)Â 

9.  å°† æ–°åˆ—åç§° è®¾ç½®ä¸º **MonthNo** ï¼Œå°† æ•°æ®ç±»å‹ è®¾ç½®ä¸º Whole
    Numberï¼Œç„¶ååœ¨ **Custom column formula** ä¸‹
    æ·»åŠ ä»¥ä¸‹å…¬å¼ï¼š**Date.Month(\[OrderDate\])** ã€‚é€‰æ‹© **OKã€‚**

![A screenshot of a computer Description automatically
generated](./media/image124.png)

10. è¯·æ³¨æ„æ·»åŠ è‡ªå®šä¹‰åˆ—çš„æ­¥éª¤æ˜¯å¦‚ä½•æ·»åŠ åˆ°æŸ¥è¯¢ä¸­çš„ã€‚ç»“æœåˆ—å°†æ˜¾ç¤ºåœ¨æ•°æ®çª—æ ¼ä¸­ã€‚

![A screenshot of a computer Description automatically
generated](./media/image125.png)

**æç¤ºï¼š**åœ¨å³ä¾§çš„ Query Settings çª—æ ¼ä¸­ï¼Œè¯·æ³¨æ„ **Applied Steps**
åŒ…æ‹¬æ¯ä¸ªè½¬æ¢æ­¥éª¤ã€‚åœ¨åº•éƒ¨ï¼Œæ‚¨è¿˜å¯ä»¥åˆ‡æ¢ **Diagram flow** æŒ‰é’®ä»¥æ‰“å¼€æ­¥éª¤çš„
Visual Diagramï¼ˆå¯è§†åŒ–å›¾è¡¨ï¼‰ã€‚

å¯ä»¥å‘ä¸Šæˆ–å‘ä¸‹ç§»åŠ¨æ­¥éª¤ï¼Œé€šè¿‡é€‰æ‹©é½¿è½®å›¾æ ‡è¿›è¡Œç¼–è¾‘ï¼Œå¹¶ä¸”æ‚¨å¯ä»¥é€‰æ‹©æ¯ä¸ªæ­¥éª¤ä»¥åœ¨é¢„è§ˆçª—æ ¼ä¸­æŸ¥çœ‹åº”ç”¨çš„è½¬æ¢ã€‚

ä»»åŠ¡ 2ï¼šä¸º Dataflow æ·»åŠ æ•°æ®ç›®æ ‡

1.  åœ¨ **Power Query** å·¥å…·æ åŠŸèƒ½åŒºä¸Šï¼Œé€‰æ‹© **Home** é€‰é¡¹å¡ã€‚ç„¶åï¼Œåœ¨
    Data destination ä¸‹æ‹‰èœå•ä¸­ï¼Œé€‰æ‹© **Lakehouse**ï¼ˆå¦‚æœå°šæœªé€‰æ‹©ï¼‰ã€‚

![](./media/image126.png)

![A screenshot of a computer Description automatically
generated](./media/image127.png)

**æ³¨æ„ï¼š**å¦‚æœæ­¤é€‰é¡¹ç°æ˜¾ï¼Œåˆ™æ‚¨å¯èƒ½å·²ç»è®¾ç½®äº†æ•°æ®ç›®æ ‡ã€‚æ£€æŸ¥ Power Query
ç¼–è¾‘å™¨å³ä¾§ Query settings ï¼ˆæŸ¥è¯¢è®¾ç½®ï¼‰
çª—æ ¼åº•éƒ¨çš„æ•°æ®ç›®æ ‡ã€‚å¦‚æœå·²è®¾ç½®ç›®æ ‡ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ é½¿è½® æ›´æ”¹å®ƒã€‚

2.  å•å‡» æ‰€é€‰ **Lakehouse** é€‰é¡¹æ—è¾¹çš„ **Settings** å›¾æ ‡ã€‚

![A screenshot of a computer Description automatically
generated](./media/image128.png)

3.  åœ¨ **Connect to data destination** å¯¹è¯æ¡†ä¸­ï¼Œé€‰æ‹© **Edit
    connectionã€‚**

![A screenshot of a computer Description automatically
generated](./media/image129.png)

4.  åœ¨ **Connect to data destination** å¯¹è¯æ¡†ä¸­ï¼Œé€‰æ‹© **Sign in**
    ä½¿ç”¨æ‚¨çš„ Power BI ç»„ç»‡å¸æˆ·ç™»å½• ,
    ä»¥è®¾ç½®æ•°æ®æµç”¨äºè®¿é—®æ¹–ä»“ä¸€ä½“çš„èº«ä»½ã€‚

![A screenshot of a computer Description automatically
generated](./media/image130.png)

![A screenshot of a computer Description automatically
generated](./media/image131.png)

5.  åœ¨ Connect to data destination å¯¹è¯æ¡†ä¸­ï¼Œé€‰æ‹© **Next**

![A screenshot of a computer Description automatically
generated](./media/image132.png)

6.  åœ¨ **Connect to data destination** å¯¹è¯æ¡†ä¸­ï¼Œé€‰æ‹© **New
    table**ã€‚å•å‡» **Lakehouse folder** ï¼Œé€‰æ‹©æ‚¨çš„å·¥ä½œåŒº â€“
    **dp_FabricXX** ç„¶åé€‰æ‹©æ‚¨çš„ Lakehouseï¼Œå³ **Fabric_lakehouseã€‚**
    ç„¶åå°† Table name æŒ‡å®šä¸º **orders** å¹¶é€‰æ‹© **Next** æŒ‰é’®ã€‚

![A screenshot of a computer Description automatically
generated](./media/image133.png)

7.  åœ¨ **Choose destination settings** å¯¹è¯æ¡†çš„ **Use automatic settings
    off** å’Œ **Update method** ä¸‹ï¼Œé€‰æ‹© **Append** ï¼Œç„¶åå•å‡» **Save
    settings** æŒ‰é’®ã€‚

![](./media/image134.png)

8.  **Lakehouse** ç›®æ ‡åœ¨ **Power Query** ç¼–è¾‘å™¨çš„ **query** ä¸­æŒ‡ç¤ºä¸º
    **iconã€‚**

![A screenshot of a computer Description automatically
generated](./media/image135.png)

![A screenshot of a computer Description automatically
generated](./media/image136.png)

9.  é€‰æ‹© **PublishÂ ** ä»¥å‘å¸ƒæ•°æ®æµã€‚ç„¶åç­‰å¾… **Dataflow 1**
    æ•°æ®æµåœ¨æ‚¨çš„å·¥ä½œåŒºä¸­åˆ›å»ºã€‚

![A screenshot of a computer Description automatically
generated](./media/image137.png)

10. å‘å¸ƒåï¼Œæ‚¨å¯ä»¥å³é”®å•å‡»å·¥ä½œåŒºä¸­çš„æ•°æ®æµï¼Œé€‰æ‹©
    **Properties**ï¼Œç„¶åé‡å‘½åæ•°æ®æµã€‚

![A screenshot of a computer Description automatically
generated](./media/image138.png)

11. åœ¨ **Dataflow1** å¯¹è¯æ¡†ä¸­ï¼Œè¾“å…¥ **Name** ä½œä¸º **Gen2_Dataflow**
    ç„¶åå•å‡» **Save** æŒ‰é’®ã€‚

![A screenshot of a computer Description automatically
generated](./media/image139.png)

![](./media/image140.png)

## ä»»åŠ¡ 3ï¼šå°†æ•°æ®æµæ·»åŠ åˆ°ç®¡é“

æ‚¨å¯ä»¥å°†æ•°æ®æµä½œä¸ºæ´»åŠ¨åŒ…å«åœ¨ç®¡é“ä¸­ã€‚ç®¡é“ç”¨äºç¼–æ’æ•°æ®æ‘„å–å’Œå¤„ç†æ´»åŠ¨ï¼Œä½¿æ‚¨èƒ½å¤Ÿåœ¨å•ä¸ªè®¡åˆ’æµç¨‹ä¸­å°†æ•°æ®æµä¸å…¶ä»–ç±»å‹çš„ä½œç›¸ç»“åˆã€‚å¯ä»¥åœ¨å‡ ç§ä¸åŒçš„ä½“éªŒä¸­åˆ›å»ºç®¡é“ï¼ŒåŒ…æ‹¬æ•°æ®å·¥å‚ä½“éªŒã€‚

1.  åœ¨ Synapse æ•°æ®å·¥ç¨‹ä¸»é¡µçš„ **dp_FabricXX** çª—æ ¼ä¸‹ï¼Œé€‰æ‹© **+New -\>
    Data pipeline**

![](./media/image141.png)

2.  åœ¨ **New pipeline** å¯¹è¯æ¡†ä¸­ï¼Œåœ¨ **Name** å­—æ®µä¸­è¾“å…¥ **Load data**
    ï¼Œç„¶åå•å‡» **Create** æŒ‰é’®ä»¥æ‰“å¼€æ–°ç®¡é“ã€‚

![A screenshot of a computer Description automatically
generated](./media/image142.png)

3.  ç®¡é“ç¼–è¾‘å™¨éšå³æ‰“å¼€ã€‚

![A screenshot of a computer Description automatically
generated](./media/image143.png)

> **æç¤º**ï¼š å¦‚æœ Copy Data å‘å¯¼è‡ªåŠ¨æ‰“å¼€ï¼Œè¯·å°†å…¶å…³é—­ï¼

4.  é€‰æ‹© **Pipeline activityï¼Œ**ç„¶åå‘ç®¡é“ä¸­æ·»åŠ  **Dataflow** æ´»åŠ¨ã€‚

![](./media/image144.png)

5.  é€‰æ‹©æ–°çš„ **Dataflow1** æ´»åŠ¨åï¼Œåœ¨ **Settings** é€‰é¡¹å¡çš„ **Dataflow**
    ä¸‹æ‹‰åˆ—è¡¨ä¸­ï¼Œé€‰æ‹© **Gen2_Dataflow** ï¼ˆæ‚¨ä¹‹å‰åˆ›å»ºçš„æ•°æ®æµï¼‰

![](./media/image145.png)

6.  åœ¨ **Home** é€‰é¡¹å¡ä¸Šï¼Œä½¿ç”¨ **ğŸ–« ï¼ˆ*Save*ï¼‰** å›¾æ ‡ä¿å­˜ç®¡é“ã€‚

![A screenshot of a computer Description automatically
generated](./media/image146.png)

7.  ä½¿ç”¨ **â–· Run** æŒ‰é’®è¿è¡Œç®¡é“ï¼Œå¹¶ç­‰å¾…å…¶å®Œæˆã€‚è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ã€‚

> ![A screenshot of a computer Description automatically
> generated](./media/image147.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image148.png)

![A screenshot of a computer Description automatically
generated](./media/image149.png)

8.  åœ¨å·¦è¾¹ç¼˜çš„èœå•æ ä¸­ï¼Œé€‰æ‹©æ‚¨çš„å·¥ä½œåŒºï¼Œå³ **dp_FabricXX**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image150.png)

9.  åœ¨ **Fabric_lakehouse** çª—æ ¼ä¸­ï¼Œé€‰æ‹© Lakehouse
    ç±»å‹çš„**Gen2_FabricLakehouse**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image151.png)

12. åœ¨ **Explorer** çª—æ ¼ä¸­ï¼Œé€‰æ‹© **...** èœå•ï¼Œé€‰æ‹©
    **Refresh**ã€‚ç„¶åå±•å¼€ **Tables** å¹¶é€‰æ‹© **orders** è¡¨ï¼Œè¯¥è¡¨å·²ç”±
    **dataflow**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image152.png)

![](./media/image153.png)

**æç¤º**ï¼š ä½¿ç”¨ Power BI Desktop
*æ•°æ®æµè¿æ¥å™¨*ç›´æ¥è¿æ¥åˆ°é€šè¿‡æ•°æ®æµå®Œæˆçš„æ•°æ®è½¬æ¢ã€‚

æ‚¨è¿˜å¯ä»¥è¿›è¡Œå…¶ä»–è½¬æ¢ï¼Œå‘å¸ƒä¸ºæ–°æ•°æ®é›†ï¼Œå¹¶ä¸ä¸“ç”¨æ•°æ®é›†çš„ç›®æ ‡å—ä¼—ä¸€èµ·åˆ†å‘ã€‚

## ä»»åŠ¡ 4ï¼šæ¸…ç†èµ„æº

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæ‚¨å­¦ä¹ äº†å¦‚ä½•ä½¿ç”¨ Spark å¤„ç† Microsoft Fabric ä¸­çš„æ•°æ®ã€‚

å¦‚æœæ‚¨å·²å®Œæˆå¯¹ Lakehouse çš„æ¢ç´¢ï¼Œåˆ™å¯ä»¥åˆ é™¤ä¸ºæœ¬ç»ƒä¹ åˆ›å»ºçš„å·¥ä½œåŒºã€‚

1.  åœ¨å·¦ä¾§çš„æ ä¸­ï¼Œé€‰æ‹©å·¥ä½œåŒºçš„å›¾æ ‡ä»¥æŸ¥çœ‹å…¶åŒ…å«çš„æ‰€æœ‰é¡¹ç›®ã€‚

> ![A screenshot of a computer Description automatically
> generated](./media/image154.png)

2.  åœ¨ **...** èœå•ä¸­ï¼Œé€‰æ‹© **Workspace settings**ã€‚

![](./media/image155.png)

3.  é€‰æ‹© **General** å¹¶å•å‡» **Remove this workspaceã€‚**

![A screenshot of a computer settings Description automatically
generated](./media/image156.png)

4.  åœ¨ **Delete workspace?** å¯¹è¯æ¡†ä¸­ï¼Œå•å‡» **Delete** æŒ‰é’®ã€‚

> ![A screenshot of a computer Description automatically
> generated](./media/image157.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image158.png)

**æ€»ç»“**

æ­¤ ç”¨ä¾‹ å°†æŒ‡å¯¼æ‚¨å®Œæˆåœ¨ Power BI ä¸­ä½¿ç”¨ Microsoft Fabric
çš„è¿‡ç¨‹ã€‚å®ƒæ¶µç›–å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬è®¾ç½®å·¥ä½œåŒºã€åˆ›å»ºæ¹–ä»“ä¸€ä½“ã€ä¸Šä¼ å’Œç®¡ç†æ•°æ®æ–‡ä»¶ä»¥åŠä½¿ç”¨
Notebook è¿›è¡Œæ•°æ®æ¢ç´¢ã€‚å‚ä¸è€…å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨
PySparkä½œå’Œè½¬æ¢æ•°æ®ã€åˆ›å»ºå¯è§†åŒ–ä»¥åŠä¿å­˜å’Œåˆ†åŒºæ•°æ®ä»¥å®ç°é«˜æ•ˆæŸ¥è¯¢ã€‚

åœ¨æ­¤ç”¨ä¾‹ä¸­ï¼Œå‚ä¸è€…å°†å‚ä¸ä¸€ç³»åˆ—ä»»åŠ¡ï¼Œé‡ç‚¹æ˜¯åœ¨ Microsoft Fabric
ä¸­ä½¿ç”¨å¢é‡è¡¨ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬ä¸Šä¼ å’Œæ¢ç´¢æ•°æ®ã€åˆ›å»ºæ‰˜ç®¡å’Œå¤–éƒ¨å¢é‡è¡¨ã€æ¯”è¾ƒå®ƒä»¬çš„å±æ€§ï¼Œè¯¥å®éªŒå®¤ä»‹ç»äº†ç”¨äºç®¡ç†ç»“æ„åŒ–æ•°æ®çš„
SQL åŠŸèƒ½ï¼Œå¹¶ä½¿ç”¨ matplotlib å’Œ seaborn ç­‰ Python
åº“æä¾›æœ‰å…³æ•°æ®å¯è§†åŒ–çš„è§è§£ã€‚è¿™äº›ç»ƒä¹ æ—¨åœ¨å…¨é¢äº†è§£å¦‚ä½•åˆ©ç”¨ Microsoft
Fabric è¿›è¡Œæ•°æ®åˆ†æï¼Œä»¥åŠåœ¨ IoT ä¸Šä¸‹æ–‡ä¸­åˆå¹¶å¢é‡è¡¨ä»¥æµå¼ä¼ è¾“æ•°æ®ã€‚

æ­¤ä½¿ç”¨æ¡ˆä¾‹å°†æŒ‡å¯¼æ‚¨å®Œæˆè®¾ç½® Fabric
å·¥ä½œåŒºã€åˆ›å»ºæ•°æ®æ¹–ä»“ä¸€ä½“å’Œæ‘„å–æ•°æ®è¿›è¡Œåˆ†æçš„è¿‡ç¨‹ã€‚å®ƒæ¼”ç¤ºäº†å¦‚ä½•å®šä¹‰æ•°æ®æµæ¥å¤„ç†
ETLä½œå¹¶é…ç½®æ•°æ®ç›®æ ‡ä»¥å­˜å‚¨è½¬æ¢åçš„æ•°æ®ã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜å°†å­¦ä¹ å¦‚ä½•å°†æ•°æ®æµé›†æˆåˆ°ç®¡é“ä¸­ä»¥è¿›è¡Œè‡ªåŠ¨åŒ–å¤„ç†ã€‚æœ€åï¼Œæ‚¨å°†è·å¾—åœ¨ç»ƒä¹ å®Œæˆåæ¸…ç†èµ„æºçš„è¯´æ˜ã€‚

æ­¤å®éªŒå®¤ä¸ºæ‚¨æä¾›ä½¿ç”¨ Fabric
çš„åŸºæœ¬æŠ€èƒ½ï¼Œä½¿æ‚¨èƒ½å¤Ÿåˆ›å»ºå’Œç®¡ç†å·¥ä½œåŒºã€å»ºç«‹æ•°æ®æ¹–ä»“ä¸€ä½“ä»¥åŠé«˜æ•ˆæ‰§è¡Œæ•°æ®è½¬æ¢ã€‚é€šè¿‡å°†æ•°æ®æµæ•´åˆåˆ°ç®¡é“ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•è‡ªåŠ¨æ‰§è¡Œæ•°æ®å¤„ç†ä»»åŠ¡ã€ç®€åŒ–å·¥ä½œæµç¨‹å¹¶æé«˜å®é™…åœºæ™¯ä¸­çš„ç”Ÿäº§åŠ›ã€‚æ¸…ç†è¯´æ˜å¯ç¡®ä¿æ‚¨ä¸ä¼šç•™ä¸‹ä¸å¿…è¦çš„èµ„æºï¼Œä»è€Œä¿ƒè¿›æœ‰åºä¸”é«˜æ•ˆçš„å·¥ä½œåŒºç®¡ç†æ–¹æ³•ã€‚
