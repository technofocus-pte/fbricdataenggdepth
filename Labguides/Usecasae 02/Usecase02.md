**Introducci√≥n**

Apache Spark es un motor de c√≥digo abierto para el procesamiento de
datos distribuidos, y se utiliza ampliamente para explorar, procesar y
analizar enormes vol√∫menes de datos en el almacenamiento de lagos de
datos. Spark est√° disponible como opci√≥n de procesamiento en muchos
productos de plataformas de datos, como Azure HDInsight, Azure
Databricks, Azure Synapse Analytics y Microsoft Fabric. Una de las
ventajas de Spark es su compatibilidad con una amplia gama de lenguajes
de programaci√≥n, como Java, Scala, Python y SQL; lo que convierte a
Spark en una soluci√≥n muy flexible para cargas de trabajo de
procesamiento de datos, como la limpieza y manipulaci√≥n de datos, el
an√°lisis estad√≠stico y el aprendizaje autom√°tico, y el an√°lisis y la
visualizaci√≥n de datos.

Las tablas de un lakehouse de Microsoft Fabric se basan en el formato de
c√≥digo abierto¬†*Delta*¬†Lake para Apache Spark. Delta Lake a√±ade
compatibilidad con la sem√°ntica relacional tanto para las operaciones de
datos por lotes como en flujo, y permite la creaci√≥n de una arquitectura
Lakehouse en la que Apache Spark puede utilizarse para procesar y
consultar datos en tablas que se basan en archivos subyacentes en un
lago de datos.

En Microsoft Fabric, los Dataflows (Gen2) se conectan a varias fuentes
de datos y realizan transformaciones en Power Query Online. A
continuaci√≥n, se pueden utilizar en Data Pipelines para ingerir datos en
un lakehouse u otro almac√©n anal√≠tico, o para definir un conjunto de
datos para un informe de Power BI.

Este laboratorio est√° dise√±ado para introducir los diferentes elementos
de Dataflow (Gen2), y no para crear una soluci√≥n compleja que pueda
existir en una empresa.

**Objetivos**:

- Crear un workspace en Microsoft Fabric con la prueba de Fabric
  activada.

- Establecer un entorno lakehouse y cargar archivos de datos para su
  an√°lisis.

- Generar un notebook para la exploraci√≥n y el an√°lisis interactivo de
  datos.

- Cargar los datos en un marco de datos para su posterior procesamiento
  y visualizaci√≥n.

- Aplicar transformaciones a los datos utilizando PySpark.

- Guardar y particionar los datos transformados para una consulta
  optimizada.

- Crear una tabla en el metastore de Spark para la gesti√≥n de datos
  estructurados¬†

- Guardar DataFrame como una tabla delta gestionada llamada
  "salesorders".

- Guardar DataFrame como una tabla delta externa denominada
  "external_salesorder" con una ruta especificada.

- Describir y comparar las propiedades de las tablas gestionadas y
  externas.

- Ejecutar consultas SQL en tablas para an√°lisis e informes.

- Visualizar datos utilizando librer√≠as Python como matplotlib y
  seaborn.

- Establecer un lakehouse de datos en la experiencia de Ingenier√≠a de
  Datos e ingerir datos relevantes para su posterior an√°lisis.

- Definir un dataflow para extraer, transformar y cargar datos en el
  lakehouse.

- Configurar destinos de datos dentro de Power Query para almacenar los
  datos transformados en el lakehouse.

- Incorporar el dataflow en un pipeline para permitir el procesamiento e
  ingesta de datos programados.

- Eliminar el workspace y los elementos asociados para concluir el
  ejercicio.

# Ejercicio 1: Crear un workspace, lakehouse, notebook y cargar datos en dataframe 

## Tarea 1: Creaci√≥n de un workspace 

Antes de trabajar con datos en Fabric, cree un workspace con la prueba
de Fabric activada.

1.  Abra su navegador, vaya a la barra de direcciones y escriba o pegue
    la siguiente URL: <https://app.fabric.microsoft.com/> y despu√©s
    presione la tecla **Enter**.

> **Nota**: Si se le dirige a la p√°gina de inicio de Microsoft Fabric,
> omita los pasos del \#2 al \#4.
>
> ![](./media/image1.png)

2.  En la ventana¬†**Microsoft Fabric**, ingrese sus credenciales y haga
    clic en el bot√≥n¬†**Submit**.

> ![](./media/image2.png)

3.  A continuaci√≥n, en la ventana de¬†**Microsoft¬†**, escriba la
    contrase√±a y haga clic en el bot√≥n¬†**Sign in.**

> ![A login screen with a red box and blue text Description
> automatically generated](./media/image3.png)

4.  En la ventana **Stay signed in?** Haga clic en el bot√≥n **Yes**.

> ![A screenshot of a computer error Description automatically
> generated](./media/image4.png)

5.  En la p√°gina de inicio de Fabric, seleccione el mosaico¬†**+New
    workspace**.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image5.png)

6.  En la pesta√±a **Create a workspace**, ingrese los siguientes datos y
    haga clic en el bot√≥n **Apply**.

[TABLE]

> ![A screenshot of a computer Description automatically
> generated](./media/image6.png)
>
> ![](./media/image7.png)

![](./media/image8.png)

![](./media/image9.png)

7.  Espere a que se complete la implementaci√≥n. Tardar√° de 2 a 3 minutos
    en completarse. Cuando se abra su nuevo workspace, deber√≠a estar
    vac√≠o.

## Tarea 2: Creaci√≥n de un lakehouse y carga de archivos

Ahora que ya tiene un workspace, es el momento de cambiar a la
experiencia de Data engineering en el portal y crear un data lakehouse
para los archivos de datos que va a analizar.

1.  Cree un nuevo Eventhouse haciendo clic en el bot√≥n **+New item** en
    la barra de navegaci√≥n.

![A screenshot of a browser AI-generated content may be
incorrect.](./media/image10.png)

2.  Haga clic en el mosaico "**Lakehouse**".

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image11.png)

3.  En el cuadro de di√°logo¬†**New lakehouse**,
    ingrese¬†**+++Fabric_lakehouse+++**¬†en el campo¬†**Name**, haga clic
    en el bot√≥n¬†**Create** y abra el nuevo lakehouse.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image12.png)

4.  Al cabo de un minuto aproximadamente, se crear√° un nuevo lakehouse
    vac√≠o. Necesita ingerir algunos datos en el data lakehouse para su
    an√°lisis.

![](./media/image13.png)

5.  Ver√° una notificaci√≥n que indica **Successfully created SQL
    endpoint**.

![](./media/image14.png)

6.  En la secci√≥n **Explorer**, debajo de **fabric_lakehouse**, pase el
    mouse por encima de la **carpeta Files**, luego haga clic en el men√∫
    de elipses horizontales **(‚Ä¶)**. Navegue y haga clic en **Upload** y
    luego haga clic en **Upload folder** como se muestra en la siguiente
    imagen.

![](./media/image15.png)

7.  En el de la carpeta **Upload folder** que aparece en ell ado
    derecho, seleccione el **icono de la carpeta** debajo de **Files/**
    y luego busque **C:\LabFiles** y seleccione la carpeta **orders** y
    haga clic en el bot√≥n **Upload**.

![](./media/image16.png)

8.  En caso de que aparezca el cuadro de di√°logo **Upload 3 files to
    this site?** haga clic en el bot√≥n **Upload**.

![](./media/image17.png)

9.  En el panel Upload folder, haga clic en el bot√≥n **Upload**.

> ![](./media/image18.png)

10. Una vez cargados los archivos, **cierre** el panel **Upload
    folder**.

> ![](./media/image19.png)

11. Expanda¬†**Files**¬†y seleccione la carpeta¬†**orders**¬†y verifique que
    los archivos CSV han sido cargados.

![](./media/image20.png)

## Tarea 3: Creaci√≥n de un notebook

Para trabajar con datos en Apache Spark, puede crear un notebook. Los
notebooks proporcionan un entorno interactivo en el que puede escribir y
ejecutar c√≥digo (en varios idiomas), y a√±adir notas para documentarlo.

1.  En la p√°gina¬†**Home**¬†mientras visualiza el contenido de la
    carpeta¬†**orders**¬†en su datalake, en el men√∫ **Open notebook**,
    seleccione¬†**New notebook**.

![](./media/image21.png)

2.  Al cabo de unos segundos, se abrir√° un nuevo notebook que contiene
    una √∫nica *celda*. Los notebooks est√°n formados por una o m√°s celdas
    que pueden contener *c√≥digo* o *markdown* (texto formateado).

![](./media/image22.png)

3.  Seleccione la primera celda (que actualmente es una celda
    de¬†*c√≥digo*) y, a continuaci√≥n, en la barra de herramientas din√°mica
    situada en la parte superior derecha, utilice el
    bot√≥n¬†**M‚Üì**¬†para¬†**convertir la celda en una celda markdown**.

![](./media/image23.png)

4.  Cuando la celda cambia a una celda markdown, el texto que contiene
    se renderiza.

![](./media/image24.png)

5.  Utilice el bot√≥n¬†**üñâ**¬†(Editar) para cambiar la celda al modo de
    edici√≥n, sustituya todo el texto y, a continuaci√≥n, modifique el
    markdown de la siguiente manera:

> CodeCopy
>
> \# Sales order data exploration
>
> Use the code in this notebook to explore sales order data.

![](./media/image25.png)

![A screenshot of a computer Description automatically
generated](./media/image26.png)

6.  Haga clic en cualquier parte del notebook fuera de la celda para
    dejar de editarlo y ver el markdown renderizado.

![A screenshot of a computer Description automatically
generated](./media/image27.png)

## Tarea 4: Carga de datos en un DataFrame

Ahora est√° listo para ejecutar c√≥digo que cargue los datos en un
dataframe. Los dataframes en Spark son similares a los dataframes de
Pandas en Python, y proporcionan una estructura com√∫n para trabajar con
datos en filas y columnas.

**Nota**: Spark admite varios lenguajes de codificaci√≥n, como Scala,
Java y otros. En este ejercicio, utilizaremos PySpark, que es una
variante de Python optimizada para Spark. PySpark es uno de los
lenguajes m√°s utilizados en Spark y es el lenguaje por defecto en los
notebooks de Fabric.

1.  Con el notebook visible, expanda la lista¬†**Files**¬†y seleccione la
    carpeta de¬†**orders**¬†para que los archivos CSV aparezcan en la
    lista junto al editor del notebook.

![](./media/image28.png)

2.  Ahora, sin embargo su rat√≥n al archivo 2019.csv. Haga clic en las
    elipses horizontales **(...)** junto a 2019.csv. Navegue y haga clic
    en **Load data**, luego seleccione **Spark**. Se a√±adir√° al notebook
    una nueva celda de c√≥digo que contiene el siguiente c√≥digo:

> CodeCopy
>
> df =
> spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
>
> \# df now is a Spark DataFrame containing CSV data from
> "Files/orders/2019.csv".
>
> display(df)

![](./media/image29.png)

![](./media/image30.png)

**Consejo**: Puede ocultar los paneles del explorador de Lakehouse
situados a la izquierda utilizando sus iconos¬†**¬´**. Hacerlo¬†le ayudar√°
a centrarse en el notebook.

3.  Utilice el bot√≥n¬†**‚ñ∑ Run cell**¬†situado a la izquierda de la celda
    para ejecutarla.

![](./media/image31.png)

**Nota**: Dado que es la primera vez que ejecuta c√≥digo Spark, debe
iniciarse una sesi√≥n Spark. Esto significa que la primera ejecuci√≥n en
la sesi√≥n puede tardar m√°s o menos un minuto en completarse. Las
ejecuciones posteriores ser√°n m√°s r√°pidas.

4.  Cuando el comando de la celda se haya completado, revise la salida
    debajo de la celda, que deber√≠a tener un aspecto similar a este:

![](./media/image32.png)

5.  La salida muestra las filas y columnas de datos del archivo
    2019.csv. Sin embargo, observe que las cabeceras de las columnas no
    tienen el aspecto correcto. El c√≥digo utilizado por defecto para
    cargar los datos en un dataframe asume que el archivo CSV incluye
    los nombres de las columnas en la primera fila, pero en este caso el
    archivo CSV s√≥lo incluye los datos sin informaci√≥n de cabecera.

6.  Modifique el c√≥digo para establecer la opci√≥n **header**¬†en
    **false**. Sustituya todo el c√≥digo de la **celda** por el siguiente
    c√≥digo y haga clic en el bot√≥n **‚ñ∑ Run cell**¬†y revisa la salida.

> CodeCopy
>
> df =
> spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
>
> \# df now is a Spark DataFrame containing CSV data from
> "Files/orders/2019.csv".
>
> display(df)
>
> ![](./media/image33.png)

7.  Ahora el dataframe incluye correctamente la primera fila como
    valores de datos, pero los nombres de las columnas se autogeneran y
    no son muy √∫tiles. Para que los datos tengan sentido, es necesario
    definir expl√≠citamente el esquema y el tipo de datos correctos para
    los valores de los datos en el archivo.

8.  Sustituya todo el c√≥digo de la celda **cell** por el siguiente
    c√≥digo y haga clic en el bot√≥n **‚ñ∑ Run cell**¬†y revise la salida.

> CodeCopy
>
> from pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> StructField("SalesOrderNumber", StringType()),
>
> StructField("SalesOrderLineNumber", IntegerType()),
>
> StructField("OrderDate", DateType()),
>
> StructField("CustomerName", StringType()),
>
> StructField("Email", StringType()),
>
> StructField("Item", StringType()),
>
> StructField("Quantity", IntegerType()),
>
> StructField("UnitPrice", FloatType()),
>
> StructField("Tax", FloatType())
>
> \])
>
> df =
> spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")
>
> display(df)

![](./media/image34.png)

> ![](./media/image35.png)

9.  Ahora el dataframe incluye los nombres de columna correctos (adem√°s
    del¬†**Index**, que es una columna incorporada en todos los
    dataframes basada en la posici√≥n ordinal de cada fila). Los tipos de
    datos de las columnas se especifican utilizando un conjunto est√°ndar
    de tipos definidos en la biblioteca SQL de Spark, que se importaron
    al principio de la celda.

10. Confirme que sus cambios se han aplicado a los datos visualizando el
    dataframe.

11. Utilice el icono **+ Code**¬†situado debajo de la salida de la celda
    para a√±adir una nueva celda de c√≥digo al notebook, e ingrese en ella
    el siguiente c√≥digo. Haga clic en el bot√≥n **‚ñ∑ Run cell**¬†y revise
    la salida.

> CodeCopy
>
> display(df)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image36.png)

12. El dataframe incluye s√≥lo los datos del archivo¬†**2019.csv**.
    Modifique el c√≥digo para que la ruta del archivo utilice un comod√≠n
    \* para leer los datos de los pedidos de venta de todos los archivos
    de la carpeta¬†**orders**.

13. Utilice el icono¬†**+ Code**¬†situado debajo de la salida de celda
    para a√±adir una nueva celda de c√≥digo al notebook, e ingrese el
    siguiente c√≥digo en ella.

CodeCopy

> from pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> ¬† ¬† StructField("SalesOrderNumber", StringType()),
>
> ¬† ¬† StructField("SalesOrderLineNumber", IntegerType()),
>
> ¬† ¬† StructField("OrderDate", DateType()),
>
> ¬† ¬† StructField("CustomerName", StringType()),
>
> ¬† ¬† StructField("Email", StringType()),
>
> ¬† ¬† StructField("Item", StringType()),
>
> ¬† ¬† StructField("Quantity", IntegerType()),
>
> ¬† ¬† StructField("UnitPrice", FloatType()),
>
> ¬† ¬† StructField("Tax", FloatType())
>
> ¬† ¬† \])
>
> df =
> spark.read.format("csv").schema(orderSchema).load("Files/orders/\*.csv")
>
> display(df)

![](./media/image37.png)

14. Ejecute la celda de c√≥digo modificada y revise el resultado, que
    ahora deber√≠a incluir las ventas para 2019, 2020 y 2021.

![](./media/image38.png)

**Nota**: S√≥lo se muestra un subconjunto de las filas, por lo que es
posible que no pueda ver ejemplos de todos los a√±os.

# Ejercicio 2: Explorar datos en un dataframe 

El objeto dataframe incluye una amplia gama de funciones que puede
utilizar para filtrar, agrupar y manipular de otro modo los datos que
contiene.

## Tarea 1: Filtrar un dataframe

1.  Utilice el icono¬†**+ Code**¬†situado debajo de la salida de la celda
    para a√±adir una nueva celda de c√≥digo al notebook, e ingrese en ella
    el siguiente c√≥digo.

**CodeCopy**

> customers = df\['CustomerName', 'Email'\]
>
> print(customers.count())
>
> print(customers.distinct().count())
>
> display(customers.distinct())
>
> ![](./media/image39.png)

2.  **Ejecute** la nueva c√©lula de c√≥digo y revise los resultados.
    Observe los siguientes detalles:

    - Cuando realiza una operaci√≥n en un dataframe, el resultado es un
      nuevo dataframe (en este caso, se crea un nuevo dataframe
      de¬†**customers**¬†seleccionando un subconjunto espec√≠fico de
      columnas del dataframe¬†**df**)

    - Los dataframes proporcionan funciones
      como¬†**count**¬†y¬†**distinct**¬†que pueden utilizarse para resumir y
      filtrar los datos que contienen.

    - La sintaxis¬†dataframe\['Field1', 'Field2', ...\]¬†es una forma
      abreviada de definir un subconjunto de columnas. Tambi√©n puede
      utilizar el m√©todo **select**, por lo que la primera l√≠nea del
      c√≥digo anterior podr√≠a escribirse como¬†customers =
      df.select("CustomerName", "Email")

> ![](./media/image40.png)

3.  Modifique el c√≥digo, sustituya todo el c√≥digo de la **celda** por el
    siguiente c√≥digo y haga clic en bot√≥n **‚ñ∑ Run cell**¬†de la siguiente
    manera:

> CodeCopy
>
> customers = df.select("CustomerName",
> "Email").where(df\['Item'\]=='Road-250 Red, 52')
>
> print(customers.count())
>
> print(customers.distinct().count())
>
> display(customers.distinct())

4.  **Ejecute¬†**el c√≥digo modificado para ver los clientes que han
    comprado el¬†**producto *Road-250 Red, 52*.** Tenga en cuenta que
    puede **‚Äùencadenar‚Äù¬†**varias funciones de forma que la salida de una
    funci√≥n se convierta en la entrada de la siguiente - en este caso,
    el dataframe creado por el m√©todo**¬†select¬†**es el dataframe de
    origen para el m√©todo¬†**where¬†**que se utiliza para aplicar los
    criterios de filtrado.

> ![](./media/image41.png)

## Tarea 2: Agregar y agrupar datos en un dataframe 

1.  Haga clic en **+ Code** y copie y pegue el siguiente c√≥digo y luego
    haga clic en el bot√≥n **Run cell**.

CodeCopy

> productSales = df.select("Item", "Quantity").groupBy("Item").sum()
>
> display(productSales)
>
> ![](./media/image42.png)

2.  Observe que los resultados muestran la suma de las cantidades
    pedidas agrupadas por producto. El m√©todo¬†**groupBy**¬†agrupa las
    filas por¬†*Item*, y la posterior funci√≥n¬†**sum**¬†agregada se aplica
    a todas las columnas num√©ricas restantes (en este caso,¬†*Quantity*)

![](./media/image43.png)

3.  Haga clic en¬†**+ Code**¬†y copie y pegue el siguiente c√≥digo y luego
    haga clic en el bot√≥n¬†**Run cell**.

> **CodeCopy**
>
> from pyspark.sql.functions import \*
>
> yearlySales =
> df.select(year("OrderDate").alias("Year")).groupBy("Year").count().orderBy("Year")
>
> display(yearlySales)

![](./media/image44.png)

4.  Observe que los resultados muestran el n√∫mero de pedidos de venta
    por a√±o. Observe que el m√©todo¬†**select**¬†incluye una funci√≥n
    SQL¬†**year**¬†para extraer el componente year del
    campo¬†*OrderDate*¬†(por eso el c√≥digo incluye un comando¬†**import**
    para importar funciones de la biblioteca SQL de Spark). A
    continuaci√≥n, se utiliza un m√©todo¬†**alias**¬†para asignar un nombre
    de columna al valor del a√±o extra√≠do. A continuaci√≥n, los datos se
    agrupan por la columna¬†*Year*¬†extra√≠da y se calcula el recuento de
    filas de cada grupo antes de utilizar finalmente el
    m√©todo¬†**orderBy**¬†para ordenar el dataframe resultante.

![](./media/image45.png)

# Ejercicio 3: Utilizar Spark para transformar archivos de datos

Una tarea com√∫n para los ingenieros de datos es ingerir datos en un
formato o estructura particular y transformarlos para su posterior
procesamiento o an√°lisis.

## Tarea 1: Utilizar m√©todos y funciones de dataframe para transformar datos

1.  Haga clic en + Code y copie y pegue el siguiente c√≥digo:

**CodeCopy**

> from pyspark.sql.functions import \*
>
> \## Create Year and Month columns
>
> transformed_df = df.withColumn("Year",
> year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))
>
> \# Create the new FirstName and LastName fields
>
> transformed_df = transformed_df.withColumn("FirstName",
> split(col("CustomerName"), " ").getItem(0)).withColumn("LastName",
> split(col("CustomerName"), " ").getItem(1))
>
> \# Filter and reorder columns
>
> transformed_df = transformed_df\["SalesOrderNumber",
> "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName",
> "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"\]
>
> \# Display the first five orders
>
> display(transformed_df.limit(5))

![](./media/image46.png)

2.  **Ejecute** el c√≥digo para crear un nuevo dataframe a partir de los
    datos del pedido original con las siguientes transformaciones:

    - A√±adir columnas **Year** y **Month** basadas en la columna
      **OrderDate**.

    - A√±adir las columnas **FirstName** y **LastName** bas√°ndose en la
      columna **CustomerName**.

    - Filtrar y reordenar las columnas, eliminando la columna
      **CustomerName**.

![](./media/image47.png)

3.  Revise la salida y verifique que se han realizado las
    transformaciones en los datos.

![](./media/image48.png)

Puede utilizar toda la potencia de la biblioteca SQL de Spark para
transformar los datos filtrando filas, derivando, eliminando,
renombrando columnas y aplicando cualquier otra modificaci√≥n de datos
necesaria.

**Consejo:** Consulte la**¬†[documentaci√≥n de dataframe de
Spark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)¬†**para
obtener m√°s informaci√≥n sobre los m√©todos del objeto dataframe.

## Tarea 2: Guardar los datos transformados

1.  **A√±ada una nueva celda** con el siguiente c√≥digo para guardar el
    dataframe transformado en formato Parquet (Sobrescribiendo los datos
    si ya existen). **Ejecute** la celda y espere el mensaje de que los
    datos se han guardado.

> CodeCopy
>
> transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')
>
> print ("Transformed data saved!")
>
> **Nota**: Com√∫nmente, se prefiere el formato Parquet para los archivos
> de datos que se utilizar√°n para un an√°lisis posterior o para su
> ingesti√≥n en un almac√©n anal√≠tico. Parquet es un formato muy eficiente
> que soportan la mayor√≠a de los sistemas de an√°lisis de datos a gran
> escala. De hecho, a veces su necesidad de transformaci√≥n de datos
> puede consistir simplemente en convertir los datos de otro formato
> (como CSV) a Parquet!

![](./media/image49.png)

![](./media/image50.png)

2.  A continuaci√≥n, en el panel izquierdo¬†del **Lakehouse explorer**, en
    el men√∫¬†**...** del nodo¬†**Files**, seleccione¬†**Refresh**.

> ![](./media/image51.png)

3.  Haga clic en la carpeta **transformed_data**¬†para comprobar que
    contiene una carpeta denominada **orders**, que a su vez contiene
    uno o varios **archivos Parquet**.

![](./media/image52.png)

4.  Haga clic en¬†**+ Code**¬†e ingrese el siguiente c√≥digo para cargar un
    nuevo dataframe a partir de los archivos parquet de la
    carpeta¬†**transformed_data -\> orders**:

> **CodeCopy**
>
> orders_df =
> spark.read.format("parquet").load("Files/transformed_data/orders")
>
> display(orders_df)
>
> ![](./media/image53.png)

5.  **Ejecute** la c√©lula y compruebe que los resultados muestran los
    datos del pedido que se han cargado desde los archivos parquet.

> ![](./media/image54.png)

## Tarea 3: Guardar datos en archivos particionados

1.  A√±ada una nueva celda, haga clic en **+Code** con el siguiente
    c√≥digo; que guarda el dataframe, particionando los datos por
    **Year** y **Month**. **Ejecute** la celda y espere el mensaje de
    que los datos se han guardado.

> CodeCopy
>
> orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")
>
> print ("Transformed data saved!")
>
> ![](./media/image55.png)
>
> ![](./media/image56.png)

2.  A continuaci√≥n, en el panel izquierdo¬†del **Lakehouse explorer**, en
    el men√∫¬†**...** del nodo¬†**Files**, seleccione¬†**Refresh.**

![](./media/image57.png)

![](./media/image58.png)

3.  Expanda la carpeta¬†**partitioned_orders**¬†para comprobar que
    contiene una jerarqu√≠a de carpetas denominadas¬†***Year=xxxx***, cada
    una de las cuales contiene carpetas denominadas¬†***Month=xxxx***.
    Cada carpeta de mes contiene un archivo parquet con los pedidos de
    ese mes.

![](./media/image59.png)

![](./media/image60.png)

> Particionar los archivos de datos es una forma habitual de optimizar
> el rendimiento cuando se trabaja con grandes vol√∫menes de datos. Esta
> t√©cnica puede mejorar significativamente el rendimiento y facilitar el
> filtrado de datos.

4.  A√±ada una nueva celda, haga clic en **+ Code** con el siguiente
    c√≥digo para cargar un nuevo dataframe desde el
    archivo¬†**orders.parquet**:

> CodeCopy
>
> orders_2021_df =
> spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=\*")
>
> display(orders_2021_df)

![](./media/image61.png)

5.  **Ejecute** la celda y compruebe que los resultados muestran los
    datos de los pedidos de ventas en 2021. Observe que las columnas de
    partici√≥n especificadas en la ruta (**Year** y **Month**) no est√°n
    incluidas en el dataframe.

![](./media/image62.png)

# **Ejercicio 3: Trabajar con tablas y SQL** 

Como ha visto, los m√©todos nativos del objeto dataframe le permiten
consultar y analizar datos de un archivo con bastante eficacia. Sin
embargo, muchos analistas de datos se sienten m√°s c√≥modos trabajando con
tablas que pueden consultar utilizando la sintaxis SQL. Spark
proporciona un¬†*metastore*¬†en el que puede definir tablas relacionales.
La biblioteca SQL de Spark que proporciona el objeto dataframe tambi√©n
admite el uso de comandos SQL para consultar tablas en el metastore.
Utilizando estas caracter√≠sticas de Spark, puede combinar la
flexibilidad de un lago de datos con el esquema de datos estructurado y
las consultas basadas en SQL de un almac√©n de datos relacional, de ah√≠
el t√©rmino "data lakehouse‚Äù.

## Tarea 1: Crear una tabla gestionada 

Las tablas de un metastore Spark son abstracciones relacionales sobre
archivos en el lago de datos. Las tablas pueden ser gestionadas (en cuyo
caso los archivos son gestionados por el metastore) o externas (en cuyo
caso la tabla hace referencia a una ubicaci√≥n de archivo en el lago de
datos que usted gestiona independientemente del metastore).

1.  A√±ada un nuevo c√≥digo, haga clic en la celda **+ Code** al notebook
    e ingrese el siguiente c√≥digo, que guarda el dataframe de datos de
    pedidos de venta como una tabla llamada **salesorders**:

> CodeCopy
>
> \# Create a new table
>
> df.write.format("delta").saveAsTable("salesorders")
>
> \# Get the table description
>
> spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)

![A screenshot of a computer Description automatically
generated](./media/image63.png)

**Nota**: Merece la pena se√±alar un par de cosas sobre este ejemplo. En
primer lugar, no se proporciona ninguna ruta expl√≠cita, por lo que los
archivos de la tabla ser√°n gestionados por el metastore. En segundo
lugar, la tabla se guarda en formato **delta**. Puede crear tablas
basadas en m√∫ltiples formatos de archivo (incluidos CSV, Parquet, Avro y
otros), pero el lago delta es una tecnolog√≠a de Spark que a√±ade
caracter√≠sticas de base de datos relacional a las tablas; incluyendo
soporte para transacciones, versionado de filas y otras caracter√≠sticas
√∫tiles. La creaci√≥n de tablas en formato delta es preferible para los
data lakehouses en Fabric.

2.  **Ejecute** la celda de c√≥digo y revise la salida, que describe la
    definici√≥n de la nueva tabla.

![A screenshot of a computer Description automatically
generated](./media/image64.png)

3.  En el panel¬†**Lakehouse** **explorer**, en el¬†**‚Ä¶**¬†men√∫ de la
    carpeta **Tables**, seleccione **Refresh.**

![A screenshot of a computer Description automatically
generated](./media/image65.png)

4.  A continuaci√≥n, expanda el nodo¬†**Tables**¬†y compruebe que se ha
    creado la tabla¬†**salesorders**.

> ![A screenshot of a computer Description automatically
> generated](./media/image66.png)

5.  Pase el cursor por encima de la tabla¬†**salesorders**, luego haga
    clic en las elipses horizontales (...). Navegue y haga clic
    en¬†**Load data**, luego seleccione¬†**Spark**.

![A screenshot of a computer Description automatically
generated](./media/image67.png)

6.  Haga clic en el bot√≥n **‚ñ∑ Run cell**¬†y que utiliza la biblioteca de
    Spark SQL para incrustrar una consulta SQL contra la tabla
    **salesorder** en c√≥digo PySpark y cargar los resultados de la
    consulta en un dataframe.

> CodeCopy
>
> df = spark.sql("SELECT \* FROM \[your_lakehouse\].salesorders LIMIT
> 1000")
>
> display(df)

![A screenshot of a computer program Description automatically
generated](./media/image68.png)

![](./media/image69.png)

## Tarea 2: Crear una tabla externa 

Tambi√©n puede crear tablas *externas* cuyos metadatos de esquema se
definan en el metastore del lakehouse, pero cuyos archivos de datos se
almacenen en una ubicaci√≥n externa.

1.  Debajo de los resultados devueltos por la primera celda de c√≥digo,
    utilice el bot√≥n **+ Code** para a√±adir una nueva celda de c√≥digo si
    a√∫n no existe ninguna. A continuaci√≥n, ingrese el siguiente c√≥digo
    en la nueva celda.

CodeCopy

> df.write.format("delta").saveAsTable("external_salesorder",
> path="\<abfs_path\>/external_salesorder")

![A screenshot of a computer Description automatically
generated](./media/image70.png)

2.  En el panel **Lakehouse explorer**, en el men√∫ **...** de la carpeta
    **Files**, seleccione **Copy ABFS** **path** en el bloc de notas.

> La ruta ABFS es la ruta completa a la carpeta **Files** en el
> almacenamiento OneLake para su lakehouse - similar a esto:

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

![A screenshot of a computer Description automatically
generated](./media/image71.png)

3.  Ahora, pase a la celda de c√≥digo, sustituya¬†**\<abfs_path\>**¬†por
    la¬†**ruta**¬†que copi√≥ en el bloc de notas para que el c√≥digo guarde
    el dataframe como una tabla externa con archivos de datos en una
    carpeta llamada¬†**external_salesorder**¬†en la ubicaci√≥n de su
    carpeta¬†**Files**. La ruta completa deber√≠a ser similar a esta.

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

4.  Utilice el bot√≥n¬†**‚ñ∑¬†(*Run cell*)** situado a la izquierda de la
    celda para ejecutarla.

![](./media/image72.png)

5.  En el panel¬†**Lakehouse explorer**, en el men√∫¬†**‚Ä¶**¬†de la
    carpeta¬†**Tables**, seleccione la opci√≥n **Refresh**.

![A screenshot of a computer Description automatically
generated](./media/image73.png)

6.  A continuaci√≥n, expanda el nodo¬†**Tables**¬†y compruebe que se ha
    creado la tabla¬†**external_salesorder**.

![A screenshot of a computer Description automatically
generated](./media/image74.png)

7.  En el panel¬†**Lakehouse explorer**, en el men√∫¬†**...** de la
    carpeta¬†**Files**, seleccione¬†**Refresh**.

![A screenshot of a computer Description automatically
generated](./media/image75.png)

8.  A continuaci√≥n, expanda el nodo¬†**Files**¬†y compruebe que se ha
    creado la carpeta¬†**external_salesorder**¬†para los archivos de datos
    de la tabla.

![A screenshot of a computer Description automatically
generated](./media/image76.png)

## Tarea 3: Comparar tablas gestionadas y externas 

Exploremos las diferencias entre las tablas gestionadas y las externas.

1.  Debajo de los resultados devueltos por la celda de c√≥digo, utilice
    el bot√≥n **+ Code** para a√±adir una nueva celda de c√≥digo. Copie el
    c√≥digo siguiente en la celda Code y utilice el bot√≥n ‚ñ∑ (***Run
    cell***) situado a la izquierda de la celda para ejecutarlo.

> SqlCopy
>
> %%sql
>
> DESCRIBE FORMATTED salesorders;

![A screenshot of a computer Description automatically
generated](./media/image77.png)

![A screenshot of a computer Description automatically
generated](./media/image78.png)

2.  En los resultados, vea la propiedad¬†**Location**¬†de la tabla, que
    deber√≠a ser una ruta al almac√©n OneLake para la lakehouse que
    termine en¬†**/Tables/salesorders**¬†(puede que necesite ampliar la
    columna¬†**Data type**¬†para ver la ruta completa).

![A screenshot of a computer Description automatically
generated](./media/image79.png)

3.  Modifique el comando¬†**DESCRIBE**¬†para mostrar los detalles de la
    tabla¬†**external_saleorder**¬†como se muestra a continuaci√≥n.

4.  Debajo de los resultados devueltos por la celda de c√≥digo, utilice
    el bot√≥n¬†**+ Code**¬†para a√±adir una nueva celda de c√≥digo. Copie el
    c√≥digo siguiente y utilice el bot√≥n¬†**‚ñ∑ (*Run cell*)**¬†situado a la
    izquierda de la celda para ejecutarlo.

> SqlCopy
>
> %%sql
>
> DESCRIBE FORMATTED external_salesorder;

![A screenshot of a email Description automatically
generated](./media/image80.png)

5.  En los resultados, vea la propiedad¬†**Location**¬†de la tabla, que
    deber√≠a ser una ruta al almac√©n OneLake para la lakehouse que
    termine en **/Files/external_saleorder**¬†(puede que necesite ampliar
    la columna¬†**Data type**¬†para ver la ruta completa).

![A screenshot of a computer Description automatically
generated](./media/image81.png)

## Tarea 4: Ejecutar c√≥digo SQL en una celda

Aunque es √∫til poder incrustar sentencias SQL en una celda que contenga
c√≥digo PySpark, los analistas de datos a menudo s√≥lo quieren trabajar
directamente en SQL.

1.  Haga clic en la celda¬†**+ Code**¬†del notebook e ingrese en ella el
    siguiente c√≥digo. Haga clic en el bot√≥n¬†**‚ñ∑ Run cell**¬†y revise los
    resultados. Observe que:

    - La l√≠nea¬†%%sql¬†al principio de la celda (llamada magic) indica que
      se debe utilizar el tiempo de ejecuci√≥n del lenguaje Spark SQL
      para ejecutar el c√≥digo en esta celda en lugar de PySpark.

    - El c√≥digo SQL hace referencia a la tabla¬†**salesorders**¬†que cre√≥
      anteriormente.

    - La salida de la consulta SQL se muestra autom√°ticamente como
      resultado bajo la celda.

> SqlCopy
>
> %%sql
>
> SELECT YEAR(OrderDate) AS OrderYear,
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue
>
> FROM salesorders
>
> GROUP BY YEAR(OrderDate)
>
> ORDER BY OrderYear;

![A screenshot of a computer Description automatically
generated](./media/image82.png)

![](./media/image83.png)

**Nota**: Para m√°s informaci√≥n sobre Spark SQL y dataframes, consulte
la¬†[documentaci√≥n de Spark
SQL](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html).

# Ejercicio 4: Visualizar datos con Spark 

Proverbialmente, una imagen vale m√°s que mil palabras, y un gr√°fico es a
menudo mejor que mil filas de datos. Aunque los notebooks en Fabric
incluyen una vista de gr√°fico integrada para los datos que se muestran
desde un dataframe o una consulta SQL de Spark, no est√° dise√±ada para la
creaci√≥n de gr√°ficos completos. Sin embargo, puede utilizar bibliotecas
gr√°ficas de Python como¬†**matplotlib**¬†y¬†**seaborn**¬†para crear gr√°ficos
a partir de datos en dataframes.

## Tarea 1: Visualizar los resultados en forma de gr√°fico

1.  Haga clic en la celda¬†**+ Code**¬†del notebook e ingrese en ella el
    siguiente c√≥digo. Haga clic en el bot√≥n¬†**‚ñ∑ Run cell**¬†y observe que
    devuelve los datos de la vista¬†**salesorders**¬†que cre√≥
    anteriormente.

> SqlCopy
>
> %%sql
>
> SELECT \* FROM salesorders

![A screenshot of a computer Description automatically
generated](./media/image84.png)

![A screenshot of a computer Description automatically
generated](./media/image85.png)

2.  En la secci√≥n de resultados situada debajo de la celda, cambie la
    opci√≥n¬†**View**¬†de¬†**Table**¬†a¬†**Chart**.

![A screenshot of a computer Description automatically
generated](./media/image86.png)

3.  Utilice el bot√≥n¬†**View options**¬†situado en la parte superior
    derecha del gr√°fico para mostrar el panel de opciones del gr√°fico. A
    continuaci√≥n, configure las opciones como se indica a continuaci√≥n y
    seleccione¬†**Apply**:

    - **Chart type**: Bar chart

    - **Key**: Item

    - **Values**: Quantity

    - **Series Group**:¬†*Dejar en blanco*

    - **Aggregation**: Sum

    - **Stacked**:¬†*Sin seleccionar*

![A blue barcode on a white background Description automatically
generated](./media/image87.png)

![A screenshot of a graph Description automatically
generated](./media/image88.png)

4.  Verifique que el gr√°fico tiene un aspecto similar al siguiente.

![A screenshot of a computer Description automatically
generated](./media/image89.png)

## Tarea 2: Empezar con matplotlib 

1.  Haga clic en **+ Code** y copie y pegue el c√≥digo siguiente.
    **Ejecute** el c√≥digo y observe que devuelve un dataframe Spark que
    contiene los ingresos anuales.

> CodeCopy
>
> sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue \\
>
> FROM salesorders \\
>
> GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\
>
> ORDER BY OrderYear"
>
> df_spark = spark.sql(sqlQuery)
>
> df_spark.show()

![A screenshot of a computer Description automatically
generated](./media/image90.png)

![](./media/image91.png)

2.  Para visualizar los datos en forma de gr√°fico, empezaremos
    utilizando la biblioteca de Python¬†**matplotlib**. Esta biblioteca
    es el n√∫cleo de la biblioteca de trazado en la que se basan muchas
    otras, y proporciona una gran flexibilidad en la creaci√≥n de
    gr√°ficos.

3.  Haga clic en **+ Code** y copie y pegue el siguiente c√≥digo.

**CodeCopy**

> from matplotlib import pyplot as plt
>
> \# matplotlib requires a Pandas dataframe, not a Spark one
>
> df_sales = df_spark.toPandas()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\])
>
> \# Display the plot
>
> plt.show()

![A screenshot of a computer Description automatically
generated](./media/image92.png)

5.  Haga clic en el bot√≥n**¬†Run cell**¬†y revise los resultados, que
    consisten en un gr√°fico de columnas con los ingresos brutos totales
    de cada a√±o. Observe las siguientes caracter√≠sticas del c√≥digo
    utilizado para elaborar este gr√°fico:

    - La biblioteca¬†**matplotlib**¬†requiere un dataframe¬†*Pandas*, por
      lo que es necesario convertir el dataframe¬†*Spark*¬†devuelto por la
      consulta Spark SQL a este formato.

    - En el n√∫cleo de la biblioteca **matplotlib** se encuentra el
      objeto **pyplot**. Es la base de la mayor√≠a de las funciones de
      trazado.

    - Los ajustes por defecto dan como resultado un gr√°fico utilizable,
      pero hay un margen considerable para personalizarlo.

![A screenshot of a computer screen Description automatically
generated](./media/image93.png)

6.  Modifique el c√≥digo para trazar el gr√°fico de la siguiente manera,
    sustituya todo el c√≥digo de la¬†**celda**¬†por el siguiente c√≥digo y
    haga clic en el bot√≥n¬†**‚ñ∑ Run cell**¬†y revise la salida.

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure
>
> plt.show()

![A screenshot of a graph Description automatically
generated](./media/image94.png)

7.  El gr√°fico incluye ahora algo m√°s de informaci√≥n. Un gr√°fico est√°
    t√©cnicamente contenido con una **Figura**. En los ejemplos
    anteriores, la figura se cre√≥ impl√≠citamente para usted; pero puede
    crearla expl√≠citamente.

8.  Modifique el c√≥digo para trazar el gr√°fico de la siguiente manera,
    sustituya todo el c√≥digo de la **celda** por el siguiente c√≥digo.

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a Figure
>
> fig = plt.figure(figsize=(8,3))
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure
>
> plt.show()

![A screenshot of a computer program Description automatically
generated](./media/image95.png)

9.  **Vuelva a ejecutar** la celda de c√≥digo y vea los resultados. La
    figura determina la forma y el tama√±o del gr√°fico.

> Una figura puede contener varias subparcelas, cada una en su propio
> *eje*.

![A screenshot of a computer Description automatically
generated](./media/image96.png)

10. Modifique el c√≥digo para trazar el gr√°fico como se indica a
    continuaci√≥n.¬†**Vuelva a ejecutar**¬†la celda de c√≥digo y vea los
    resultados. La figura contiene los subgrupos especificados en el
    c√≥digo.

> CodeCopy
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a figure for 2 subplots (1 row, 2 columns)
>
> fig, ax = plt.subplots(1, 2, figsize = (10,4))
>
> \# Create a bar plot of revenue by year on the first axis
>
> ax\[0\].bar(x=df_sales\['OrderYear'\],
> height=df_sales\['GrossRevenue'\], color='orange')
>
> ax\[0\].set_title('Revenue by Year')
>
> \# Create a pie chart of yearly order counts on the second axis
>
> yearly_counts = df_sales\['OrderYear'\].value_counts()
>
> ax\[1\].pie(yearly_counts)
>
> ax\[1\].set_title('Orders per Year')
>
> ax\[1\].legend(yearly_counts.keys().tolist())
>
> \# Add a title to the Figure
>
> fig.suptitle('Sales Data')
>
> \# Show the figure
>
> plt.show()

![A screenshot of a computer program Description automatically
generated](./media/image97.png)

![](./media/image98.png)

**Nota**: Para saber m√°s sobre el trazado con matplotlib, consulte
la¬†[documentaci√≥n de matplotlib](https://matplotlib.org/).

## Tarea 3: Utilizar la biblioteca seaborn 

Aunque¬†**matplotlib**¬†le permite crear gr√°ficos complejos de m√∫ltiples
tipos, puede requerir cierto c√≥digo complejo para lograr los mejores
resultados. Por este motivo, a lo largo de los a√±os, se han construido
muchas bibliotecas nuevas sobre la base de matplotlib para abstraer su
complejidad y mejorar sus caracter√≠sticas. Una de estas bibliotecas
es¬†**seaborn**.

1.  Haga clic en **+ Code** y copie y pegue el c√≥digo siguiente.

CodeCopy

> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar chart
>
> ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()

![A screenshot of a graph Description automatically
generated](./media/image99.png)

2.  **Ejecute¬†**el c√≥digo y observe que muestra un gr√°fico de barras
    utilizando la biblioteca seaborn.

![A screenshot of a graph Description automatically
generated](./media/image100.png)

3.  **Modifique¬†**el c√≥digo como se indica a
    continuaci√≥n.**¬†Ejecute¬†**el c√≥digo modificado y observe que seaborn
    le permite establecer un tema de color coherente para sus parcelas.

> CodeCopy
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Set the visual theme for seaborn
>
> sns.set_theme(style="whitegrid")
>
> \# Create a bar chart
>
> ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()
>
> ![](./media/image101.png)

4.  **Modifique¬†**de nuevo el c√≥digo como se indica a
    continuaci√≥n.¬†**Ejecute¬†**el c√≥digo modificado para ver los ingresos
    anuales en forma de gr√°fico de l√≠neas.

> CodeCopy
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar chart
>
> ax = sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()

![](./media/image102.png)

**Nota**: Para saber m√°s sobre el trazado con seaborn, consulte
la¬†[documentaci√≥n de seaborn](https://seaborn.pydata.org/index.html).

## Tarea 4: Procesamiento de flujos de datos con tablas Delta

Delta Lake soporta flujos de datos continuos. Las tablas Delta pueden
funcionar como *destino (sink)* o *fuente (source)* para flujos creados
con la API de Spark Structured Streaming. En este ejemplo, utilizar√°s
una tabla Delta como destino para datos de streaming en un escenario
simulado de Internet de las Cosas (IoT).

1.  Haga clic en **+ Code** y copie y pegue el c√≥digo de abajo y luego
    haga clic en el bot√≥n **Run cell**.

CodeCopy

> from notebookutils import mssparkutils
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import \*
>
> \# Create a folder
>
> inputPath = 'Files/data/'
>
> mssparkutils.fs.mkdirs(inputPath)
>
> \# Create a stream that reads data from the folder, using a JSON
> schema
>
> jsonSchema = StructType(\[
>
> StructField("device", StringType(), False),
>
> StructField("status", StringType(), False)
>
> \])
>
> iotstream =
> spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger",
> 1).json(inputPath)
>
> \# Write some event data to the folder
>
> device_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "data.txt", device_data, True)
>
> print("Source stream created...")

![A screenshot of a computer program Description automatically
generated](./media/image103.png)

![A screenshot of a computer Description automatically
generated](./media/image104.png)

2.  Aseg√∫rese de que se imprime el mensaje¬†***Source stream
    created***.... El c√≥digo que acaba de ejecutar ha creado una fuente
    de datos de flujo basada en una carpeta en la que se han guardado
    algunos datos, que representan lecturas de dispositivos IoT
    hipot√©ticos.

3.  Haga clic en¬†**+ Code**¬†y copie y pegue el siguiente c√≥digo y luego
    haga clic en el bot√≥n¬†**Run cell**.

CodeCopy

> \# Write the stream to a delta table
>
> delta_stream_table_path = 'Tables/iotdevicedata'
>
> checkpointpath = 'Files/delta/checkpoint'
>
> deltastream =
> iotstream.writeStream.format("delta").option("checkpointLocation",
> checkpointpath).start(delta_stream_table_path)
>
> print("Streaming to delta sink...")

![A screenshot of a computer Description automatically
generated](./media/image105.png)

4.  Este c√≥digo escribe los datos del dispositivo de streaming en
    formato delta en una carpeta llamada¬†**iotdevicedata**. Dado que la
    ruta para la ubicaci√≥n de la carpeta se encuentra en la
    carpeta¬†**Tables**, se crear√° autom√°ticamente una tabla para ella.
    Haga clic en las elipses horizontales junto a table, y luego haga
    clic en¬†**Refresh**.

![](./media/image106.png)

![A screenshot of a computer Description automatically
generated](./media/image107.png)

5.  Haga clic en¬†**+ Code**¬†y copie y pegue el siguiente c√≥digo y luego
    haga clic en el bot√≥n¬†**Run cell**.

> SqlCopy
>
> %%sql
>
> SELECT \* FROM IotDeviceData;

![A screenshot of a computer Description automatically
generated](./media/image108.png)

6.  Este c√≥digo consulta la tabla¬†**IotDeviceData**, que contiene los
    datos del dispositivo de la fuente de streaming.

7.  Haga clic en¬†**+ Code**¬†y copie y pegue el siguiente c√≥digo y luego
    haga clic en el bot√≥n¬†**Run cell**.

> CodeCopy
>
> \# Add more data to the source stream
>
> more_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)

![A screenshot of a computer Description automatically
generated](./media/image109.png)

8.  Este c√≥digo escribe m√°s datos del dispositivo hipot√©tico en la
    fuente de streaming.

9.  Haga clic en¬†**+ Code**¬†y copie y pegue el siguiente c√≥digo y luego
    haga clic en el bot√≥n¬†**Run cell**.

> SqlCopy
>
> %%sql
>
> SELECT \* FROM IotDeviceData;
>
> ![A screenshot of a computer Description automatically
> generated](./media/image110.png)

10. Este c√≥digo consulta de nuevo la tabla¬†**IotDeviceData**, que ahora
    deber√≠a incluir los datos adicionales que se a√±adieron a la fuente
    de streaming.

11. Haga clic en¬†**+ Code** y copie y pegue el siguiente c√≥digo y luego
    haga clic en el bot√≥n¬†**Run cell**.

> CodeCopy
>
> deltastream.stop()

![A screenshot of a computer Description automatically
generated](./media/image111.png)

12. Este c√≥digo detiene el flujo.

## Tarea 5: Guardar el notebook y finalizar la sesi√≥n Spark 

Ahora que ha terminado de trabajar con los datos, puede guardar el
notebook con un nombre significativo y finalizar la sesi√≥n Spark.

1.  En la barra de men√∫ del notebook, utilice el icono
    ‚öôÔ∏è¬†**Settings**¬†para ver la configuraci√≥n del notebook.

![A screenshot of a computer Description automatically
generated](./media/image112.png)

2.  Establezca el nombre en el campo¬†**Name**¬†del notebook a¬†++**Explore
    Sales Orders++**, y luego cierre el panel de configuraci√≥n.

![A screenshot of a computer Description automatically
generated](./media/image113.png)

3.  En el men√∫ del notebook, seleccione¬†**Stop session**¬†para finalizar
    la sesi√≥n Spark.

![A screenshot of a computer Description automatically
generated](./media/image114.png)

![A screenshot of a computer Description automatically
generated](./media/image115.png)

# Ejercicio 5: Crear un dataflow (Gen2) en Microsoft Fabric 

En Microsoft Fabric, los Dataflows (Gen2) se conectan a varias fuentes
de datos y realizan transformaciones en Power Query Online. A
continuaci√≥n, pueden utilizarse en Data Pipelines para ingerir datos en
un lakehouse u otro almac√©n anal√≠tico, o para definir un conjunto de
datos para un informe de Power BI.

Este ejercicio est√° dise√±ado para introducir los distintos elementos de
Dataflow (Gen2), y no para crear una soluci√≥n compleja que pueda existir
en una empresa.

## Tarea 1: Crear un Dataflow (Gen2) para la ingesta de datos 

Ahora que tiene un lakehouse, necesita ingerir algunos datos en √©l. Una
forma de hacerlo es definir un dataflow que encapsule un proceso ETL
(*extraer, transformar y cargar*)*.*

1.  Ahora, haga clic en¬†**Fabric_lakehouse**¬†en el panel de navegaci√≥n
    de la izquierda.

![A screenshot of a computer Description automatically
generated](./media/image116.png)

2.  En la p√°gina de inicio¬†**de Fabric_lakehouse¬†**, haga clic en la
    flecha desplegable¬†**Get data**¬†y seleccione¬†**New Dataflow
    Gen2.**¬†Se abrir√° el editor de Power Query para su nuevo dataflow.

![](./media/image117.png)

3.  En el panel¬†**Power Query**¬†bajo¬†la **pesta√±a** **Home**, haga clic
    en¬†**Import from a Text/CSV file**.

![](./media/image118.png)

4.  En el panel **Connect to data source**, en **Connection settings**,
    seleccione la opci√≥n **Link to file (Preview)**

- **Link to file**:¬†*Seleccionado*

- **File path or
  URL**:¬†<https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/orders.csv>

![](./media/image119.png)

5.  En el panel¬†**Connect to data source**, en¬†**Connection
    credentials,¬†**ingrese los siguientes detalles y haga clic en el
    bot√≥n¬†**Next**.

    - **Connection**: Create new connection

    - **data gateway**: (none)

    - **Authentication kind**: Organizational account

![](./media/image120.png)

6.  En el panel **Preview file data**, haga clic en **Create**¬†para
    crear la fuente de datos. ![A screenshot of a computer Description
    automatically generated](./media/image121.png)

7.  El editor de¬†**Power Query**¬†muestra la fuente de datos y un
    conjunto inicial de pasos de consulta para formatear los datos.

![A screenshot of a computer Description automatically
generated](./media/image122.png)

8.  En la cinta de la barra de herramientas, seleccione la pesta√±a¬†**Add
    column**. A continuaci√≥n, seleccione¬†**Custom column.**

![A screenshot of a computer Description automatically
generated](./media/image123.png)¬†

9.  Establezca el nombre de New column como **MonthNo**, establezca el
    tipo de datos como **Whole Number** y, a continuaci√≥n, a√±ada la
    siguiente f√≥rmula:**Date.Month(\[OrderDate\])** en **Custom column
    formula**. Seleccione **OK**.

![A screenshot of a computer Description automatically
generated](./media/image124.png)

10. Observe c√≥mo el paso para a√±adir la columna personalizada se a√±ade a
    la consulta. La columna resultante se muestra en el panel de datos.

![A screenshot of a computer Description automatically
generated](./media/image125.png)

**Consejo:**¬†En el panel Query Settings, en la parte derecha, observe
que los**¬†Applied Steps¬†**incluyen cada paso de transformaci√≥n. En la
parte inferior, tambi√©n puede activar el bot√≥n¬†**Diagram flow**¬†para
activar el Visual Diagram (diagrama visual) de los pasos.

Los pasos pueden moverse hacia arriba o hacia abajo, editarse
seleccionando el icono de engranaje, y puede seleccionar cada paso para
ver las transformaciones aplicadas en el panel de vista previa.

Tarea 2: A√±adir destino de datos para Dataflow

1.  En la cinta de la barra de herramientas¬†**Power Query**, seleccione
    la pesta√±a¬†**Home**. A continuaci√≥n, en el men√∫ desplegable **Data
    destination**, seleccione¬†**Lakehouse** (si no est√° ya
    seleccionado).

![](./media/image126.png)

![A screenshot of a computer Description automatically
generated](./media/image127.png)

**Nota:**¬†Si esta opci√≥n aparece en gris, es posible que ya tenga un
destino de datos establecido. Compruebe el destino de los datos en la
parte inferior del panel de configuraci√≥n de consultas, a la derecha del
editor de Power Query. Si ya hay un destino establecido, puede cambiarlo
utilizando el engranaje.

2.  Haga clic en el icono¬†**Settings**¬†junto a la
    opci√≥n¬†**Lakehouse**¬†seleccionada.

![A screenshot of a computer Description automatically
generated](./media/image128.png)

3.  En el cuadro de di√°logo¬†**Connect to data destination**,
    seleccione¬†**Edit connection.**

![A screenshot of a computer Description automatically
generated](./media/image129.png)

4.  En el cuadro de di√°logo¬†**Connect to data destination**,
    seleccione¬†**sign in**¬†usando su cuenta de organizaci√≥n de Power BI
    para establecer la identidad que utiliza el dataflow para acceder al
    lakehouse.

![A screenshot of a computer Description automatically
generated](./media/image130.png)

![A screenshot of a computer Description automatically
generated](./media/image131.png)

5.  En el cuadro de di√°logo Connect to data destination, seleccione
    **Next.**

![A screenshot of a computer Description automatically
generated](./media/image132.png)

6.  En el cuadro de di√°logo Connect to data destination,
    seleccione¬†**New table**. Haga clic en la¬†**carpeta
    Lakehouse**,seleccione su workspace -¬†**dp_FabricXX¬†**y**¬†**luego
    seleccione**¬†**su lakehouse es decir¬†**Fabric_lakehouse.**¬†A
    continuaci√≥n, especifique el nombre de la tabla como¬†**orders**¬†y
    seleccione el bot√≥n¬†**Next**.

![A screenshot of a computer Description automatically
generated](./media/image133.png)

7.  En el cuadro de di√°logo¬†**Choose destination settings**, en¬†**Use
    automatic settings off**¬†y en¬†**Update
    method¬†**seleccione¬†**Append**, luego haga clic en el bot√≥n¬†**Save
    settings**.

![](./media/image134.png)

8.  El destino¬†**Lakehouse**¬†se indica como un¬†**icono**¬†en
    la¬†**consulta**¬†en el editor de Power Query.

![A screenshot of a computer Description automatically
generated](./media/image135.png)

![A screenshot of a computer Description automatically
generated](./media/image136.png)

9.  Seleccione¬†**Publish**¬†para publicar el dataflow. A continuaci√≥n,
    espere a que se cree el¬†**Dataflow 1**¬†en su workspace.

![A screenshot of a computer Description automatically
generated](./media/image137.png)

10. Una vez publicado, puede hacer clic con el bot√≥n derecho en el
    dataflow en su workspace, seleccione¬†**Properties**, y cambie el
    nombre de su dataflow.

![A screenshot of a computer Description automatically
generated](./media/image138.png)

11. En el cuadro de di√°logo¬†**Dataflow1**, ingrese el nombre en el
    campo¬†**Name**¬†como¬†**Gen2_Dataflow**¬†y haga clic en el
    bot√≥n¬†**Save**.

![A screenshot of a computer Description automatically
generated](./media/image139.png)

![](./media/image140.png)

## Tarea 3: A√±adir un dataflow a un pipeline

Puede incluir un dataflow como actividad dentro de un pipeline. Los
pipelines se utilizan para orquestar procesos de ingesta y
transformaci√≥n de datos, permitiendo combinar dataflows con otros tipos
de operaciones en un flujo programado. Estos pipelines pueden crearse en
diferentes entornos, incluyendo la experiencia de Data Factory.

1.  En la p√°gina de inicio de Synapse Data Engineering, en el panel
    **dp_FabricXX**, seleccione **+New item** -\> **Data pipeline**

![](./media/image141.png)

2.  En el cuadro de di√°logo¬†**New pipeline**, ingrese **Load data** en
    el campo¬†**Name**, haga clic en el bot√≥n **Create** para abrir el
    nuevo pipeline.

![A screenshot of a computer Description automatically
generated](./media/image142.png)

3.  Se abre el editor de pipelines.

![A screenshot of a computer Description automatically
generated](./media/image143.png)

> **Consejo**: Si el asistente de copia de datos se abre
> autom√°ticamente, ¬°ci√©rrelo!

4.  Seleccione¬†**Pipeline activity**, y a√±ada una
    actividad¬†**Dataflow**¬†al pipeline.

![](./media/image144.png)

5.  Con la nueva actividad¬†**Dataflow1**¬†seleccionada, en la
    pesta√±a¬†**Settings**, en la lista desplegable¬†**Dataflow**,
    seleccione¬†**Gen2_Dataflow**¬†(el flujo de datos que cre√≥
    anteriormente).

![](./media/image145.png)

6.  En la pesta√±a **Home**, guarde el pipeline utilizando el
    icono¬†**üñ´¬†(*Save)***.

![A screenshot of a computer Description automatically
generated](./media/image146.png)

7.  Utilice el bot√≥n¬†**‚ñ∑ Run**¬†para ejecutar el pipeline y espere a que
    se complete. Puede tardar unos minutos.

> ![A screenshot of a computer Description automatically
> generated](./media/image147.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image148.png)

![A screenshot of a computer Description automatically
generated](./media/image149.png)

8.  En la barra de men√∫ del borde izquierdo, seleccione su workspace, es
    decir,¬†**dp_FabricXX.**

![A screenshot of a computer Description automatically
generated](./media/image150.png)

9.  En el panel¬†**Fabric_lakehouse**,
    seleccione¬†**Gen2_FabricLakehouse¬†**de tipo Lakehouse.

![A screenshot of a computer Description automatically
generated](./media/image151.png)

10. En el panel¬†**Explorer**, seleccione el men√∫¬†**...** de¬†**Tables**,
    seleccione¬†**refresh**. A continuaci√≥n, expanda¬†**Tables**¬†y
    seleccione la tabla de¬†**orders**, que ha sido creada por su
    dataflow.

![A screenshot of a computer Description automatically
generated](./media/image152.png)

![](./media/image153.png)

**Consejo**: Utilice el¬†*conector*¬†Power BI Desktop¬†*Dataflows*¬†para
conectarse directamente a las transformaciones de datos realizadas con
su dataflow.  
Tambi√©n puede realizar transformaciones adicionales, publicar como un
nuevo conjunto de datos y distribuir con la audiencia prevista para
conjuntos de datos especializados.

## Tarea 4: Limpieza de recursos

En este ejercicio, ha aprendido a utilizar Spark para trabajar con datos
en Microsoft Fabric.

Si ha terminado de explorar su lakehouse, puede borrar el workspace que
cre√≥ para este ejercicio.

1.  En la barra de la izquierda, seleccione el icono de su workspace
    para ver todos los elementos que contiene.

> ![A screenshot of a computer Description automatically
> generated](./media/image154.png)

2.  En el men√∫ **‚Ä¶**¬†de la barra de herramientas, seleccione **Workspace
    settings**.

![](./media/image155.png)

3.  Seleccione¬†**General** y haga clic en **Remove this workspace.**

![A screenshot of a computer settings Description automatically
generated](./media/image156.png)

4.  En el cuadro de di√°logo **Delete workspace?** haga clic en el bot√≥n
    **Delete**.

> ![A screenshot of a computer Description automatically
> generated](./media/image157.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image158.png)

**Resumen**

Este caso de uso le gu√≠a a trav√©s del proceso de trabajo con Microsoft
Fabric dentro de Power BI. Cubre varias tareas, incluyendo la
configuraci√≥n de un workspace, la creaci√≥n de un lakehouse, la carga y
gesti√≥n de archivos de datos y el uso de notebooks para la exploraci√≥n
de datos. Los participantes aprender√°n a manipular y transformar datos
utilizando PySpark, a crear visualizaciones y a guardar y particionar
datos para una consulta eficiente.

En este caso de uso, los participantes realizar√°n una serie de tareas
centradas en el trabajo con tablas delta en Microsoft Fabric. Las tareas
abarcan la carga y exploraci√≥n de datos, la creaci√≥n de tablas delta
gestionadas y externas, la comparaci√≥n de sus propiedades, el
laboratorio introduce las caracter√≠sticas de SQL para la gesti√≥n de
datos estructurados y proporciona conocimientos sobre la visualizaci√≥n
de datos utilizando bibliotecas de Python como matplotlib y seaborn. El
objetivo de los ejercicios es proporcionar una comprensi√≥n exhaustiva de
la utilizaci√≥n de Microsoft Fabric para el an√°lisis de datos y la
incorporaci√≥n de tablas delta para el flujo de datos en un contexto de
IoT.

Este caso de uso le gu√≠a a trav√©s del proceso de configuraci√≥n de un
workspace de Fabric, la creaci√≥n de un lakehouse de datos y la ingesta
de datos para su an√°lisis. Demuestra c√≥mo definir un dataflow para
manejar las operaciones ETL y configurar los destinos de datos para
almacenar los datos transformados. Adem√°s, aprender√° a integrar el
dataflow en un pipeline para su procesamiento automatizado. Por √∫ltimo,
se le proporcionar√°n instrucciones para limpiar los recursos una vez
finalizado el ejercicio.

Este laboratorio le dota de las habilidades esenciales para trabajar con
Fabric, permiti√©ndole crear y gestionar workspaces, establecer
lakehouses de datos y realizar transformaciones de datos de forma
eficiente. Al incorporar dataflows a los pipelines, aprender√° a
automatizar las tareas de procesamiento de datos, agilizando su flujo de
trabajo y mejorando la productividad en escenarios del mundo real. Las
instrucciones de limpieza le garantizan que no dejar√° recursos
innecesarios, promoviendo un enfoque de gesti√≥n del workspace organizado
y eficiente.
