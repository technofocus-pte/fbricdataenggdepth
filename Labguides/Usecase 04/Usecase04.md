# Anwendungsfall 04: Moderne Analysen im Cloud-MaÃŸstab mit Azure Databricks und Microsoft Fabric

**Einleitung**

In diesem Lab untersuchen Sie die Integration von Azure Databricks mit
Microsoft Fabric, um ein Lakehouse mithilfe der Medallion-Architektur zu
erstellen und zu verwalten, eine Delta-Tabelle mithilfe Ihres Azure Data
Lake Storage (ADLS) Gen2-Kontos mit Azure Databricks zu erstellen und
Daten mit Azure Databricks zu erfassen. Dieser praktische Leitfaden
fÃ¼hrt Sie durch die Schritte, die erforderlich sind, um ein Lakehouse zu
erstellen, Daten darin zu laden und die strukturierten Datenschichten zu
erkunden, um eine effiziente Datenanalyse und Berichterstellung zu
ermÃ¶glichen.

Die Medaillon-Architektur besteht aus drei unterschiedlichen Schichten
(oder Zonen).

- Bronze: Diese erste Ebene, die auch als Rohzone bezeichnet wird,
  speichert Quelldaten in ihrem ursprÃ¼nglichen Format. Die Daten in
  diesem Layer sind in der Regel nur durch AnfÃ¼gen versehen und
  unverÃ¤nderlich.

- Silber: Diese Schicht, die auch als angereicherte Zone bezeichnet
  wird, speichert Daten, die aus der Bronzeschicht stammen. Die Rohdaten
  wurden bereinigt und standardisiert und sind nun als Tabellen (Zeilen
  und Spalten) strukturiert. Es kann auch in andere Daten integriert
  werden, um eine Unternehmensansicht aller GeschÃ¤ftseinheiten wie
  Kunde, Produkt und andere bereitzustellen.

- Gold: Diese letzte Schicht, die auch als kuratierte Zone bezeichnet
  wird, speichert Daten, die aus der Silberschicht stammen. Die Daten
  werden verfeinert, um spezifische nachgelagerte GeschÃ¤fts- und
  Analyseanforderungen zu erfÃ¼llen. Tabellen entsprechen in der Regel
  dem Sternschemadesign, das die Entwicklung von Datenmodellen
  unterstÃ¼tzt, die fÃ¼r Leistung und Benutzerfreundlichkeit optimiert
  sind.

**Ziele**:

- Machen Sie sich mit den Prinzipien der Medallion-Architektur in
  Microsoft Fabric Lakehouse vertraut.

- Implementieren Sie einen strukturierten Datenmanagementprozess mit
  Medallion-Schichten (Bronze, Silber, Gold).

- Wandeln Sie Rohdaten in validierte und angereicherte Daten fÃ¼r
  erweiterte Analysen und Berichte um.

- Lernen Sie Best Practices fÃ¼r Datensicherheit, CI/CD und effiziente
  Datenabfragen kennen.

- Laden Sie Daten mit dem OneLake-Datei-Explorer in OneLake hoch.

- Verwenden Sie ein Fabric-Notizbuch, um Daten in OneLake zu lesen und
  als Delta-Tabelle zurÃ¼ckzuschreiben.

- Analysieren und transformieren Sie Daten mit Spark mithilfe eines
  Fabric-Notebooks.

- Abfragen einer Kopie der Daten in OneLake mit SQL.

- Erstellen Sie mit Azure Databricks eine Delta-Tabelle in Ihrem Azure
  Data Lake Storage (ADLS) Gen2-Konto.

- Erstellen Sie eine OneLake-VerknÃ¼pfung zu einer Delta-Tabelle in ADLS.

- Verwenden Sie Power BI, um Daten Ã¼ber die ADLS-VerknÃ¼pfung zu
  analysieren.

- Lesen und Ã„ndern einer Delta-Tabelle in OneLake mit Azure Databricks.

# Ãœbung 1: Importieren von Beispieldaten in Lakehouse

In dieser Ãœbung durchlaufen Sie den Prozess des Erstellens eines
Lakehouse und des Ladens von Daten in dieses Haus mithilfe von Microsoft
Fabric.

Aufgabe:Weg

## **Aufgabe 1: Erstellen eines Fabric-Workspace**

In dieser Aufgabe erstellen Sie einen Fabric-Workspace. Der Workspace
enthÃ¤lt alle Elemente, die fÃ¼r dieses Lakehouse-Tutorial erforderlich
sind, einschlieÃŸlich Lakehouse, Dataflows, Data Factory-Pipelines,
Notebooks, Power BI-Datasets und Berichte.

1.  Ã–ffnen Sie Ihren Browser, navigieren Sie zur Adressleiste, und geben
    Sie die folgende URL ein oder fÃ¼gen Sie sie ein:
    [https://app.fabric.microsoft.com/](https://app.fabric.microsoft.com/,)
    DrÃ¼cken Sie dann **Enter**.

> ![A search engine window with a red box Description automatically
> generated with medium confidence](./media/image1.png)

2.  Wechseln Sie zurÃ¼ck zum **Power** BI-Fenster. Navigieren Sie im
    linken NavigationsmenÃ¼ der Power BI-Startseite zu und klicken Sie
    auf **Workspaces**.

![](./media/image2.png)

3.  Klicken Sie im Bereich Workspace auf **+**Â **New workspace.**

> ![](./media/image3.png)

4.  Geben Sie im Bereich **Create a workspace**, der auf der rechten
    Seite angezeigt wird, die folgenden Details ein, und klicken Sie auf
    die SchaltflÃ¤che **Apply**.

[TABLE]

> ![](./media/image4.png)

![A screenshot of a computer Description automatically
generated](./media/image5.png)

5.  Warten Sie, bis die Bereitstellung abgeschlossen ist. Es dauert 2-3
    Minuten, bis der Vorgang abgeschlossen ist.

![](./media/image6.png)

## **Aufgabe 2: Erstellen Sie ein Lakehouse**

1.  Klicken Sie auf der Seite **Power BI Fabric Lakehouse Tutorial-XX**
    auf das Power **BI-Symbol** unten links, und wÃ¤hlen Sie **Data
    Engineering** aus.

> ![](./media/image7.png)

2.  WÃ¤hlen Sie auf der **Synapse**Â **Data Engineering**Â **Home** die
    Option **Lakehouse** aus, um ein Lakehouse zu erstellen.

![](./media/image8.png)

![A screenshot of a computer Description automatically
generated](./media/image9.png)

3.  Geben Sie im Dialogfeld **New Lakehouse** im Feld **Name** den Namen
    **wwilakehouse** ein, klicken Sie auf die SchaltflÃ¤che **Create**
    und Ã¶ffnen Sie das neue Lakehouse.

> **Hinweis**: Stellen Sie sicher, dass Sie vor **wwilakehouse**
> Leerzeichen entfernen.
>
> ![](./media/image10.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image11.png)
>
> ![](./media/image12.png)

4.  Sie sehen eine Benachrichtigung mit dem Hinweis **Successfully
    created SQL endpoint**.

> ![](./media/image13.png)

# Ãœbung 2: Implementieren der Medallion-Architektur mit Azure Databricks

## **Aufgabe 1: Einrichten der Bronzeschicht**

1.  WÃ¤hlen Sie auf der Seite **wwilakehouse** das Symbol Mehr neben den
    Dateien (...) und dann **New Subfolder** aus**.**

![](./media/image14.png)

2.  Geben Sie im Popup-Fenster den Ordnernamen als **Bronze** ein, und
    wÃ¤hlen Sie **Create**.

![A screenshot of a computer Description automatically
generated](./media/image15.png)

3.  WÃ¤hlen Sie nun das Symbol More neben den Bronze-Dateien (...) aus,
    wÃ¤hlen Sie **Upload** und dann, **upload files**.

![A screenshot of a computer Description automatically
generated](./media/image16.png)

4.  WÃ¤hlen Sie im Bereich **upload file**Â das Optionsfeld **Upload
    file** aus. Klicken Sie auf die SchaltflÃ¤che **browse** und
    navigieren Sie zu **C:\LabFiles**, wÃ¤hlen Sie dann die erforderliche
    Datei mit Verkaufsdatendateien (2019, 2020, 2021) aus und klicken
    Sie auf die SchaltflÃ¤che **Open**.

WÃ¤hlen Sie dann **Upload** aus, um die Dateien in den neuen Ordner
"Bronze" in Ihrem Lakehouse hochzuladen.

![A screenshot of a computer Description automatically
generated](./media/image17.png)

> ![A screenshot of a computer Description automatically
> generated](./media/image18.png)

5.  Klicken Sie auf den **Bronze**-Ordner, um zu bestÃ¤tigen, dass die
    Dateien erfolgreich hochgeladen wurden und die Dateien.

![A screenshot of a computer Description automatically
generated](./media/image19.png)

# Ãœbung 3: Transformieren von Daten mit Apache Spark und Abfragen mit SQL in der Medallion-Architektur

## **Aufgabe 1: Transformieren von Daten und Laden in die silberne Delta-Tabelle**

Navigieren Sie auf der Seite **wwilakehouse** und klicken Sie in der
Befehlsleiste auf **Open Notebook** und wÃ¤hlen Sie dann **New Notebook**
aus.

![A screenshot of a computer Description automatically
generated](./media/image20.png)

1.  WÃ¤hlen Sie die erste Zelle aus (die derzeit eine *Codezelle ist*)
    und verwenden Sie dann in der dynamischen Symbolleiste oben rechts
    die SchaltflÃ¤che **Mâ†“**, um **die Zelle in eine Markdown-Zelle
    umzuwandeln**.

![A screenshot of a computer Description automatically
generated](./media/image21.png)

2.  Wenn sich die Zelle in eine Markdown-Zelle Ã¤ndert, wird der darin
    enthaltene Text gerendert.

![A screenshot of a computer Description automatically
generated](./media/image22.png)

3.  Verwenden Sie die **ðŸ–‰** SchaltflÃ¤che (Bearbeiten), um die Zelle in
    den Bearbeitungsmodus zu versetzen, ersetzen Sie den gesamten Text
    und Ã¤ndern Sie dann das Markdown wie folgt:

CodeCopy

\# Sales order data exploration

Verwenden Sie den Code in diesem Notebook, um Auftragsdaten zu
untersuchen.

![A screenshot of a computer Description automatically
generated](./media/image23.png)

![A screenshot of a computer Description automatically
generated](./media/image24.png)

4.  Klicken Sie auf eine beliebige Stelle im Notebook auÃŸerhalb der
    Zelle, um die Bearbeitung zu beenden und das gerenderte Markdown
    anzuzeigen.

![A screenshot of a computer Description automatically
generated](./media/image25.png)

5.  Verwenden Sie das Symbol **+** Code unter der Zellenausgabe, um dem
    Notebook eine neue Codezelle hinzuzufÃ¼gen.

![A screenshot of a computer Description automatically
generated](./media/image26.png)

6.  Verwenden Sie nun das Notebook, um die Daten aus der Bronze-Schicht
    in einen Spark-DataFrame zu laden.

WÃ¤hlen Sie die vorhandene Zelle im Notebook aus, die einen einfachen
auskommentierten Code enthÃ¤lt. Markieren und lÃ¶schen Sie diese beiden
Zeilen - Sie benÃ¶tigen diesen Code nicht.

*Hinweis: Mit Notebooks kÃ¶nnen Sie Code in einer Vielzahl von Sprachen
ausfÃ¼hren, einschlieÃŸlich Python, Scala und SQL. In dieser Ãœbung
verwenden Sie PySpark und SQL. Sie kÃ¶nnen auch Markdownzellen
hinzufÃ¼gen, um formatierten Text und Bilder zum Dokumentieren des Codes
bereitzustellen.*

Geben Sie dazu den folgenden Code ein und klicken Sie auf **Run**.

CodeCopy

from pyspark.sql.types import \*

\# Create the schema for the table

orderSchema = StructType(\[

StructField("SalesOrderNumber", StringType()),

StructField("SalesOrderLineNumber", IntegerType()),

StructField("OrderDate", DateType()),

StructField("CustomerName", StringType()),

StructField("Email", StringType()),

StructField("Item", StringType()),

StructField("Quantity", IntegerType()),

StructField("UnitPrice", FloatType()),

StructField("Tax", FloatType())

\])

\# Import all files from bronze folder of lakehouse

df = spark.read.format("csv").option("header",
"true").schema(orderSchema).load("Files/bronze/\*.csv")

\# Display the first 10 rows of the dataframe to preview your data

display(df.head(10))

![](./media/image27.png)

***Hinweis**: Da dies das erste Mal ist, dass Sie Spark-Code in diesem
Notebook ausfÃ¼hren, muss eine Spark-Sitzung gestartet werden. Das
bedeutet, dass die erste AusfÃ¼hrung etwa eine Minute dauern kann.
Nachfolgende DurchlÃ¤ufe sind schneller.*

![A screenshot of a computer Description automatically
generated](./media/image28.png)

7.  Der Code, den Sie ausgefÃ¼hrt haben, lud die Daten aus den
    CSV-Dateien im **Bronze**-Ordner in einen Spark-Datenrahmen und
    zeigte dann die ersten Zeilen des Datenrahmens an.

> **Hinweis**: Sie kÃ¶nnen den Inhalt der Zellenausgabe lÃ¶schen,
> ausblenden und automatisch in der GrÃ¶ÃŸe Ã¤ndern, indem Sie die
> SchaltflÃ¤che **...** MenÃ¼ oben links im Ausgabebereich.

8.  Jetzt fÃ¼gen Sie **Spalten fÃ¼r die DatenÃ¼berprÃ¼fung und -bereinigung
    hinzu**, indem Sie einen PySpark-Datenrahmen verwenden, um Spalten
    hinzuzufÃ¼gen und die Werte einiger der vorhandenen Spalten zu
    aktualisieren. Verwenden Sie die SchaltflÃ¤che +, um **einen neuen
    Codeblock hinzuzufÃ¼gen**, und fÃ¼gen Sie der Zelle den folgenden Code
    hinzu:

> CodeCopy
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> \# Add columns IsFlagged, CreatedTS and ModifiedTS
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \<
> '2019-08-01',True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()).withColumn("ModifiedTS",
> current_timestamp())
>
> \# Update CustomerName to "Unknown" if CustomerName null or empty
>
> df = df.withColumn("CustomerName", when((col("CustomerName").isNull()
> |
> (col("CustomerName")=="")),lit("Unknown")).otherwise(col("CustomerName")))
>
> Die erste Zeile des Codes importiert die erforderlichen Funktionen aus
> PySpark. AnschlieÃŸend fÃ¼gen Sie dem Datenrahmen neue Spalten hinzu,
> damit Sie den Namen der Quelldatei nachverfolgen kÃ¶nnen, ob der
> Auftrag als vor dem GeschÃ¤ftsjahr von Interesse gekennzeichnet wurde
> und wann die Zeile erstellt und geÃ¤ndert wurde.
>
> SchlieÃŸlich aktualisieren Sie die Spalte "CustomerName" auf "Unknown",
> wenn sie null oder leer ist.
>
> FÃ¼hren Sie dann die Zelle aus, um den Code mit der SchaltflÃ¤che
> **\*\*â–·** (*Run Cell*)\*\* auszufÃ¼hren.

![A screenshot of a computer Description automatically
generated](./media/image29.png)

![A screenshot of a computer Description automatically
generated](./media/image30.png)

9.  Als NÃ¤chstes definieren Sie das Schema fÃ¼r die Tabelle
    **sales_silver** in der sales-Datenbank im Delta Lake-Format.
    Erstellen Sie einen neuen Codeblock, und fÃ¼gen Sie der Zelle den
    folgenden Code hinzu:

> CodeCopy

from pyspark.sql.types import \*

from delta.tables import \*

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

Â  Â  StructField("SalesOrderNumber", StringType(), True),

Â  Â  StructField("SalesOrderLineNumber", IntegerType(), True),

Â  Â  StructField("OrderDate", DateType(), True),

Â  Â  StructField("CustomerName", StringType(), True),

Â  Â  StructField("Email", StringType(), True),

Â  Â  StructField("Item", StringType(), True),

Â  Â  StructField("Quantity", IntegerType(), True),

Â  Â  StructField("UnitPrice", FloatType(), True),

Â  Â  StructField("Tax", FloatType(), True),

Â  Â  StructField("FileName", StringType(), True),

Â  Â  StructField("IsFlagged", BooleanType(), True),

Â  Â  StructField("CreatedTS", TimestampType(), True),

Â  Â  StructField("ModifiedTS", TimestampType(), True)

\])

\# Create or replace the sales_silver table with the defined schema

DeltaTable.createIfNotExists(spark) \\

Â  Â  .tableName("wwilakehouse.sales_silver") \\

Â  Â  .addColumns(silver_table_schema) \\

Â  Â  .execute()

Â  Â 

10. FÃ¼hren Sie die Zelle aus, um den Code mit der SchaltflÃ¤che **\*\*â–·**
    (*Run Cell*)\*\* auszufÃ¼hren.

11. WÃ¤hlen Sie die Option **...** im Abschnitt Tabellen des
    Lakehouse-Explorer-Bereichs, und wÃ¤hlen Sie **Refresh** aus. Die
    neue **sales_silver** Tabelle sollte nun aufgelistet sein. Das **â–²**
    (Dreieckssymbol) zeigt an, dass es sich um eine Delta-Tabelle
    handelt.

> **Hinweis**: Wenn die neue Tabelle nicht angezeigt wird, warten Sie
> einige Sekunden, und wÃ¤hlen Sie dann erneut **Refresh** aus, oder
> aktualisieren Sie die gesamte Browserregisterkarte.
>
> ![A screenshot of a computer Description automatically
> generated](./media/image31.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image32.png)

12. Jetzt fÃ¼hren Sie einen **upsert operation**Â fÃ¼r eine Delta-Tabelle
    aus, aktualisieren vorhandene DatensÃ¤tze basierend auf bestimmten
    Bedingungen und fÃ¼gen neue DatensÃ¤tze ein, wenn keine
    Ãœbereinstimmung gefunden wird. FÃ¼gen Sie einen neuen Codeblock
    hinzu, und fÃ¼gen Sie den folgenden Code ein:

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> from delta.tables import \*
>
> \# Define the schema for the source data
>
> orderSchema = StructType(\[
>
> StructField("SalesOrderNumber", StringType(), True),
>
> StructField("SalesOrderLineNumber", IntegerType(), True),
>
> StructField("OrderDate", DateType(), True),
>
> StructField("CustomerName", StringType(), True),
>
> StructField("Email", StringType(), True),
>
> StructField("Item", StringType(), True),
>
> StructField("Quantity", IntegerType(), True),
>
> StructField("UnitPrice", FloatType(), True),
>
> StructField("Tax", FloatType(), True)
>
> \])
>
> \# Read data from the bronze folder into a DataFrame
>
> df = spark.read.format("csv").option("header",
> "true").schema(orderSchema).load("Files/bronze/\*.csv")
>
> \# Add additional columns
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \< '2019-08-01',
> True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()) \\
>
> .withColumn("ModifiedTS", current_timestamp()) \\
>
> .withColumn("CustomerName", when((col("CustomerName").isNull()) |
> (col("CustomerName") == ""),
> lit("Unknown")).otherwise(col("CustomerName")))
>
> \# Define the path to the Delta table
>
> deltaTablePath = "Tables/sales_silver"
>
> \# Create a DeltaTable object for the existing Delta table
>
> deltaTable = DeltaTable.forPath(spark, deltaTablePath)
>
> \# Perform the merge (upsert) operation
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> df.alias('updates'),
>
> 'silver.SalesOrderNumber = updates.SalesOrderNumber AND \\
>
> silver.OrderDate = updates.OrderDate AND \\
>
> silver.CustomerName = updates.CustomerName AND \\
>
> silver.Item = updates.Item'
>
> ) \\
>
> .whenMatchedUpdate(set = {
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "Email": "updates.Email",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .whenNotMatchedInsert(values = {
>
> "SalesOrderNumber": "updates.SalesOrderNumber",
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "OrderDate": "updates.OrderDate",
>
> "CustomerName": "updates.CustomerName",
>
> "Email": "updates.Email",
>
> "Item": "updates.Item",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "CreatedTS": "current_timestamp()",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .execute()

13. FÃ¼hren Sie die Zelle aus, um den Code mit der SchaltflÃ¤che **\*\*â–·**
    (***Run Cell***) \*\* auszufÃ¼hren.

![A screenshot of a computer Description automatically
generated](./media/image33.png)

Dieser Vorgang ist wichtig, da er es Ihnen ermÃ¶glicht, vorhandene
DatensÃ¤tze in der Tabelle basierend auf den Werten bestimmter Spalten zu
aktualisieren und neue DatensÃ¤tze einzufÃ¼gen, wenn keine Ãœbereinstimmung
gefunden wird. Dies ist eine hÃ¤ufige Anforderung, wenn Sie Daten aus
einem Quellsystem laden, das Aktualisierungen vorhandener und neuer
DatensÃ¤tze enthalten kann.

Sie verfÃ¼gen nun Ã¼ber Daten in Ihrer Silver-Delta-Tabelle, die fÃ¼r die
weitere Transformation und Modellierung bereit sind.

Sie haben erfolgreich Daten aus Ihrer Bronze-Schicht Ã¼bernommen,
transformiert und in eine silberne Delta-Tabelle geladen. Jetzt
verwenden Sie ein neues Notebook, um die Daten weiter zu transformieren,
sie in ein Sternschema zu modellieren und in goldene Delta-Tabellen zu
laden.

*Beachten Sie, dass Sie all dies in einem einzigen Notebook hÃ¤tten tun
kÃ¶nnen, aber fÃ¼r die Zwecke dieser Ãœbung verwenden Sie separate
Notebooks, um den Prozess der Transformation von Daten von Bronze zu
Silber und dann von Silber zu Gold zu veranschaulichen. Dies kann beim
Debuggen, bei der Fehlerbehebung und bei der Wiederverwendung hilfreich
sein*.

## **Aufgabe 2: Laden von Daten in Gold-Delta-Tabellen**

1.  ZurÃ¼ck zur Startseite des Fabric Lakehouse Tutorial-29.

> ![A screenshot of a computer Description automatically
> generated](./media/image34.png)

2.  WÃ¤hlen Sie **wwilakehouse** aus**.**

![A screenshot of a computer Description automatically
generated](./media/image35.png)

3.  Im Explorer-Bereich "Lakehouse" sollte die Tabelle
    **"sales_silver"** im Abschnitt **Tables** des Explorer-Bereichs
    aufgefÃ¼hrt sein.

![A screenshot of a computer Description automatically
generated](./media/image36.png)

4.  Erstellen Sie nun ein neues Notebook mit dem NamenÂ **Transform data
    for Gold**. Navigieren Sie dazu und klicken Sie in der Befehlsleiste
    auf **Open Notebook** und wÃ¤hlen Sie dann **New Notebook** aus.

![A screenshot of a computer Description automatically
generated](./media/image37.png)

5.  Entfernen Sie im vorhandenen Codeblock den Textbaustein, und **fÃ¼gen
    Sie den folgenden Code hinzu,** um Daten in Ihren Datenrahmen zu
    laden, mit dem Erstellen Ihres Sternschemas zu beginnen und es dann
    auszufÃ¼hren:

> CodeCopy

\# Load data to the dataframe as a starting point to create the gold
layer

df = spark.read.table("wwilakehouse.sales_silver")

\# Display the first few rows of the dataframe to verify the data

df.show()

![A screenshot of a computer Description automatically
generated](./media/image38.png)

6.  **FÃ¼gen Sie als NÃ¤chstes einen neuen Codeblock hinzu,** und fÃ¼gen
    Sie den folgenden Code ein, um Ihre Datumsdimensionstabelle zu
    erstellen und auszufÃ¼hren:

Â from pyspark.sql.types import \*

Â from delta.tables import\*

Â  Â 

Â # Define the schema for the dimdate_gold table

Â DeltaTable.createIfNotExists(spark) \\

Â  Â  Â .tableName("wwilakehouse.dimdate_gold") \\

Â  Â  Â .addColumn("OrderDate", DateType()) \\

Â  Â  Â .addColumn("Day", IntegerType()) \\

Â  Â  Â .addColumn("Month", IntegerType()) \\

Â  Â  Â .addColumn("Year", IntegerType()) \\

Â  Â  Â .addColumn("mmmyyyy", StringType()) \\

Â  Â  Â .addColumn("yyyymm", StringType()) \\

Â  Â  Â .execute()

![A screenshot of a computer Description automatically
generated](./media/image39.png)

**Hinweis**: Sie kÃ¶nnen den Befehl display(df) jederzeit ausfÃ¼hren, um
den Fortschritt Ihrer Arbeit zu Ã¼berprÃ¼fen. In diesem Fall wÃ¼rden Sie
'display(dfdimDate_gold)" ausfÃ¼hren, um den Inhalt des dimDate_gold
Datenrahmens anzuzeigen.

7.  FÃ¼gen Sie in einem neuen Codeblock **den folgenden Code hinzu, und
    fÃ¼hren Sie ihn aus**, um einen Datenrahmen fÃ¼r die Datumsdimension
    zu erstellen, **dimdate_gold**:

> CodeCopy

from pyspark.sql.functions import col, dayofmonth, month, year,
date_format

Â  Â 

Â # Create dataframe for dimDate_gold

Â  Â 

dfdimDate_gold
=df.dropDuplicates(\["OrderDate"\]).select(col("OrderDate"), \\

Â  Â  Â  Â  Â dayofmonth("OrderDate").alias("Day"), \\

Â  Â  Â  Â  Â month("OrderDate").alias("Month"), \\

Â  Â  Â  Â  Â year("OrderDate").alias("Year"), \\

Â  Â  Â  Â  Â date_format(col("OrderDate"), "MMM-yyyy").alias("mmmyyyy"), \\

Â  Â  Â  Â  Â date_format(col("OrderDate"), "yyyyMM").alias("yyyymm"), \\

Â  Â  Â ).orderBy("OrderDate")

Â # Display the first 10 rows of the dataframe to preview your data

display(dfdimDate_gold.head(10))

![A screenshot of a computer Description automatically
generated](./media/image40.png)

![A screenshot of a computer Description automatically
generated](./media/image41.png)

8.  Sie unterteilen den Code in neue CodeblÃ¶cke, damit Sie verstehen und
    beobachten kÃ¶nnen, was im Notebook geschieht, wÃ¤hrend Sie die Daten
    transformieren. FÃ¼gen Sie in einem anderen neuen Codeblock **den
    folgenden Code hinzu, und fÃ¼hren Sie ihn aus**, um die
    Datumsdimension zu aktualisieren, wenn neue Daten eingehen:

> CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')
>
> dfUpdates = dfdimDate_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "OrderDate": "updates.OrderDate",
>
> "Day": "updates.Day",
>
> "Month": "updates.Month",
>
> "Year": "updates.Year",
>
> "mmmyyyy": "updates.mmmyyyy",
>
> "yyyymm": "yyyymm"
>
> }
>
> ) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image42.png)

> Ihre Datumsdimension ist eingerichtet.

![A screenshot of a computer Description automatically
generated](./media/image43.png)

## **Aufgabe 3: Erstellen der Kundendimension.**

1.  Um die Kundendimensionstabelle zu erstellen, **fÃ¼gen Sie einen neuen
    Codeblock hinzu**, fÃ¼gen Sie den folgenden Code ein, und fÃ¼hren Sie
    ihn aus:

> CodeCopy

Â from pyspark.sql.types import \*

Â from delta.tables import \*

Â  Â 

Â # Create customer_gold dimension delta table

Â DeltaTable.createIfNotExists(spark) \\

Â  Â  Â .tableName("wwilakehouse.dimcustomer_gold") \\

Â  Â  Â .addColumn("CustomerName", StringType()) \\

Â  Â  Â .addColumn("Email", Â StringType()) \\

Â  Â  Â .addColumn("First", StringType()) \\

Â  Â  Â .addColumn("Last", StringType()) \\

Â  Â  Â .addColumn("CustomerID", LongType()) \\

Â  Â  Â .execute()

![A screenshot of a computer Description automatically
generated](./media/image44.png)

![A screenshot of a computer Description automatically
generated](./media/image45.png)

2.  FÃ¼gen Sie in einem neuen Codeblock **den folgenden Code hinzu, und
    fÃ¼hren Sie ihn aus**, um doppelte Kunden zu lÃ¶schen, bestimmte
    Spalten auszuwÃ¤hlen und die Spalte "CustomerName" zu teilen, um die
    Spalten "First " und "Last" zu erstellen:

> CodeCopy
>
> from pyspark.sql.functions import col, split
>
> \# Create customer_silver dataframe
>
> dfdimCustomer_silver =
> df.dropDuplicates(\["CustomerName","Email"\]).select(col("CustomerName"),col("Email"))
> \\
>
> .withColumn("First",split(col("CustomerName"), " ").getItem(0)) \\
>
> .withColumn("Last",split(col("CustomerName"), " ").getItem(1))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimCustomer_silver.head(10))

![A screenshot of a computer Description automatically
generated](./media/image46.png)

Hier haben Sie einen neuen DataFrame-dfdimCustomer_silver erstellt,
indem Sie verschiedene Transformationen durchgefÃ¼hrt haben, z. B. das
LÃ¶schen von Duplikaten, das AuswÃ¤hlen bestimmter Spalten und das Teilen
der Spalte "CustomerName", um die Spalten "First" und "Last" zu
erstellen. Das Ergebnis ist ein DataFrame mit bereinigten und
strukturierten Kundendaten, einschlieÃŸlich separater Spalten "First" und
"Last", die aus der Spalte "Kundenname" extrahiert wurden.

![A screenshot of a computer Description automatically
generated](./media/image47.png)

3.  Als NÃ¤chstes erstellen wir **die ID-Spalte fÃ¼r unsere Kunden**.
    FÃ¼gen Sie in einem neuen Codeblock Folgendes ein, und fÃ¼hren Sie es
    aus:

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col,
when, coalesce, max, lit

\# Read the existing data from the Delta table

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

\# Find the maximum CustomerID or use 0 if the table is empty

MAXCustomerID =
dfdimCustomer_temp.select(coalesce(max(col("CustomerID")),
lit(0)).alias("MAXCustomerID")).first()\[0\]

\# Assume dfdimCustomer_silver is your source DataFrame with new data

\# Here, we select only the new customers by doing a left anti join

dfdimCustomer_gold = dfdimCustomer_silver.join(

Â  Â  dfdimCustomer_temp,

Â  Â  (dfdimCustomer_silver.CustomerName ==
dfdimCustomer_temp.CustomerName) &

Â  Â  (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email),

Â  Â  "left_anti"

)

\# Add the CustomerID column with unique values starting from
MAXCustomerID + 1

dfdimCustomer_gold = dfdimCustomer_gold.withColumn(

Â  Â  "CustomerID",

Â  Â  monotonically_increasing_id() + MAXCustomerID + 1

)

\# Display the first 10 rows of the dataframe to preview your data

dfdimCustomer_gold.show(10)

![](./media/image48.png)

![A screenshot of a computer Description automatically
generated](./media/image49.png)

4.  Jetzt stellen Sie sicher, dass Ihre Kundentabelle auf dem neuesten
    Stand bleibt, wenn neue Daten eingehen. **FÃ¼gen Sie in einem neuen
    Codeblock** Folgendes ein, und fÃ¼hren Sie es aus:

> CodeCopy

from delta.tables import DeltaTable

\# Define the Delta table path

deltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')

\# Use dfUpdates to refer to the DataFrame with new or updated records

dfUpdates = dfdimCustomer_gold

\# Perform the merge operation to update or insert new records

deltaTable.alias('silver') \\

Â  .merge(

Â  Â  dfUpdates.alias('updates'),

Â  Â  'silver.CustomerName = updates.CustomerName AND silver.Email =
updates.Email'

Â  ) \\

Â  .whenMatchedUpdate(set =

Â  Â  {

Â  Â  Â  "CustomerName": "updates.CustomerName",

Â  Â  Â  "Email": "updates.Email",

Â  Â  Â  "First": "updates.First",

Â  Â  Â  "Last": "updates.Last",

Â  Â  Â  "CustomerID": "updates.CustomerID"

Â  Â  }

Â  ) \\

Â  .whenNotMatchedInsert(values =

Â  Â  {

Â  Â  Â  "CustomerName": "updates.CustomerName",

Â  Â  Â  "Email": "updates.Email",

Â  Â  Â  "First": "updates.First",

Â  Â  Â  "Last": "updates.Last",

Â  Â  Â  "CustomerID": "updates.CustomerID"

Â  Â  }

Â  ) \\

Â  .execute()

![](./media/image50.png)

![A screenshot of a computer Description automatically
generated](./media/image51.png)

5.  Jetzt wiederholen Sie **diese Schritte, um Ihre Produktdimension zu
    erstellen**. FÃ¼gen Sie in einem neuen Codeblock Folgendes ein, und
    fÃ¼hren Sie es aus:

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.dimproduct_gold") \\
>
> .addColumn("ItemName", StringType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("ItemInfo", StringType()) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image52.png)

![A screenshot of a computer Description automatically
generated](./media/image53.png)

6.  **FÃ¼gen Sie einen weiteren Codeblock** hinzu, um den
    **product_silver** Datenrahmen zu erstellen.

> CodeCopy
>
> from pyspark.sql.functions import col, split, lit
>
> \# Create product_silver dataframe
>
> dfdimProduct_silver =
> df.dropDuplicates(\["Item"\]).select(col("Item")) \\
>
> .withColumn("ItemName",split(col("Item"), ", ").getItem(0)) \\
>
> .withColumn("ItemInfo",when((split(col("Item"), ",
> ").getItem(1).isNull() | (split(col("Item"), ",
> ").getItem(1)=="")),lit("")).otherwise(split(col("Item"), ",
> ").getItem(1)))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimProduct_silver.head(10))

![A screenshot of a computer Description automatically
generated](./media/image54.png)

![A screenshot of a computer Description automatically
generated](./media/image55.png)

7.  Jetzt erstellen Sie IDs fÃ¼r Ihre **dimProduct_gold Tabelle**. FÃ¼gen
    Sie einem neuen Codeblock die folgende Syntax hinzu, und fÃ¼hren Sie
    sie aus:

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col, lit,
max, coalesce

\#dfdimProduct_temp = dfdimProduct_silver

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

MAXProductID =
dfdimProduct_temp.select(coalesce(max(col("ItemID")),lit(0)).alias("MAXItemID")).first()\[0\]

dfdimProduct_gold =
dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName
== dfdimProduct_temp.ItemName) & (dfdimProduct_silver.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left_anti")

dfdimProduct_gold =
dfdimProduct_gold.withColumn("ItemID",monotonically_increasing_id() +
MAXProductID + 1)

\# Display the first 10 rows of the dataframe to preview your data

display(dfdimProduct_gold.head(10))

![A screenshot of a computer Description automatically
generated](./media/image56.png)

Dadurch wird die nÃ¤chste verfÃ¼gbare Produkt-ID basierend auf den
aktuellen Daten in der Tabelle berechnet, diese neuen IDs den Produkten
zugewiesen und dann die aktualisierten Produktinformationen angezeigt.

![A screenshot of a computer Description automatically
generated](./media/image57.png)

8.  Ã„hnlich wie bei Ihren anderen Dimensionen mÃ¼ssen Sie sicherstellen,
    dass Ihre Produkttabelle auf dem neuesten Stand bleibt, wenn neue
    Daten eingehen. **FÃ¼gen Sie in einem neuen Codeblock** Folgendes
    ein, und fÃ¼hren Sie es aus:

CodeCopy

from delta.tables import \*

deltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')

dfUpdates = dfdimProduct_gold

deltaTable.alias('silver') \\

.merge(

dfUpdates.alias('updates'),

'silver.ItemName = updates.ItemName AND silver.ItemInfo =
updates.ItemInfo'

) \\

.whenMatchedUpdate(set =

{

}

) \\

.whenNotMatchedInsert(values =

{

"ItemName": "updates.ItemName",

"ItemInfo": "updates.ItemInfo",

"ItemID": "updates.ItemID"

}

) \\

.execute()

![A screenshot of a computer Description automatically
generated](./media/image58.png)

![A screenshot of a computer Description automatically
generated](./media/image59.png)

**Nachdem Sie nun Ihre Dimensionen erstellt haben, besteht der letzte
Schritt darin, die Faktentabelle zu erstellen.**

9.  **FÃ¼gen Sie in einem neuen Codeblock** den folgenden Code ein, und
    fÃ¼hren Sie ihn aus, um die **Faktentabelle zu erstellen**:

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.factsales_gold") \\
>
> .addColumn("CustomerID", LongType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("OrderDate", DateType()) \\
>
> .addColumn("Quantity", IntegerType()) \\
>
> .addColumn("UnitPrice", FloatType()) \\
>
> .addColumn("Tax", FloatType()) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image60.png)

![A screenshot of a computer Description automatically
generated](./media/image61.png)

10. **FÃ¼gen Sie in einem neuen Codeblock** den folgenden Code ein, und
    fÃ¼hren Sie ihn aus, um einen **neuen Datenrahmen** zu erstellen, um
    Verkaufsdaten mit Kunden- und Produktinformationen zu kombinieren,
    einschlieÃŸlich Kunden-ID, Artikel-ID, Bestelldatum, Menge,
    StÃ¼ckpreis und Steuer:

CodeCopy

from pyspark.sql import SparkSession

from pyspark.sql.functions import split, col, when, lit

from pyspark.sql.types import StructType, StructField, StringType,
IntegerType, DateType, FloatType, BooleanType, TimestampType

\# Initialize Spark session

spark = SparkSession.builder \\

Â  Â  .appName("DeltaTableUpsert") \\

Â  Â  .config("spark.sql.extensions",
"io.delta.sql.DeltaSparkSessionExtension") \\

Â  Â  .config("spark.sql.catalog.spark_catalog",
"org.apache.spark.sql.delta.catalog.DeltaCatalog") \\

Â  Â  .getOrCreate()

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

Â  Â  StructField("SalesOrderNumber", StringType(), True),

Â  Â  StructField("SalesOrderLineNumber", IntegerType(), True),

Â  Â  StructField("OrderDate", DateType(), True),

Â  Â  StructField("CustomerName", StringType(), True),

Â  Â  StructField("Email", StringType(), True),

Â  Â  StructField("Item", StringType(), True),

Â  Â  StructField("Quantity", IntegerType(), True),

Â  Â  StructField("UnitPrice", FloatType(), True),

Â  Â  StructField("Tax", FloatType(), True),

Â  Â  StructField("FileName", StringType(), True),

Â  Â  StructField("IsFlagged", BooleanType(), True),

Â  Â  StructField("CreatedTS", TimestampType(), True),

Â  Â  StructField("ModifiedTS", TimestampType(), True)

\])

\# Define the path to the Delta table (ensure this path is correct)

delta_table_path =
"abfss://\<container\>@\<storage-account\>.dfs.core.windows.net/path/to/wwilakehouse/sales_silver"

\# Create a DataFrame with the defined schema

empty_df = spark.createDataFrame(\[\], silver_table_schema)

\# Register the Delta table in the Metastore

spark.sql(f"""

Â  Â  CREATE TABLE IF NOT EXISTS wwilakehouse.sales_silver

Â  Â  USING DELTA

Â  Â  LOCATION '{delta_table_path}'

""")

\# Load data into DataFrame

df = spark.read.table("wwilakehouse.sales_silver")

\# Perform transformations on df

df = df.withColumn("ItemName", split(col("Item"), ", ").getItem(0)) \\

Â  Â  .withColumn("ItemInfo", when(

Â  Â  Â  Â  (split(col("Item"), ", ").getItem(1).isNull()) |
(split(col("Item"), ", ").getItem(1) == ""),

Â  Â  Â  Â  lit("")

Â  Â  ).otherwise(split(col("Item"), ", ").getItem(1)))

\# Load additional DataFrames for joins

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

\# Create Sales_gold dataframe

dffactSales_gold = df.alias("df1").join(dfdimCustomer_temp.alias("df2"),
(df.CustomerName == dfdimCustomer_temp.CustomerName) & (df.Email ==
dfdimCustomer_temp.Email), "left") \\

Â  Â  .join(dfdimProduct_temp.alias("df3"), (df.ItemName ==
dfdimProduct_temp.ItemName) & (df.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left") \\

Â  Â  .select(

Â  Â  Â  Â  col("df2.CustomerID"),

Â  Â  Â  Â  col("df3.ItemID"),

Â  Â  Â  Â  col("df1.OrderDate"),

Â  Â  Â  Â  col("df1.Quantity"),

Â  Â  Â  Â  col("df1.UnitPrice"),

Â  Â  Â  Â  col("df1.Tax")

Â  Â  ).orderBy(col("df1.OrderDate"), col("df2.CustomerID"),
col("df3.ItemID"))

\# Show the result

dffactSales_gold.show()

![A screenshot of a computer Description automatically
generated](./media/image62.png)

![A screenshot of a computer Description automatically
generated](./media/image63.png)

![A screenshot of a computer Description automatically
generated](./media/image64.png)

1.  Jetzt stellen Sie sicher, dass die Verkaufsdaten auf dem neuesten
    Stand bleiben, indem Sie den folgenden Code in einem **neuen
    Codeblock ausfÃ¼hren**:

> CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')
>
> dfUpdates = dffactSales_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate AND silver.CustomerID =
> updates.CustomerID AND silver.ItemID = updates.ItemID'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "CustomerID": "updates.CustomerID",
>
> "ItemID": "updates.ItemID",
>
> "OrderDate": "updates.OrderDate",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax"
>
> }
>
> ) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image65.png)

Hier verwenden Sie den ZusammenfÃ¼hrungsvorgang von Delta Lake, um die
factsales_gold Tabelle mit neuen Umsatzdaten (dffactSales_gold) zu
synchronisieren und zu aktualisieren. Der Vorgang vergleicht das
Bestelldatum, die Kunden-ID und die Artikel-ID zwischen den vorhandenen
Daten (silberne Tabelle) und den neuen Daten (aktualisierter DataFrame),
aktualisiert Ã¼bereinstimmende DatensÃ¤tze und fÃ¼gt bei Bedarf neue
DatensÃ¤tze ein.

![A screenshot of a computer Description automatically
generated](./media/image66.png)

Sie verfÃ¼gen nun Ã¼ber eine modeledÂ **gold**Â layer, die fÃ¼r Berichte und
Analysen verwendet werden kann.

# Ãœbung 4: Einrichten der KonnektivitÃ¤t zwischen Azure Databricks und Azure Data Lake Storage (ADLS) Gen 2

Erstellen Sie nun mithilfe Ihres Azure Data Lake Storage (ADLS)
Gen2-Kontos mithilfe von Azure Databricks eine Delta-Tabelle.
AnschlieÃŸend erstellen Sie eine OneLake-VerknÃ¼pfung zu einer
Delta-Tabelle in ADLS und verwenden Power BI, um Daten Ã¼ber die
ADLS-VerknÃ¼pfung zu analysieren.

## **Aufgabe 0: EinlÃ¶sen einer Azure-Karte und Aktivieren des Azure-Abonnements**

1.  Navigieren Sie unter folgendem Link !!
    https://www.microsoftazurepass.com/!! und klicken Sie auf die
    SchaltflÃ¤che **Start**.

![](./media/image67.png)

2.  Geben Sie auf der Microsoft-Anmeldeseite die **Tenant-ID** ein**,**
    klicken Sie auf **Next**.

![](./media/image68.png)

3.  Geben Sie auf der nÃ¤chsten Seite Ihr Passwort ein und klicken Sie
    auf **Sign In**.

![](./media/image69.png)

![A screenshot of a computer error Description automatically
generated](./media/image70.png)

4.  Sobald Sie angemeldet sind, klicken Sie auf der Microsoft
    Azure-Seite auf die Registerkarte **Confirm Microsoft Account**.

![](./media/image71.png)

5.  Geben Sie auf der nÃ¤chsten Seite den Promo-Code und die
    Captcha-Zeichen ein und klicken Sie auf **Submit.**

![A screenshot of a computer Description automatically
generated](./media/image72.png)

![A screenshot of a computer error Description automatically
generated](./media/image73.png)

6.  Geben Sie auf der Seite Ihr Profil Ihre Profildaten ein und klicken
    Sie auf **Sign up.**

7.  Wenn Sie dazu aufgefordert werden, registrieren Sie sich fÃ¼r die
    Multifaktor-Authentifizierung, und melden Sie sich dann beim
    Azure-Portal an, indem Sie zum folgenden Link navigieren!!
    <https://portal.azure.com/#home>!!

![](./media/image74.png)

8.  Geben Sie in der Suchleiste Abonnement ein und klicken Sie auf das
    Symbol Abonnement unter **Services.**

![A screenshot of a computer Description automatically
generated](./media/image75.png)

9.  Nach erfolgreicher EinlÃ¶sung von Azure Pass wird eine Abonnement-ID
    generiert.

![](./media/image76.png)

## **Aufgabe 1: Erstellen eines Azure Data Storage-Kontos**

1.  Melden Sie sich mit Ihren Azure-Anmeldeinformationen bei Ihrem
    Azure-Portal an.

2.  WÃ¤hlen Sie auf der Startseite im MenÃ¼ des linken Portals die Option
    **Storage Account** aus, um eine Liste Ihrer Speicherkonten
    anzuzeigen. Wenn das PortalmenÃ¼ nicht angezeigt wird, wÃ¤hlen Sie die
    MenÃ¼schaltflÃ¤che aus, um es zu aktivieren.

![A screenshot of a computer Description automatically
generated](./media/image77.png)

3.  WÃ¤hlen Sie auf der Seite **Storage Account** die Option **Create**.

![A screenshot of a computer Description automatically
generated](./media/image78.png)

4.  Geben Sie auf der Registerkarte Grundlagen nach Auswahl einer
    Ressourcengruppe die wesentlichen Informationen fÃ¼r Ihr
    Speicherkonto an:

[TABLE]

Lassen Sie die anderen Einstellungen unverÃ¤ndert und wÃ¤hlen Sie
**Review + Create** aus, um die Standardoptionen zu Ã¼bernehmen und mit
der Validierung und Erstellung des Kontos fortzufahren.

Hinweis: Wenn Sie noch keine Ressourcengruppe erstellt haben, kÃ¶nnen Sie
auf "**Create New**" klicken und eine neue Ressource fÃ¼r Ihr
Speicherkonto erstellen.

![](./media/image79.png)

![](./media/image80.png)

![A screenshot of a computer Description automatically
generated](./media/image81.png)

![A screenshot of a computer Description automatically
generated](./media/image82.png)

![A screenshot of a computer Description automatically
generated](./media/image83.png)

5.  Wenn Sie zur Registerkarte **Review + create** navigieren, fÃ¼hrt
    Azure die ÃœberprÃ¼fung fÃ¼r die von Ihnen ausgewÃ¤hlten
    Speicherkontoeinstellungen aus. Wenn die ÃœberprÃ¼fung erfolgreich
    ist, kÃ¶nnen Sie mit dem Erstellen des Speicherkontos fortfahren.

Wenn die Validierung fehlschlÃ¤gt, gibt das Portal an, welche
Einstellungen geÃ¤ndert werden mÃ¼ssen.

![A screenshot of a computer Description automatically
generated](./media/image84.png)

![A screenshot of a computer Description automatically
generated](./media/image85.png)

Sie haben jetzt Ihr Azure-Datenspeicherkonto erfolgreich erstellt.

6.  Navigieren Sie zur Seite Storage Account indem Sie in der Suchleiste
    oben auf der Seite suchen, und wÃ¤hlen Sie das neu erstellte
    Speicherkonto aus.

![A screenshot of a computer Description automatically
generated](./media/image86.png)

7.  Navigieren Sie auf der Seite des Speicherkontos zu **Container**
    unter **Data storage,** und erstellen Sie im linken
    Navigationsbereich einen neuen Container mit dem Namen!!
    Medaillon1!! und klicken Sie auf die SchaltflÃ¤che **Create**.Â 

Â 

![A screenshot of a computer Description automatically
generated](./media/image87.png)

8.  Navigieren Sie nun zurÃ¼ck auf der **storage account**, und wÃ¤hlen
    Sie im linken NavigationsmenÃ¼ **Endpoints** aus. Scrollen Sie nach
    unten, kopieren Sie die URL des **Primary endpoint URL**, und fÃ¼gen
    Sie sie in einem Editor ein. Dies ist hilfreich beim Erstellen der
    VerknÃ¼pfung.

![](./media/image88.png)

9.  Navigieren Sie auf Ã¤hnliche Weise zu den **Access keys** im selben
    Navigationsbereich.

![A screenshot of a computer Description automatically
generated](./media/image89.png)

## **Aufgabe 2: Erstellen einer Delta-Tabelle, Erstellen einer VerknÃ¼pfung und Analysieren der Daten in Ihrem Lakehouse**

1.  WÃ¤hlen Sie in Ihrem Lakehouse die Ellipsen **aus (...)** neben
    Dateien und wÃ¤hlen Sie dann **New shortcut**.

![](./media/image90.png)

2.  WÃ¤hlen Sie auf dem Bildschirm **New Shortcut** die Kachel **Azure
    Data Lake Storage Gen2** aus.

![Screenshot of the tile options in the New shortcut
screen.](./media/image91.png)

3.  Angeben der Verbindungsdetails fÃ¼r die VerknÃ¼pfung:

[TABLE]

4.  Und klicken Sie auf **Next**.

![A screenshot of a computer Description automatically
generated](./media/image92.png)

5.  Dadurch wird eine VerknÃ¼pfung mit Ihrem Azure Storage-Container
    hergestellt. WÃ¤hlen Sie den Speicher aus und klicken Sie auf die
    SchaltflÃ¤che **Next**.

![A screenshot of a computer Description automatically
generated](./media/image93.png)

![A screenshot of a computer Description automatically
generated](./media/image94.png)![A screenshot of a computer Description
automatically generated](./media/image95.png)

6.  Nachdem der Assistent gestartet wurde, wÃ¤hlen Sie **Files** und
    wÃ¤hlen Sie die SchaltflÃ¤che **"... "** auf der **Bronze** Datei.

![A screenshot of a computer Description automatically
generated](./media/image96.png)

7.  WÃ¤hlen Sie **load to tables** und dann **new table** aus.

![](./media/image97.png)

8.  Geben Sie im Popup-Fenster den Namen fÃ¼r Ihre Tabelle **bronze_01**
    ein und wÃ¤hlen Sie den Dateityp als **Parquet** aus.

![A screenshot of a computer Description automatically
generated](./media/image98.png)

![A screenshot of a computer Description automatically
generated](./media/image99.png)

![A screenshot of a computer Description automatically
generated](./media/image100.png)

![A screenshot of a computer Description automatically
generated](./media/image101.png)

9.  Die Datei **bronze_01** ist nun in den Dateien.

![A screenshot of a computer Description automatically
generated](./media/image102.png)

10. WÃ¤hlen Sie anschlieÃŸend die **Option "... "** in der Bronze Datei.
    WÃ¤hlen Sie **load to tables** und dann **existing table** aus.

![A screenshot of a computer Description automatically
generated](./media/image103.png)

11. Geben Sie den vorhandenen Tabellennamen als **dimcustomer_gold.**
    WÃ¤hlen Sie den Dateityp als **Parquet** aus, und wÃ¤hlen Sie
    **load.**

![A screenshot of a computer Description automatically
generated](./media/image104.png)

![A screenshot of a computer Description automatically
generated](./media/image105.png)

## **Aufgabe 3: Erstellen eines semantischen Modells Verwenden der Goldschicht zum Erstellen eines Berichts**

In Ihrem Workspace kÃ¶nnen Sie jetzt die Goldebene verwenden, um einen
Bericht zu erstellen und die Daten zu analysieren. Sie kÃ¶nnen direkt in
Ihrem Workspace auf das semantische Modell zugreifen, um Beziehungen und
Measures fÃ¼r die Berichterstellung zu erstellen.

*Beachten Sie, dass Sie das **standardmÃ¤ÃŸige semantische Modell**, das
automatisch erstellt wird, wenn Sie ein Lakehouse erstellen, nicht
verwenden kÃ¶nnen. Sie mÃ¼ssen ein neues semantisches Modell erstellen,
das die Goldtabellen enthÃ¤lt, die Sie in diesem Lab aus dem
Lakehouse-Explorer erstellt haben.*

1.  Navigieren Sie in Ihrem Workspace zu Ihrem **WWIlakehouse**
    Lakehouse. WÃ¤hlen Sie dann **New semantic Model** aus dem MenÃ¼band
    der Lakehouse-Explorer-Ansicht aus.

![A screenshot of a computer Description automatically
generated](./media/image106.png)

2.  Weisen Sie im Popup Ihrem neuen semantischen Modell den Namen
    **DatabricksTutorial** zu, und wÃ¤hlen Sie den Workspace als **Fabric
    Lakehouse Tutorial-29** aus.

![](./media/image107.png)

3.  Scrollen Sie anschlieÃŸend nach unten, und wÃ¤hlen Sie Alle aus, die
    in Ihr semantisches Modell aufgenommen werden sollen, und wÃ¤hlen Sie
    **Confirm**.

Dadurch wird das semantische Modell in Fabric geÃ¶ffnet, in dem Sie
Beziehungen und Measures erstellen kÃ¶nnen, wie hier gezeigt:

![A screenshot of a computer Description automatically
generated](./media/image108.png)

Von hier aus kÃ¶nnen Sie oder andere Mitglieder Ihres Datenteams Berichte
und Dashboards basierend auf den Daten in Ihrem Lakehouse erstellen.
Diese Berichte werden direkt mit der Goldschicht Ihres Lakehouse
verbunden, sodass sie immer die neuesten Daten widerspiegeln.

# Ãœbung 5: Erfassen und Analysieren von Daten mit Azure Databricks

1.  Navigieren Sie im Power BI-Dienst zu Ihrem Lakehouse, und wÃ¤hlen Sie
    **\>Get Data** und dann **New Data Pipeline** aus.

![Screenshot showing how to navigate to new data pipeline option from
within the UI.](./media/image109.png)

1.  Geben Sie in der Eingabeaufforderung **New Pipeline** einen Namen
    fÃ¼r die neue Pipeline ein, und wÃ¤hlen Sie dann **Create**.
    **IngestDatapipeline01**

![](./media/image110.png)

2.  WÃ¤hlen Sie fÃ¼r diese Ãœbung die Beispieldaten **NYC Taxi - Green**
    als Datenquelle aus.

![A screenshot of a computer Description automatically
generated](./media/image111.png)

3.  WÃ¤hlen Sie auf dem Vorschaubildschirm **Next**.

![A screenshot of a computer Description automatically
generated](./media/image112.png)

4.  WÃ¤hlen Sie als Datenziel den Namen der Tabelle aus, die Sie zum
    Speichern der OneLake Delta-Tabellendaten verwenden mÃ¶chten. Sie
    kÃ¶nnen eine vorhandene Tabelle auswÃ¤hlen oder eine neue erstellen.
    WÃ¤hlen Sie fÃ¼r diese Ãœbung **load into new table** und dann **Next**
    aus.

![A screenshot of a computer Description automatically
generated](./media/image113.png)

5.  WÃ¤hlen Sie auf dem Bildschirm **Review + Save**Â die Option **Start
    data transfer immediately**Â und wÃ¤hlen Sie dann **Save + Run**.

![Screenshot showing how to enter table name.](./media/image114.png)

![A screenshot of a computer Description automatically
generated](./media/image115.png)

6.  Wenn der Auftrag abgeschlossen ist, navigieren Sie zu Ihrem
    Lakehouse, und zeigen Sie die Delta-Tabelle an, die unter /Tables
    aufgefÃ¼hrt ist.

![A screenshot of a computer Description automatically
generated](./media/image116.png)

7.  Kopieren Sie den ABFS-Pfad (Azure Blob Filesystem) in Ihre
    Deltatabelle, indem Sie in der Explorer-Ansicht mit der rechten
    Maustaste auf den Tabellennamen klicken und dann **Properties**.

![A screenshot of a computer Description automatically
generated](./media/image117.png)

8.  Ã–ffnen Sie Ihr Azure Databricks Notebook, und fÃ¼hren Sie den Code
    aus.

olsPath = "**abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample**"

df=spark.read.format('delta').option("inferSchema","true").load(olsPath)

df.show(5)

*Hinweis: Ersetzen Sie den fett formatierten Dateipfad durch den
kopierten Pfad.*

![](./media/image118.png)

9.  Aktualisieren Sie die Daten der Delta-Tabelle, indem Sie einen Field
    view Ã¤ndern.

%sql

update delta.\`abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample\` set vendorID = 99999 where vendorID
= 1;

*Hinweis: Ersetzen Sie den fett formatierten Dateipfad durch den
kopierten Pfad.*

![A screenshot of a computer Description automatically
generated](./media/image119.png)

# Ãœbung 6: Bereinigen von Ressourcen

In dieser Ãœbung haben Sie gelernt, wie Sie eine Medaillon-Architektur in
einem Microsoft Fabric-Lakehouse erstellen.

Wenn Sie mit der Erkundung Ihres Seehauses fertig sind, kÃ¶nnen Sie den
Arbeitsbereich lÃ¶schen, den Sie fÃ¼r diese Ãœbung erstellt haben.

1.  WÃ¤hlen Sie Ihren Workspace, das **Fabric Lakehouse Tutorial-29,**
    aus dem linken NavigationsmenÃ¼ aus. Es Ã¶ffnet sich die Ansicht der
    Workspace Elemente.

![A screenshot of a computer Description automatically
generated](./media/image120.png)

2.  WÃ¤hlen Sie die Option ***...*** unter dem Namen des Workspace und
    wÃ¤hlen Sie **Workspace settings**.

![A screenshot of a computer Description automatically
generated](./media/image121.png)

3.  Scrollen Sie nach unten und klicken Sie **Remove this workspace.**

![A screenshot of a computer Description automatically
generated](./media/image122.png)

4.  Klicken Sie in der Warnung, die sich Ã¶ffnet**,** auf **Delete**.

![A white background with black text Description automatically
generated](./media/image123.png)

5.  Warten Sie auf eine Benachrichtigung, dass der Workspace gelÃ¶scht
    wurde, bevor Sie mit dem nÃ¤chsten Lab fortfahren.

![A screenshot of a computer Description automatically
generated](./media/image124.png)

**Zusammenfassung**:

Dieses Lab fÃ¼hrt die Teilnehmer durch das Erstellen einer
Medaillon-Architektur in einem Microsoft Fabric Lakehouse mithilfe von
Notebooks. Zu den wichtigsten Schritten gehÃ¶ren das Einrichten eines
Workspaces, das Einrichten eines Lakehouse, das Hochladen von Daten auf
die Bronze-Schicht fÃ¼r die erste Aufnahme, die Umwandlung in eine
silberne Delta-Tabelle fÃ¼r die strukturierte Verarbeitung, die weitere
Verfeinerung in goldene Delta-Tabellen fÃ¼r erweiterte Analysen, das
Untersuchen semantischer Modelle und das Erstellen von Datenbeziehungen
fÃ¼r eine aufschlussreiche Analyse.

## 
