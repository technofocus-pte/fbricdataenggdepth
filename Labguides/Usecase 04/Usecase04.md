# Caso d'uso 04: Analisi moderna su scala cloud con Azure Databricks e Microsoft Fabric

**Introduzione**

In questo lab si esplorerÃ  l'integrazione di Azure Databricks con
Microsoft Fabric per creare e gestire un lakehouse usando l'architettura
Medallion, creare una tabella Delta con l'aiuto dell'account Azure Data
Lake Storage (ADLS) Gen2 usando Azure Databricks e inserire dati con
Azure Databricks. Questa guida pratica ti guiderÃ  attraverso i passaggi
necessari per creare una lakehouse, caricare i dati al suo interno ed
esplorare i livelli di dati strutturati per facilitare l'analisi e la
reportistica efficienti dei dati.

L'architettura Medallion Ã¨ composta da tre strati distinti (o zone).

- Bronzo: noto anche come zona grezza, questo primo livello memorizza i
  dati di origine nel loro formato originale. I dati in questo livello
  sono in genere di sola accodamento e non modificabili.

- Argento: noto anche come zona arricchita, questo livello memorizza i
  dati provenienti dallo strato di bronzo. I dati grezzi sono stati
  puliti e standardizzati e ora sono strutturati come tabelle (righe e
  colonne). Potrebbe anche essere integrato con altri dati per fornire
  una visione aziendale di tutte le entitÃ  aziendali, come cliente,
  prodotto e altri.

- Oro: noto anche come zona curata, questo livello finale memorizza i
  dati provenienti dal livello argento. I dati vengono perfezionati per
  soddisfare specifici requisiti aziendali e analitici a valle. Le
  tabelle sono in genere conformi alla progettazione dello schema a
  stella, che supporta lo sviluppo di modelli di dati ottimizzati per le
  prestazioni e l'usabilitÃ .

**Obiettivi**:

- Comprendere i principi dell'architettura Medallion all'interno di
  Microsoft Fabric Lakehouse.

- Implementare un processo di gestione dei dati strutturati utilizzando
  i livelli Medallion (Bronzo, Argento, Oro).

- Trasformare i dati grezzi in dati convalidati e arricchiti per analisi
  e reportistica avanzate.

- Scoprire le best practice per la sicurezza dei dati, CI/CD e
  l'esecuzione efficiente delle query sui dati.

- Caricare i dati su OneLake con l'esploratore di file OneLake.

- Usare un notebook Fabric per leggere i dati in OneLake e riscriverli
  come tabella Delta.

- Analizzare e trasformare i dati con Spark usando un notebook Fabric.

- Interrogare una copia dei dati su OneLake con SQL.

- Creare una tabella Delta nell'account Azure Data Lake Storage (ADLS)
  Gen2 usando Azure Databricks.

- Creare un collegamento OneLake a una tabella Delta in ADLS.

- Usare Power BI per analizzare i dati tramite il collegamento ADLS.

- Leggere e modificare una tabella Delta in OneLake con Azure
  Databricks.

# Esercizio 1: Trasferimento dei dati di esempio in Lakehouse

In questo esercizio, verrÃ  illustrato il processo di creazione di un
lakehouse e il caricamento dei dati al suo interno utilizzando Microsoft
Fabric.

Compito:sentiero

## **AttivitÃ  1: Creare un'area di lavoro Fabric**

In questa attivitÃ  viene creata un'area di lavoro Fabric. L'area di
lavoro contiene tutti gli elementi necessari per questa esercitazione
lakehouse, che include lakehouse, flussi di dati, pipeline di Data
Factory, notebook, set di dati di Power BI e report.

1.  Aprire il browser, andare alla barra degli indirizzi e digitar o
    incollare il seguente URL:
    [https://app.fabric.microsoft.com/](https://app.fabric.microsoft.com/,)
    quindi premi il pulsante **Enter**.

> ![Una finestra del motore di ricerca con una casella rossa Descrizione
> generata automaticamente con confidenza media](./media/image1.png)

2.  Tornare alla finestra di **Power BI**. Nel menu di spostamento a
    sinistra della home page di Power BI spostarsi e fare clic su
    **Workspaces**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image2.png)

3.  Nel riquadro Aree di lavoro, fare clic sul pulsante **+**â€¯**New
    workspace.**

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image3.png)

4.  Nel riquadro **Create a workspace,** visualizzato sul lato destro,
    immettere i dettagli seguenti e fare clic sul pulsante **Apply**.

[TABLE]

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image4.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image5.png)

5.  Attendere il completamento della distribuzione. Ci vogliono 2-3
    minuti per completarlo.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image6.png)

## **Compito 2: Creare una lakehouse**

1.  Nella pagina **Power BI Fabric Lakehouse Tutorial-XX** fare clic
    sull'icona **Power BI** situata in basso a sinistra e selezionare
    **Data Engineering**.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image7.png)

2.  Nella home page di **Synapseâ€¯Data Engineering,** selezionare
    **Lakehouse** per creare una lakehouse.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image8.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image9.png)

3.  Nella finestra di dialogo **New lakehouse**â€¯, inserire
    **wwilakehouse** nel campo **Name**, fare clic sul pulsante
    **Create** e aprire la nuova lakehouse.

> **NOTA**: Assicurati di rimuovere lo spazio prima di **wwilakehouse**.
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image10.png)
>
> ![Uno screenshot di un computer Descrizione generata
> automaticamente](./media/image11.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image12.png)

4.  VerrÃ  visualizzata una notifica che indica **Successfully created
    SQL endpoint**.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image13.png)

# Esercizio 2: Implementazione dell'architettura Medallion con Azure Databricks

## **AttivitÃ  1: Impostazione del livello di bronzo**

1.  Nella pagina **wwilakehouse**, selezionare l'icona More accanto ai
    file (...) e selezionar **New subfolder**

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image14.png)

2.  Nella finestra a comparsa, specificare il nome della cartella come
    **bronze** e selezionare **Create**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image15.png)

3.  A questo punto, selezionare l'icona More accanto ai file di bronzo
    (...) e selezionare **Upload**, quindi **upload files**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image16.png)

4.  Nel riquadro di **upload file,** selezionare il pulsante di opzione
    **Upload file**. Fare clic sul pulsante **Browse** e accedere a
    **C:\LabFiles**, quindi selezionare il file dei file di dati di
    vendita richiesto (2019, 2020, 2021) e fare clic sul pulsante
    **Open**.

Quindi, selezionare **Upload** per caricare i file nella nuova cartella
"bronze" nella tua Lakehouse.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image17.png)

> ![Uno screenshot di un computer Descrizione generata
> automaticamente](./media/image18.png)

5.  Fare clic sulla cartella **bronze** per verificare che i file siano
    stati caricati correttamente e che i file riflettano.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image19.png)

# Esercizio 3: Trasformazione dei dati con Apache Spark ed esecuzione di query con SQL nell'architettura medallion 

## **AttivitÃ  1: Trasformare i dati e caricarli in una tabella Delta d'argento**

Nella pagina **wwilakehouse**, navigare e fare clic su **Open notebook**
nella barra dei comandi, quindi seleziona **New notebook**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image20.png)

1.  Selezionare la prima cella (che attualmente Ã¨ una cella di
    *codice*), quindi nella barra degli strumenti dinamica in alto a
    destra, usa il pulsante **Mâ†“** per **convertire la cella in una
    cella markdown**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image21.png)

2.  Quando la cella si trasforma in una cella markdown, viene eseguito
    il rendering del testo in essa contenuto.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image22.png)

3.  Utilizzare il **ðŸ–‰** pulsante (Modifica) per passare la cella alla
    modalitÃ  di modifica, sostituire tutto il testo, quindi modificare
    il markdown come segue:

CodeCopyÂ 

\# Sales order data exploration

Utilizzare il codice in questo blocco appunti per esplorare i dati degli
ordini di vendita.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image23.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image24.png)

4.  Fare clic in un punto qualsiasi del notebook all'esterno della cella
    per interrompere la modifica e visualizzare il markdown di cui Ã¨
    stato eseguito il rendering.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image25.png)

5.  Utilizzare l'icona **+ Code** sotto l'output della cella per
    aggiungere una nuova cella di codice al notebook.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image26.png)

6.  A questo punto, usare il notebook per caricare i dati dal livello di
    bronzo in un DataFrame Spark.

Selezionare la cella esistente nel notebook, che contiene un semplice
codice commentato. Evidenzia ed elimina queste due righe: non avrai
bisogno di questo codice.

*Nota: i notebook consentono di eseguire codice in una varietÃ  di
linguaggi, tra cui Python, Scala e SQL. In questo esercizio si useranno
PySpark e SQL. Puoi anche aggiungere celle markdown per fornire testo e
immagini formattati per documentare il tuo codice.*

Per questo, inserire il seguente codice e fare clic su **Run**.

CodeCopyÂ 

from pyspark.sql.types import \*Â 

Â 

\# Create the schema for the tableÂ 

orderSchema = StructType(\[Â 

Â Â Â  StructField("SalesOrderNumber", StringType()),Â 

Â Â Â  StructField("SalesOrderLineNumber", IntegerType()),Â 

Â Â Â  StructField("OrderDate", DateType()),Â 

Â Â Â  StructField("CustomerName", StringType()),Â 

Â Â Â  StructField("Email", StringType()),Â 

Â Â Â  StructField("Item", StringType()),Â 

Â Â Â  StructField("Quantity", IntegerType()),Â 

Â Â Â  StructField("UnitPrice", FloatType()),Â 

Â Â Â  StructField("Tax", FloatType())Â 

\])Â 

Â 

\# Import all files from bronze folder of lakehouseÂ 

df = spark.read.format("csv").option("header",
"true").schema(orderSchema).load("Files/bronze/\*.csv")Â 

Â 

\# Display the first 10 rows of the dataframe to preview your dataÂ 

display(df.head(10))

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image27.png)

***Nota**: poichÃ© questa Ã¨ la prima volta che esegui un codice Spark in
questo notebook, Ã¨ necessario avviare una sessione Spark. CiÃ² significa
che il completamento della prima esecuzione puÃ² richiedere circa un
minuto. Le corse successive saranno piÃ¹ veloci.*

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image28.png)

7.  Il codice eseguito ha caricato i dati dai file CSV nella cartella
    **bronze** in un frame di dati Spark e quindi ha visualizzato le
    prime righe del frame di dati.

> **Nota**: Ã¨ possibile cancellare, nascondere e ridimensionare
> automaticamente il contenuto dell'output della cella selezionando
> l'icona **...** in alto a sinistra nel riquadro di output.

8.  A questo punto si **aggiungeranno** **colonne per la convalida e la
    pulizia dei dati**, usando un frame di dati PySpark per aggiungere
    colonne e aggiornare i valori di alcune delle colonne esistenti.
    Utilizzare il pulsante + per **aggiungere un nuovo blocco di
    codice** e aggiungere il seguente codice alla cella:

> CodeCopy
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_nameÂ 
>
> Â Â Â Â Â 
>
> Â # Add columns IsFlagged, CreatedTS and ModifiedTSÂ 
>
> Â df = df.withColumn("FileName", input_file_name()) \\
>
> Â Â Â Â  .withColumn("IsFlagged", when(col("OrderDate") \<
> '2019-08-01',True).otherwise(False)) \\
>
> Â Â Â Â  .withColumn("CreatedTS",
> current_timestamp()).withColumn("ModifiedTS", current_timestamp())Â 
>
> Â Â Â Â Â 
>
> Â # Update CustomerName to "Unknown" if CustomerName null or emptyÂ 
>
> Â df = df.withColumn("CustomerName", when((col("CustomerName").isNull()
> |
> (col("CustomerName")=="")),lit("Unknown")).otherwise(col("CustomerName")))
>
> La prima riga del codice importa le funzioni necessarie da PySpark. Si
> aggiungono quindi nuove colonne al frame di dati in modo da poter
> tenere traccia del nome del file di origine, se l'ordine Ã¨ stato
> contrassegnato come precedente all'anno fiscale di interesse e quando
> la riga Ã¨ stata creata e modificata.
>
> Infine, si aggiorna la colonna CustomerName a "Unknown" se Ã¨ null o
> vuota.
>
> Quindi, esegui la cella per eseguire il codice utilizzando il pulsante
> **\*\*â–·** (*Run cell*)\*\*.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image29.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image30.png)

9.  Successivamente, si definirÃ  lo schema per la tabella
    **sales_silver** nel database delle vendite utilizzando il formato
    Delta Lake. Creare un nuovo blocco di codice e aggiungere il
    seguente codice alla cella:

> CodeCopy

from pyspark.sql.types import \*Â 

from delta.tables import \*Â 

Â 

\# Define the schema for the sales_silver tableÂ 

silver_table_schema = StructType(\[Â 

â€¯ â€¯ StructField("SalesOrderNumber", StringType(), True),Â 

â€¯ â€¯ StructField("SalesOrderLineNumber", IntegerType(), True),Â 

â€¯ â€¯ StructField("OrderDate", DateType(), True),Â 

â€¯ â€¯ StructField("CustomerName", StringType(), True),Â 

â€¯ â€¯ StructField("Email", StringType(), True),Â 

â€¯ â€¯ StructField("Item", StringType(), True),Â 

â€¯ â€¯ StructField("Quantity", IntegerType(), True),Â 

â€¯ â€¯ StructField("UnitPrice", FloatType(), True),Â 

â€¯ â€¯ StructField("Tax", FloatType(), True),Â 

â€¯ â€¯ StructField("FileName", StringType(), True),Â 

â€¯ â€¯ StructField("IsFlagged", BooleanType(), True),Â 

â€¯ â€¯ StructField("CreatedTS", TimestampType(), True),Â 

â€¯ â€¯ StructField("ModifiedTS", TimestampType(), True)Â 

\])Â 

Â 

\# Create or replace the sales_silver table with the defined schemaÂ 

DeltaTable.createIfNotExists(spark) \\

â€¯ â€¯ .tableName("wwilakehouse.sales_silver") \\

â€¯ â€¯ .addColumns(silver_table_schema) \\

â€¯ â€¯ .execute()Â 

Â  Â 

10. Eseguire la cella per eseguire il codice utilizzando il pulsante
    **\*\*â–·** (*Run cell*)\*\*.

11. Seleziona l'icona **...** nella sezione Tables del riquadro
    Lakehouse Explorer e selezionare **Refresh**. A questo punto
    dovrebbe essere elencata la nuova tabella **sales_silver**. Il **â–²**
    (icona a forma di triangolo) indica che si tratta di una tabella
    Delta.

> **Nota**: se la nuova tabella non viene visualizzata, attendere
> qualche secondo, quindi selezionare nuovamente **Refreshâ€¯** oppure
> aggiornare l'intera scheda del browser.
>
> ![Uno screenshot di un computer Descrizione generata
> automaticamente](./media/image31.png)
>
> ![Uno screenshot di un computer Descrizione generata
> automaticamente](./media/image32.png)

12. A questo punto si eseguirÃ  un'**operazione di upsert** su una
    tabella Delta, aggiornando i record esistenti in base a condizioni
    specifiche e inserendo nuovi record quando non viene trovata alcuna
    corrispondenza. Aggiungere un nuovo blocco di codice e incollare il
    codice seguente:

> CodeCopy
>
> from pyspark.sql.types import \*Â 
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_nameÂ 
>
> from delta.tables import \*Â 
>
> Â 
>
> \# Define the schema for the source dataÂ 
>
> orderSchema = StructType(\[Â 
>
> Â Â Â  StructField("SalesOrderNumber", StringType(), True),Â 
>
> Â Â Â  StructField("SalesOrderLineNumber", IntegerType(), True),Â 
>
> Â Â Â  StructField("OrderDate", DateType(), True),Â 
>
> Â Â Â  StructField("CustomerName", StringType(), True),Â 
>
> Â Â Â  StructField("Email", StringType(), True),Â 
>
> Â Â Â  StructField("Item", StringType(), True),Â 
>
> Â Â Â  StructField("Quantity", IntegerType(), True),Â 
>
> Â Â Â  StructField("UnitPrice", FloatType(), True),Â 
>
> Â Â Â  StructField("Tax", FloatType(), True)Â 
>
> \])Â 
>
> Â 
>
> \# Read data from the bronze folder into a DataFrameÂ 
>
> df = spark.read.format("csv").option("header",
> "true").schema(orderSchema).load("Files/bronze/\*.csv")Â 
>
> Â 
>
> \# Add additional columnsÂ 
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> Â Â Â  .withColumn("IsFlagged", when(col("OrderDate") \< '2019-08-01',
> True).otherwise(False)) \\
>
> Â Â Â  .withColumn("CreatedTS", current_timestamp()) \\
>
> Â Â Â  .withColumn("ModifiedTS", current_timestamp()) \\
>
> Â Â Â  .withColumn("CustomerName", when((col("CustomerName").isNull()) |
> (col("CustomerName") == ""),
> lit("Unknown")).otherwise(col("CustomerName")))Â 
>
> Â 
>
> \# Define the path to the Delta tableÂ 
>
> deltaTablePath = "Tables/sales_silver"Â 
>
> Â 
>
> \# Create a DeltaTable object for the existing Delta tableÂ 
>
> deltaTable = DeltaTable.forPath(spark, deltaTablePath)Â 
>
> Â 
>
> \# Perform the merge (upsert) operationÂ 
>
> deltaTable.alias('silver') \\
>
> Â Â Â  .merge(Â 
>
> Â Â Â Â Â Â Â  df.alias('updates'),Â 
>
> Â Â Â Â Â Â Â  'silver.SalesOrderNumber = updates.SalesOrderNumber AND \\
>
> Â Â Â Â Â Â Â Â  silver.OrderDate = updates.OrderDate AND \\
>
> Â Â Â Â Â Â Â Â  silver.CustomerName = updates.CustomerName AND \\
>
> Â Â Â Â Â Â Â Â  silver.Item = updates.Item'Â 
>
> Â Â Â  ) \\
>
> Â Â Â  .whenMatchedUpdate(set = {Â 
>
> Â Â Â Â Â Â Â  "SalesOrderLineNumber": "updates.SalesOrderLineNumber",Â 
>
> Â Â Â Â Â Â Â  "Email": "updates.Email",Â 
>
> Â Â Â Â Â Â Â  "Quantity": "updates.Quantity",Â 
>
> Â Â Â Â Â Â Â  "UnitPrice": "updates.UnitPrice",Â 
>
> Â Â Â Â Â Â Â  "Tax": "updates.Tax",Â 
>
> Â Â Â Â Â Â Â  "FileName": "updates.FileName",Â 
>
> Â Â Â Â Â Â Â  "IsFlagged": "updates.IsFlagged",Â 
>
> Â Â Â Â Â Â Â  "ModifiedTS": "current_timestamp()"Â 
>
> Â Â Â  }) \\
>
> Â Â Â  .whenNotMatchedInsert(values = {Â 
>
> Â Â Â Â Â Â Â  "SalesOrderNumber": "updates.SalesOrderNumber",Â 
>
> Â Â Â Â Â Â Â  "SalesOrderLineNumber": "updates.SalesOrderLineNumber",Â 
>
> Â Â Â Â Â Â Â  "OrderDate": "updates.OrderDate",Â 
>
> Â Â Â Â Â Â Â  "CustomerName": "updates.CustomerName",Â 
>
> Â Â Â Â Â Â Â  "Email": "updates.Email",Â 
>
> Â Â Â Â Â Â Â  "Item": "updates.Item",Â 
>
> Â Â Â Â Â Â Â  "Quantity": "updates.Quantity",Â 
>
> Â Â Â Â Â Â Â  "UnitPrice": "updates.UnitPrice",Â 
>
> Â Â Â Â Â Â Â  "Tax": "updates.Tax",Â 
>
> Â Â Â Â Â Â Â  "FileName": "updates.FileName",Â 
>
> Â Â Â Â Â Â Â  "IsFlagged": "updates.IsFlagged",Â 
>
> Â Â Â Â Â Â Â  "CreatedTS": "current_timestamp()",Â 
>
> Â Â Â Â Â Â Â  "ModifiedTS": "current_timestamp()"Â 
>
> Â Â Â  }) \\
>
> Â Â Â  .execute()Â 

13. Eseguire la cella per eseguire il codice utilizzando il pulsante
    **\*\*â–·** (*Eseguire cella*)\*\*.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image33.png)

Questa operazione Ã¨ importante perchÃ© consente di aggiornare i record
esistenti nella tabella in base ai valori di colonne specifiche e di
inserire nuovi record quando non viene trovata alcuna corrispondenza. Si
tratta di un requisito comune quando si caricano dati da un sistema di
origine che puÃ² contenere aggiornamenti a record esistenti e nuovi.

A questo punto sono presenti dati nella tabella delta d'argento pronti
per ulteriori trasformazioni e modellazioni.

I dati sono stati prelevati dal livello di bronzo, li sono stati
trasformati e caricati in una tabella Delta in argento. A questo punto
si userÃ  un nuovo notebook per trasformare ulteriormente i dati,
modellarli in uno schema a stella e caricarli in tabelle Delta dorate.

*Si noti che tutte queste operazioni potrebbero essere state eseguite in
un unico blocco appunti, ma ai fini di questo esercizio si utilizzano
blocchi appunti separati per illustrare il processo di trasformazione
dei dati da bronzo ad argento e quindi da argento a oro. Questo puÃ²
aiutare con il debug, la risoluzione dei problemi e il riutilizzo*.

## **AttivitÃ  2: Caricare i dati nelle tabelle Gold Delta**

1.  Tornare alla home page di Fabric Lakehouse Tutorial-29.

> ![Uno screenshot di un computer Descrizione generata
> automaticamente](./media/image34.png)

2.  Selezionare **wwilakehouse.**

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image35.png)

3.  Nel riquadro di esplorazione lakehouse, dovrebbe essere visualizzata
    la tabella **sales_silver** elencata nella sezione **Tablesâ€¯**del
    riquadro di esplorazione.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image36.png)

4.  A questo punto, creare un nuovo notebook denominato **Transform data
    for Gold**. Per questo, navigare e fare clic su **Open notebook**
    nella barra dei comandi, quindi selezionare **New notebook**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image37.png)

5.  Nel blocco di codice esistente, rimuovi il testo standard e
    **aggiungi il seguente codice** per caricare i dati nel tuo
    dataframe e iniziare a creare il tuo schema a stella, quindi
    eseguilo:

> CodeCopy

\# Load data to the dataframe as a starting point to create the gold
layerÂ 

df = spark.read.table("wwilakehouse.sales_silver")Â 

Â 

\# Display the first few rows of the dataframe to verify the dataÂ 

df.show()Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image38.png)

6.  Successivamente**, Add a new code block** e incollare il seguente
    codice per creare la tabella della dimensione della data ed
    eseguirla:

from pyspark.sql.types import \*Â 

â€¯from delta.tables import\*Â 

â€¯ â€¯Â Â 

â€¯# Define the schema for the dimdate_gold tableÂ 

â€¯DeltaTable.createIfNotExists(spark) \\

â€¯ â€¯ â€¯.tableName("wwilakehouse.dimdate_gold") \\

â€¯ â€¯ â€¯.addColumn("OrderDate", DateType()) \\

â€¯ â€¯ â€¯.addColumn("Day", IntegerType()) \\

â€¯ â€¯ â€¯.addColumn("Month", IntegerType()) \\

â€¯ â€¯ â€¯.addColumn("Year", IntegerType()) \\

â€¯ â€¯ â€¯.addColumn("mmmyyyy", StringType()) \\

â€¯ â€¯ â€¯.addColumn("yyyymm", StringType()) \\

â€¯ â€¯ â€¯.execute()

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image39.png)

**Nota**: Ãˆ possibile eseguire il commando display(df) in qualsiasi
momento per controllare l'avanzamento del lavoro. In questo caso,
eseguiresti 'display(dfdimDate_gold)' per vedere il contenuto del
dataframe dimDate_gold.

7.  In un nuovo blocco di codice, **add and run the following code** per
    creare un dataframe per la dimensione data, **dimdate_gold**:

> CodeCopy

Â from pyspark.sql.functions import col, dayofmonth, month, year,
date_formatÂ 

â€¯ â€¯Â Â 

â€¯# Create dataframe for dimDate_goldÂ 

â€¯ â€¯Â Â 

dfdimDate_gold
=df.dropDuplicates(\["OrderDate"\]).select(col("OrderDate"), \\

â€¯ â€¯ â€¯ â€¯ â€¯dayofmonth("OrderDate").alias("Day"), \\

â€¯ â€¯ â€¯ â€¯ â€¯month("OrderDate").alias("Month"), \\

â€¯ â€¯ â€¯ â€¯ â€¯year("OrderDate").alias("Year"), \\

â€¯ â€¯ â€¯ â€¯ â€¯date_format(col("OrderDate"), "MMM-yyyy").alias("mmmyyyy"), \\

â€¯ â€¯ â€¯ â€¯ â€¯date_format(col("OrderDate"), "yyyyMM").alias("yyyymm"), \\

â€¯ â€¯ â€¯).orderBy("OrderDate")Â 

Â 

â€¯# Display the first 10 rows of the dataframe to preview your dataÂ 

Â 

display(dfdimDate_gold.head(10))Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image40.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image41.png)

8.  Si separa il codice in nuovi blocchi di codice in modo da poter
    comprendere e osservare ciÃ² che accade nel notebook durante la
    trasformazione dei dati. In un altro nuovo blocco di codice, **â€¯add
    and run the following code** per aggiornare la dimensione della data
    man mano che arrivano nuovi dati:

> CodeCopy
>
> Â from delta.tables import \*Â 
>
> Â Â Â Â Â 
>
> Â deltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')Â 
>
> Â Â Â Â Â 
>
> Â dfUpdates = dfdimDate_goldÂ 
>
> Â Â Â Â Â 
>
> Â deltaTable.alias('silver') \\
>
> Â Â  .merge(Â 
>
> Â Â Â Â  dfUpdates.alias('updates'),Â 
>
> Â Â Â Â  'silver.OrderDate = updates.OrderDate'Â 
>
> Â Â  ) \\
>
> Â Â Â  .whenMatchedUpdate(set =Â 
>
> Â Â Â Â  {Â 
>
> Â Â Â Â Â Â Â Â Â Â Â 
>
> Â Â Â Â  }Â 
>
> Â Â  ) \\
>
> Â  .whenNotMatchedInsert(values =Â 
>
> Â Â Â Â  {Â 
>
> Â Â Â Â Â Â  "OrderDate": "updates.OrderDate",Â 
>
> Â Â Â Â Â Â  "Day": "updates.Day",Â 
>
> Â Â Â Â Â Â  "Month": "updates.Month",Â 
>
> Â Â Â Â Â Â  "Year": "updates.Year",Â 
>
> Â Â Â Â Â Â  "mmmyyyy": "updates.mmmyyyy",Â 
>
> Â Â Â Â Â Â  "yyyymm": "yyyymm"Â 
>
> Â Â Â Â  }Â 
>
> Â Â  ) \\
>
> Â Â  .execute()Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image42.png)

> La dimensione della data Ã¨ configurata.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image43.png)

## **AttivitÃ  3: Creare la dimensione cliente.**

1.  Per creare la tabella delle dimensioni del cliente, **add a new code
    block**, incollare ed eseguire il seguente codice:

> CodeCopy

â€¯from pyspark.sql.types import \*Â 

â€¯from delta.tables import \*Â 

â€¯ â€¯Â Â 

â€¯# Create customer_gold dimension delta tableÂ 

â€¯DeltaTable.createIfNotExists(spark) \\

â€¯ â€¯ â€¯.tableName("wwilakehouse.dimcustomer_gold") \\

â€¯ â€¯ â€¯.addColumn("CustomerName", StringType()) \\

â€¯ â€¯ â€¯.addColumn("Email", â€¯StringType()) \\

â€¯ â€¯ â€¯.addColumn("First", StringType()) \\

â€¯ â€¯ â€¯.addColumn("Last", StringType()) \\

â€¯ â€¯ â€¯.addColumn("CustomerID", LongType()) \\

â€¯ â€¯ â€¯.execute()Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image44.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image45.png)

2.  In un nuovo blocco di codice, **add and run the following code** per
    eliminare i clienti duplicati, selezionare colonne specifiche e
    dividere la colonna "CustomerName" per creare le colonne "Nome" e
    "Cognome":

> CodeCopy
>
> Â from pyspark.sql.functions import col, splitÂ 
>
> Â Â Â Â Â 
>
> Â # Create customer_silver dataframeÂ 
>
> Â Â Â Â Â 
>
> Â dfdimCustomer_silver =
> df.dropDuplicates(\["CustomerName","Email"\]).select(col("CustomerName"),col("Email"))
> \\
>
> Â Â Â Â  .withColumn("First",split(col("CustomerName"), " ").getItem(0))
> \\
>
> Â Â Â Â  .withColumn("Last",split(col("CustomerName"), " ").getItem(1))Â Â 
>
> Â Â Â Â Â 
>
> Â # Display the first 10 rows of the dataframe to preview your dataÂ 
>
> Â 
>
> Â display(dfdimCustomer_silver.head(10))Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image46.png)

Qui Ã¨ stato creato un nuovo dfdimCustomer_silver DataFrame eseguendo
varie trasformazioni, ad esempio l'eliminazione di duplicati, la
selezione di colonne specifiche e la suddivisione della colonna
"CustomerName" per creare colonne "Nome" e "Cognome". Il risultato Ã¨ un
DataFrame con dati dei clienti puliti e strutturati, incluse le colonne
separate "Nome" e "Cognome" estratte dalla colonna "CustomerName".

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image47.png)

3.  Successivamente, creeremo **la colonna ID per i nostri clienti**. In
    un nuovo blocco di codice, incollare ed eseguire quanto segue:

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col,
when, coalesce, max, litÂ 

Â 

\# Read the existing data from the Delta tableÂ 

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")Â 

Â 

\# Find the maximum CustomerID or use 0 if the table is emptyÂ 

MAXCustomerID =
dfdimCustomer_temp.select(coalesce(max(col("CustomerID")),
lit(0)).alias("MAXCustomerID")).first()\[0\]Â 

Â 

\# Assume dfdimCustomer_silver is your source DataFrame with new dataÂ 

\# Here, we select only the new customers by doing a left anti joinÂ 

dfdimCustomer_gold = dfdimCustomer_silver.join(Â 

â€¯ â€¯ dfdimCustomer_temp,Â 

â€¯ â€¯ (dfdimCustomer_silver.CustomerName ==
dfdimCustomer_temp.CustomerName) &Â Â 

â€¯ â€¯ (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email),Â 

â€¯ â€¯ "left_anti"Â 

)Â 

Â 

\# Add the CustomerID column with unique values starting from
MAXCustomerID + 1Â 

dfdimCustomer_gold = dfdimCustomer_gold.withColumn(Â 

â€¯ â€¯ "CustomerID",Â 

â€¯ â€¯ monotonically_increasing_id() + MAXCustomerID + 1Â 

)Â 

Â 

\# Display the first 10 rows of the dataframe to preview your dataÂ 

dfdimCustomer_gold.show(10)Â 

Â 

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image48.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image49.png)

4.  Ora ti assicurerai che la tua tabella dei clienti rimanga aggiornata
    man mano che arrivano nuovi dati. **In a new code block**, incollare
    ed eseguire quanto segue:

> CodeCopy

from delta.tables import DeltaTableÂ 

Â 

\# Define the Delta table pathÂ 

deltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')Â 

Â 

\# Use dfUpdates to refer to the DataFrame with new or updated recordsÂ 

dfUpdates = dfdimCustomer_goldÂ 

Â 

\# Perform the merge operation to update or insert new recordsÂ 

deltaTable.alias('silver') \\

â€¯ .merge(Â 

â€¯ â€¯ dfUpdates.alias('updates'),Â 

â€¯ â€¯ 'silver.CustomerName = updates.CustomerName AND silver.Email =
updates.Email'Â 

â€¯ ) \\

â€¯ .whenMatchedUpdate(set =Â 

â€¯ â€¯ {Â 

â€¯ â€¯ â€¯ "CustomerName": "updates.CustomerName",Â 

â€¯ â€¯ â€¯ "Email": "updates.Email",Â 

â€¯ â€¯ â€¯ "First": "updates.First",Â 

â€¯ â€¯ â€¯ "Last": "updates.Last",Â 

â€¯ â€¯ â€¯ "CustomerID": "updates.CustomerID"Â 

â€¯ â€¯ }Â 

â€¯ ) \\

â€¯ .whenNotMatchedInsert(values =Â 

â€¯ â€¯ {Â 

â€¯ â€¯ â€¯ "CustomerName": "updates.CustomerName",Â 

â€¯ â€¯ â€¯ "Email": "updates.Email",Â 

â€¯ â€¯ â€¯ "First": "updates.First",Â 

â€¯ â€¯ â€¯ "Last": "updates.Last",Â 

â€¯ â€¯ â€¯ "CustomerID": "updates.CustomerID"Â 

â€¯ â€¯ }Â 

â€¯ ) \\

â€¯ .execute()Â 

![](./media/image50.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image51.png)

5.  Ora ripeterai **questi passaggi per creare la dimensione del
    prodotto**. In un nuovo blocco di codice, incollare ed eseguire
    quanto segue:

> CodeCopy
>
> from pyspark.sql.types import \*Â 
>
> from delta.tables import \*Â 
>
> Â Â Â Â Â 
>
> DeltaTable.createIfNotExists(spark) \\
>
> Â Â Â  .tableName("wwilakehouse.dimproduct_gold") \\
>
> Â Â Â  .addColumn("ItemName", StringType()) \\
>
> Â Â Â  .addColumn("ItemID", LongType()) \\
>
> Â Â Â  .addColumn("ItemInfo", StringType()) \\
>
> Â Â Â  .execute()Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image52.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image53.png)

6.  **Aggiungere un altro blocco di codice** per creare il
    **product_silver** dataframe.

> CodeCopy
>
> from pyspark.sql.functions import col, split, litÂ 
>
> Â Â Â Â Â 
>
> \# Create product_silver dataframeÂ 
>
> Â Â Â Â Â 
>
> dfdimProduct_silver =
> df.dropDuplicates(\["Item"\]).select(col("Item")) \\
>
> Â Â Â  .withColumn("ItemName",split(col("Item"), ", ").getItem(0)) \\
>
> Â Â Â  .withColumn("ItemInfo",when((split(col("Item"), ",
> ").getItem(1).isNull() | (split(col("Item"), ",
> ").getItem(1)=="")),lit("")).otherwise(split(col("Item"), ",
> ").getItem(1)))Â Â 
>
> Â Â Â Â Â 
>
> \# Display the first 10 rows of the dataframe to preview your dataÂ 
>
> Â 
>
> display(dfdimProduct_silver.head(10))Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image54.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image55.png)

7.  A questo punto creerai gli ID per la tua **dimProduct_gold table**.
    Aggiungere la sintassi seguente a un nuovo blocco di codice ed
    eseguirlo:

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col, lit,
max, coalesceÂ 

Â Â Â Â Â 

\#dfdimProduct_temp = dfdimProduct_silverÂ 

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")Â 

Â Â Â Â Â 

MAXProductID =
dfdimProduct_temp.select(coalesce(max(col("ItemID")),lit(0)).alias("MAXItemID")).first()\[0\]Â 

Â Â Â Â Â 

dfdimProduct_gold =
dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName
== dfdimProduct_temp.ItemName) & (dfdimProduct_silver.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left_anti")Â 

Â Â Â Â Â 

dfdimProduct_gold =
dfdimProduct_gold.withColumn("ItemID",monotonically_increasing_id() +
MAXProductID + 1)Â 

Â Â Â Â Â 

\# Display the first 10 rows of the dataframe to preview your dataÂ 

Â 

display(dfdimProduct_gold.head(10))Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image56.png)

In questo modo viene calcolato il successivo ID prodotto disponibile in
base ai dati correnti nella tabella, vengono assegnati questi nuovi ID
ai prodotti e quindi vengono visualizzate le informazioni aggiornate sul
prodotto.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image57.png)

8.  Analogamente a quanto hai fatto con le altre dimensioni, devi
    assicurarti che la tabella dei prodotti rimanga aggiornata man mano
    che arrivano nuovi dati. **In un nuovo blocco di codice**, incollare
    ed eseguire quanto segue:

CodeCopy

from delta.tables import \*Â 

Â Â Â Â Â 

deltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')Â 

Â Â Â Â Â Â Â Â Â Â Â Â Â 

dfUpdates = dfdimProduct_goldÂ 

Â Â Â Â Â Â Â Â Â Â Â Â Â 

deltaTable.alias('silver') \\

Â  .merge(Â 

Â Â Â Â Â Â Â  dfUpdates.alias('updates'),Â 

Â Â Â Â Â Â Â  'silver.ItemName = updates.ItemName AND silver.ItemInfo =
updates.ItemInfo'Â 

Â Â Â Â Â Â Â  ) \\

Â Â Â Â Â Â Â  .whenMatchedUpdate(set =Â 

Â Â Â Â Â Â Â  {Â 

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 

Â Â Â Â Â Â Â  }Â 

Â Â Â Â Â Â Â  ) \\

Â Â Â Â Â Â Â  .whenNotMatchedInsert(values =Â 

Â Â Â Â Â Â Â Â  {Â 

Â Â Â Â Â Â Â Â Â  "ItemName": "updates.ItemName",Â 

Â Â Â Â Â Â Â Â Â  "ItemInfo": "updates.ItemInfo",Â 

Â Â Â Â Â Â Â Â Â  "ItemID": "updates.ItemID"Â 

Â Â Â Â Â Â Â Â Â  }Â 

Â Â Â Â Â Â Â Â Â  ) \\

Â Â Â Â Â Â Â Â Â  .execute()

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image58.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image59.png)

**Dopo aver creato le dimensioni, il passaggio finale consiste nel
creare la tabella dei fatti.**

9.  **In un nuovo blocco di codice** incollare ed eseguire il codice
    seguente per creare la â€¯**fact table**:

> CodeCopy
>
> from pyspark.sql.types import \*Â 
>
> from delta.tables import \*Â 
>
> Â Â Â Â Â 
>
> DeltaTable.createIfNotExists(spark) \\
>
> Â Â Â  .tableName("wwilakehouse.factsales_gold") \\
>
> Â Â Â  .addColumn("CustomerID", LongType()) \\
>
> Â Â Â  .addColumn("ItemID", LongType()) \\
>
> Â Â Â  .addColumn("OrderDate", DateType()) \\
>
> Â Â Â  .addColumn("Quantity", IntegerType()) \\
>
> Â Â Â  .addColumn("UnitPrice", FloatType()) \\
>
> Â Â Â  .addColumn("Tax", FloatType()) \\
>
> Â Â Â  .execute()Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image60.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image61.png)

10. **In un nuovo blocco di codice**, incollare ed eseguire il seguente
    codice per creare un **new dataframe** per combinare i dati di
    vendita con le informazioni sul cliente e sul prodotto, tra cui l'ID
    cliente, l'ID articolo, la data dell'ordine, la quantitÃ , il prezzo
    unitario e l'imposta:

CodeCopy

from pyspark.sql import SparkSessionÂ 

from pyspark.sql.functions import split, col, when, litÂ 

from pyspark.sql.types import StructType, StructField, StringType,
IntegerType, DateType, FloatType, BooleanType, TimestampTypeÂ 

Â 

\# Initialize Spark sessionÂ 

spark = SparkSession.builder \\

â€¯ â€¯ .appName("DeltaTableUpsert") \\

â€¯ â€¯ .config("spark.sql.extensions",
"io.delta.sql.DeltaSparkSessionExtension") \\

â€¯ â€¯ .config("spark.sql.catalog.spark_catalog",
"org.apache.spark.sql.delta.catalog.DeltaCatalog") \\

â€¯ â€¯ .getOrCreate()Â 

Â 

\# Define the schema for the sales_silver tableÂ 

silver_table_schema = StructType(\[Â 

â€¯ â€¯ StructField("SalesOrderNumber", StringType(), True),Â 

â€¯ â€¯ StructField("SalesOrderLineNumber", IntegerType(), True),Â 

â€¯ â€¯ StructField("OrderDate", DateType(), True),Â 

â€¯ â€¯ StructField("CustomerName", StringType(), True),Â 

â€¯ â€¯ StructField("Email", StringType(), True),Â 

â€¯ â€¯ StructField("Item", StringType(), True),Â 

â€¯ â€¯ StructField("Quantity", IntegerType(), True),Â 

â€¯ â€¯ StructField("UnitPrice", FloatType(), True),Â 

â€¯ â€¯ StructField("Tax", FloatType(), True),Â 

â€¯ â€¯ StructField("FileName", StringType(), True),Â 

â€¯ â€¯ StructField("IsFlagged", BooleanType(), True),Â 

â€¯ â€¯ StructField("CreatedTS", TimestampType(), True),Â 

â€¯ â€¯ StructField("ModifiedTS", TimestampType(), True)Â 

\])Â 

Â 

\# Define the path to the Delta table (ensure this path is correct)Â 

delta_table_path =
"abfss://\<container\>@\<storage-account\>.dfs.core.windows.net/path/to/wwilakehouse/sales_silver"Â 

Â 

\# Create a DataFrame with the defined schemaÂ 

empty_df = spark.createDataFrame(\[\], silver_table_schema)Â 

Â 

\# Register the Delta table in the MetastoreÂ 

spark.sql(f"""Â 

â€¯ â€¯ CREATE TABLE IF NOT EXISTS wwilakehouse.sales_silverÂ 

â€¯ â€¯ USING DELTAÂ 

â€¯ â€¯ LOCATION '{delta_table_path}'Â 

""")Â 

Â 

\# Load data into DataFrameÂ 

df = spark.read.table("wwilakehouse.sales_silver")Â 

Â 

\# Perform transformations on dfÂ 

df = df.withColumn("ItemName", split(col("Item"), ", ").getItem(0)) \\

â€¯ â€¯ .withColumn("ItemInfo", when(Â 

â€¯ â€¯ â€¯ â€¯ (split(col("Item"), ", ").getItem(1).isNull()) |
(split(col("Item"), ", ").getItem(1) == ""),Â 

â€¯ â€¯ â€¯ â€¯ lit("")Â 

â€¯ â€¯ ).otherwise(split(col("Item"), ", ").getItem(1)))Â 

Â 

\# Load additional DataFrames for joinsÂ 

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")Â 

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")Â 

Â 

\# Create Sales_gold dataframeÂ 

dffactSales_gold = df.alias("df1").join(dfdimCustomer_temp.alias("df2"),
(df.CustomerName == dfdimCustomer_temp.CustomerName) & (df.Email ==
dfdimCustomer_temp.Email), "left") \\

â€¯ â€¯ .join(dfdimProduct_temp.alias("df3"), (df.ItemName ==
dfdimProduct_temp.ItemName) & (df.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left") \\

â€¯ â€¯ .select(Â 

â€¯ â€¯ â€¯ â€¯ col("df2.CustomerID"),Â 

â€¯ â€¯ â€¯ â€¯ col("df3.ItemID"),Â 

â€¯ â€¯ â€¯ â€¯ col("df1.OrderDate"),Â 

â€¯ â€¯ â€¯ â€¯ col("df1.Quantity"),Â 

â€¯ â€¯ â€¯ â€¯ col("df1.UnitPrice"),Â 

â€¯ â€¯ â€¯ â€¯ col("df1.Tax")Â 

â€¯ â€¯ ).orderBy(col("df1.OrderDate"), col("df2.CustomerID"),
col("df3.ItemID"))Â 

Â 

\# Show the resultÂ 

dffactSales_gold.show()Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image62.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image63.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image64.png)

1.  A questo punto Ã¨ possibile assicurarsi che i dati di vendita
    rimangano aggiornati eseguendo il codice seguente in un**â€¯new code
    block**:

> CodeCopy
>
> from delta.tables import \*Â 
>
> Â Â Â Â Â 
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')Â 
>
> Â Â Â Â Â 
>
> dfUpdates = dffactSales_goldÂ 
>
> Â Â Â Â Â 
>
> deltaTable.alias('silver') \\
>
> Â  .merge(Â 
>
> Â Â Â  dfUpdates.alias('updates'),Â 
>
> Â Â Â  'silver.OrderDate = updates.OrderDate AND silver.CustomerID =
> updates.CustomerID AND silver.ItemID = updates.ItemID'Â 
>
> Â  ) \\
>
> Â Â  .whenMatchedUpdate(set =Â 
>
> Â Â Â  {Â 
>
> Â Â Â Â Â Â Â Â Â Â Â 
>
> Â Â Â  }Â 
>
> Â  ) \\
>
> Â .whenNotMatchedInsert(values =Â 
>
> Â Â Â  {Â 
>
> Â Â Â Â Â  "CustomerID": "updates.CustomerID",Â 
>
> Â Â Â Â Â  "ItemID": "updates.ItemID",Â 
>
> Â Â Â Â Â  "OrderDate": "updates.OrderDate",Â 
>
> Â Â Â Â Â  "Quantity": "updates.Quantity",Â 
>
> Â Â Â Â Â  "UnitPrice": "updates.UnitPrice",Â 
>
> Â Â Â Â Â  "Tax": "updates.Tax"Â 
>
> Â Â Â  }Â 
>
> Â  ) \\
>
> Â  .execute()Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image65.png)

In questo caso si utilizza l'operazione di unione di Delta Lake per
sincronizzare e aggiornare la tabella factsales_gold con i nuovi dati di
vendita (dffactSales_gold). L'operazione confronta la data dell'ordine,
l'ID cliente e l'ID articolo tra i dati esistenti (tabella argento) e i
nuovi dati (aggiorna DataFrame), aggiornando i record corrispondenti e
inserendo nuovi record in base alle esigenze.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image66.png)

A questo punto si dispone di un livello **d'oro** curato e modellato che
puÃ² essere utilizzato per la creazione di report e l'analisi.

# Esercizio 4: Stabilire la connettivitÃ  tra Azure Databricks e Azure Data Lake Storage (ADLS) Gen 2

Creiamo ora una tabella Delta con l'aiuto dell'account Azure Data Lake
Storage (ADLS) Gen2 usando Azure Databricks. Si creerÃ  quindi un
collegamento OneLake a una tabella Delta in ADLS e si userÃ  Power BI per
analizzare i dati tramite il collegamento ADLS.

## **AttivitÃ  0: Riscattare un pass di Azure e abilitare la sottoscrizione di Azure**

1.  Navigare sul seguente link !! https://www.microsoftazurepass.com/!!
    e fare clic sul pulsante **Start**.

![](./media/image67.png)

2.  Nella pagina di accesso di Microsoft, inserire **Tenant ID,** fare
    clic su **Next**.

![](./media/image68.png)

3.  Nella pagina successiva inserire la tua password e fare clic su
    **Sign In**.

![A screenshot of a login box AI-generated content may be
incorrect.](./media/image69.png)

![Uno screenshot di un errore del computer Descrizione generata
automaticamente](./media/image70.png)

4.  Una volta effettuato l'accesso, nella pagina Microsoft Azure, fare
    clic sulla scheda **Confirm Microsoft Account**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image71.png)

5.  Nella pagina successiva, inserire il codice promozionale, i
    caratteri Captcha e cliccare su **Submit.**

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image72.png)

![Uno screenshot di un errore del computer Descrizione generata
automaticamente](./media/image73.png)

6.  Nella pagina Il tuo profilo inserire i dettagli del tuo profilo e
    cliccare su **Sign up.**

7.  se richiesto, iscriversi all'autenticazione a piÃ¹ fattori e quindi
    accedere al portale Azure navigando fino al seguente link!!
    <https://portal.azure.com/#home>!!

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image74.png)

8.  Nella barra di ricerca, digitare **Subscription** e fare clic
    sull'icona **Subscription** sotto **Services.**

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image75.png)

9.  Dopo il riscatto di Azure Pass, verrÃ  generato un ID sottoscrizione.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image76.png)

## **AttivitÃ  1: Creare un account Azure Data Storage**

1.  Accedere al portale di Azure usando le credenziali di Azure.

2.  Nella home page, dal menu del portale a sinistra, selezionare
    **Storage accounts** per visualizzare un elenco degli account di
    archiviazione. Se il menu del portale non Ã¨ visibile, selezionare il
    pulsante del menu per attivarlo.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image77.png)

3.  Nella pagina **Storage accounts,** selezionare **Create**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image78.png)

4.  Nella scheda Basics, dopo aver selezionato un gruppo di risorse,
    specificare le informazioni essenziali per l'account di
    archiviazione:

[TABLE]

Lasciare invariate le altre impostazioni e selezionare **Review +
create** per accettare le opzioni predefinite e procedere con la
convalida e la creazione dell'account.

Nota: Se non Ã¨ giÃ  stato creato un gruppo di risorse, Ã¨ possibile fare
clic su " **Create new**" e creare una nuova risorsa per l'account di
archiviazione.

![](./media/image79.png)

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image80.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image81.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image82.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image83.png)

5.  Quando si passa alla scheda **Review + create**, Azure esegue la
    convalida sulle impostazioni dell'account di archiviazione scelte.
    Se la convalida viene superata, Ã¨ possibile procedere alla creazione
    dell'account di archiviazione.

Se la convalida non riesce, il portale indica quali impostazioni devono
essere modificate.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image84.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image85.png)

A questo punto Ã¨ stato creato correttamente l'account di archiviazione
dati di Azure.

6.  Passare alla pagina degli account di archiviazione eseguendo una
    ricerca sulla barra di ricerca nella parte superiore della pagina,
    selezionare l'account di archiviazione appena creato.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image86.png)

7.  Nella pagina dell'account di archiviazione passare a **Containers**
    in **Data storage** nel riquadro di spostamento a sinistra per
    creare un nuovo contenitore con il nome !!medalion1!! e fare clic
    sul pulsante **Create**.

Â 

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image87.png)

8.  A questo punto tornare alla pagina **storage account**, selezionare
    **Endpoints** dal menu di spostamento a sinistra. Scorrere verso il
    basso e copiare **Primary endpoint URL** e incollarlo in un blocco
    note. Questo sarÃ  utile durante la creazione del collegamento.

![](./media/image88.png)

9.  Allo stesso modo, andare ai **Access keys** sullo stesso pannello di
    navigazione.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image89.png)

## 

## **AttivitÃ  2: Creare una tabella Delta, creare un collegamento e analizzare i dati nel Lakehouse**

1.  Nella tua lakehouse, selezionare le ellissi **(...)** accanto a
    file, quindi selezionare **New shortcut**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image90.png)

2.  Nella schermata **New shortcut,** selezionare il riquadro **Azure
    Data Lake Storage Gen2**.

![Screenshot delle opzioni del riquadro nella schermata Nuova scelta
rapida.](./media/image91.png)

3.  Specificare i dettagli di connessione per il collegamento:

[TABLE]

4.  E fare clic su **Next**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image92.png)

5.  In questo modo verrÃ  stabilito un collegamento con il contenitore di
    archiviazione di Azure. Selezionare lo spazio di archiviazione e
    selezionare il pulsante **Next**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image93.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image94.png)![Uno screenshot di un computer
Descrizione generata automaticamente](./media/image95.png)

6.  Una volta avviata la procedura guidata, selezionare **Files** e
    selezionare l'icona **"... "** sulla lima di **bronze**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image96.png)

7.  Selezionare **load to tables** e **New table**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image97.png)

8.  Nella finestra pop-up, fornire il nome della tua tabella come
    **bronze_01** e selezionare il tipo di file come **parquet**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image98.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image99.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image100.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image101.png)

9.  Il file **bronze_01** Ã¨ ora visibile nei file.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image102.png)

10. Quindi, selezionare l'icona **"... "** sul file di **bronze**.
    Selezionare **load to tables** e **existing table.**

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image103.png)

11. Specificare il nome della tabella esistente come
    **dimcustomer_gold.** Selezionare il tipo di file come **parquet** e
    selezionare **load.**

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image104.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image105.png)

## **AttivitÃ  3: Creare un modello Semantico utilizzando livello oro per creare un report**

Nell'area di lavoro, Ã¨ ora possibile utilizzare il livello oro per
creare un report e analizzare i dati. Ãˆ possibile accedere al modello
semantico direttamente nell'area di lavoro per creare relazioni e misure
per la creazione di report.

*Si noti che non Ã¨ possibile utilizzare il **modello semantico
predefinito** che viene creato automaticamente quando si crea una
lakehouse. Ãˆ necessario creare un nuovo modello semantico che includa le
tabelle gold create in questo lab, dal lakehouse explorer.*

1.  Nel tuo spazio di lavoro, andare alla tua lakehouse
    **wwilakehouse**. Selezionare quindi **New semantic model** dalla
    barra multifunzione della visualizzazione explorer lakehouse.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image106.png)

2.  Nel popup, assegnare il nome **DatabricksTutorial** al nuovo modello
    semantico e selezionare l'area di lavoro come **Fabric Lakehouse
    Tutorial-29**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image107.png)

3.  Quindi, scorrere verso il basso e selezionare tutto da includere nel
    tuo modello semantico e selezionare **Confirm**.

VerrÃ  aperto il modello semantico in Fabric in cui Ã¨ possibile creare
relazioni e misure, come illustrato di seguito:

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image108.png)

Da qui, tu o altri membri del tuo team dati potete creare report e
dashboard basati sui dati nel tuo lakehouse. Questi report saranno
collegati direttamente allo strato d'oro della tua casa sul lago, in
modo da riflettere sempre i dati piÃ¹ recenti.

# Esercizio 5: Inserimento di dati e analisi con Azure Databricks

1.  Passare al lakehouse nel servizio Power BI e selezionare **Get
    data** e quindi selezionare **New data pipeline**.

![Screenshot che mostra come passare alla nuova opzione della pipeline
di dati dall'interfaccia utente.](./media/image109.png)

1.  Nel prompt **â€¯New Pipeline** immettere un nome per la nuova pipeline
    e quindi selezionare **Create**. **IngestDatapipeline01**

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image110.png)

2.  Per questo esercizio, selezionare i dati di esempio **NYC Taxi -
    Green** come origine dati.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image111.png)

3.  Nella schermata di anteprima, selezionare **Next**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image112.png)

4.  Per Destinazione dati, selezionare il nome della tabella che si
    desidera utilizzare per archiviare i dati della tabella Delta di
    OneLake. Ãˆ possibile scegliere una tabella esistente o crearne una
    nuova. Ai fini di questo lab, selezionare **load into new table** e
    selezionare **Next**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image113.png)

5.  Nella schermata **â€¯Review + Save,** selezionare **Start data
    transfer immediately** e quindi selezionare **â€¯Save + Run**.

![Schermata che mostra come inserire il nome della
tabella.](./media/image114.png)

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image115.png)

6.  Al termine del processo, accedere alla lakehouse e visualizzare la
    tabella delta elencata in /Tables.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image116.png)

7.  Copiare il percorso ABFS (Azure Blob Filesystem) nella tabella delta
    facendo clic con il pulsante destro del mouse sul nome della tabella
    nella visualizzazione Esplora risorse e scegliendo **Properties**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image117.png)

8.  Aprire il notebook di Azure Databricks ed eseguire il codice.

olsPath = "**abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample**"Â Â 

df=spark.read.format('delta').option("inferSchema","true").load(olsPath)Â 

df.show(5)Â 

*Nota: sostituire il percorso del file in grassetto con quello che hai
copiato.*

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image118.png)

9.  Aggiornare i dati della tabella Delta modificando il valore di un
    campo.

%sqlÂ 

update delta.\`abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample\` set vendorID = 99999 where vendorID
= 1;Â 

*Nota: sostituire il percorso del file in grassetto con quello che hai
copiato.*

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image119.png)

# Esercizio 6: Pulire le risorse

In questo esercizio si Ã¨ appreso come creare un'architettura medallion
in una lakehouse di Microsoft Fabric.

Se hai finito di esplorare la tua casa sul lago, puoi eliminare l'area
di lavoro che hai creato per questo esercizio.

1.  Seleziona il tuo spazio di lavoro, il **Fabric Lakehouse
    Tutorial-29** dal menu di navigazione a sinistra. Apre la
    visualizzazione degli elementi dell'area di lavoro.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image120.png)

2.  Selezionare l'icona ***...*** sotto il nome dell'area di lavoro e
    selezionare **Workspace settings**.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image121.png)

3.  Scorrere verso il basso e selezionare **Remove this workspace.**

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image122.png)

4.  Fare clic su **Delete** nell'avviso che si apre.

![Uno sfondo bianco con testo nero Descrizione generata
automaticamente](./media/image123.png)

5.  Attendere una notifica che indica che l'area di lavoro Ã¨ stata
    eliminata prima di passare al lab successivo.

![Uno screenshot di un computer Descrizione generata
automaticamente](./media/image124.png)

**Sommario**:

Questo lab guida i partecipanti nella creazione di un'architettura
medallion in una lakehouse di Microsoft Fabric usando notebooks. I
passaggi chiave includono la configurazione di un'area di lavoro, la
creazione di una lakehouse, il caricamento dei dati sul livello di
bronzo per l'acquisizione iniziale, la trasformazione in una tabella
Delta argentata per l'elaborazione strutturata, l'ulteriore
perfezionamento in tabelle Delta dorate per analisi avanzate,
l'esplorazione di modelli semantici e la creazione di relazioni tra i
dati per un'analisi approfondita.

## 
