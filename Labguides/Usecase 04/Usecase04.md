# ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ 04: Azure Databricks ã¨ Microsoft Fabri ã«ã‚ˆã‚‹æœ€æ–°ã®ã‚¯ãƒ©ã‚¦ãƒ‰ ã‚¹ã‚±ãƒ¼ãƒ«åˆ†æ

**ç´¹ä»‹**

ã“ã®ãƒ©ãƒœã§ã¯ã€Azure Databricks ã¨ Microsoft Fabric
ã®çµ±åˆã«ã¤ã„ã¦å­¦ç¿’ã—ã€Medallion
ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã¦Lakehouseã‚’ä½œæˆãƒ»ç®¡ç†ã™ã‚‹æ–¹æ³•ã€Azure Databricks
ã‚’ä½¿ç”¨ã—ã¦ Azure Data Lake Storage (ADLS) Gen2 ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’æ´»ç”¨ã—ãŸ
Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã€ãã—ã¦ Azure Databricks
ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã‚€æ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚ã“ã®ãƒãƒ³ã‚ºã‚ªãƒ³ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Lakehouseã®ä½œæˆã€ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã€ãã—ã¦åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿åˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚’å¯èƒ½ã«ã™ã‚‹æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ´»ç”¨ã«å¿…è¦ãªæ‰‹é †ã‚’é †ã‚’è¿½ã£ã¦èª¬æ˜ã—ã¾ã™ã€‚

ãƒ¡ãƒ€ãƒªã‚ªãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯3ã¤ã®ç•°ãªã‚‹å±¤ï¼ˆã¾ãŸã¯ã‚¾ãƒ¼ãƒ³ï¼‰ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™.

- ãƒ–ãƒ­ãƒ³ã‚º:
  RAWã‚¾ãƒ¼ãƒ³ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ã“ã®æœ€åˆã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ã€ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã®å½¢å¼ã§ä¿å­˜ã—ã¾ã™ã€‚ã“ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€é€šå¸¸ã€è¿½åŠ ã®ã¿å¯èƒ½ã§ä¸å¤‰ã§ã™ã€‚

- ã‚·ãƒ«ãƒãƒ¼ï¼šã‚¨ãƒ³ãƒªãƒƒãƒãƒ‰ã‚¾ãƒ¼ãƒ³ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ã“ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ã€ãƒ–ãƒ­ãƒ³ã‚ºãƒ¬ã‚¤ãƒ¤ãƒ¼ã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã—ã¾ã™ã€‚ç”Ÿãƒ‡ãƒ¼ã‚¿ã¯ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã¨æ¨™æº–åŒ–ãŒè¡Œã‚ã‚Œã€è¡¨ï¼ˆè¡Œã¨åˆ—ï¼‰ã¨ã—ã¦æ§‹é€ åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãŸã€ä»–ã®ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆã™ã‚‹ã“ã¨ã§ã€é¡§å®¢ã€è£½å“ãªã©ã€ã‚ã‚‰ã‚†ã‚‹ãƒ“ã‚¸ãƒã‚¹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ“ãƒ¥ãƒ¼ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

- ã‚´ãƒ¼ãƒ«ãƒ‰ï¼šã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¾ãƒ¼ãƒ³ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ã“ã®æœ€çµ‚å±¤ã¯ã€ã‚·ãƒ«ãƒãƒ¼å±¤ã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã¯ã€ä¸‹æµã®ãƒ“ã‚¸ãƒã‚¹è¦ä»¶ã‚„åˆ†æè¦ä»¶ã«åˆã‚ã›ã¦èª¿æ•´ã•ã‚Œã¾ã™ã€‚ãƒ†ãƒ¼ãƒ–ãƒ«ã¯é€šå¸¸ã€ã‚¹ã‚¿ãƒ¼ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆã«æº–æ‹ ã—ã¦ãŠã‚Šã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ãŒæœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

**ç›®çš„**:

- Microsoft Fabric Lakehouse ã«ãŠã‘ã‚‹ Medallion
  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åŸå‰‡ã‚’ç†è§£ã—ã¾ã™ã€‚

- Medallion ãƒ¬ã‚¤ãƒ¤ãƒ¼ (Bronzeã€Silverã€Gold)
  ã‚’ä½¿ç”¨ã—ã¦ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

- ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼æ¸ˆã¿ãŠã‚ˆã³ã‚¨ãƒ³ãƒªãƒƒãƒã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã—ã€é«˜åº¦ãªåˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚’å®Ÿç¾ã—ã¾ã™ã€‚

- ãƒ‡ãƒ¼ã‚¿ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€CI/CDã€åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ã‚¯ã‚¨ãƒªã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

- OneLake ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã€OneLake
  ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

- Fabric ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ã€OneLake ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚Šã€Delta
  ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦æ›¸ãæˆ»ã—ã¾ã™ã€‚

- Fabric ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ã€Spark ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æãŠã‚ˆã³å¤‰æ›ã—ã¾ã™ã€‚

- SQL ã‚’ä½¿ç”¨ã—ã¦ã€OneLake ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã® 1 ã¤ã®ã‚³ãƒ”ãƒ¼ã‚’ã‚¯ã‚¨ãƒªã—ã¾ã™ã€‚

- Azure Databricks ã‚’ä½¿ç”¨ã—ã¦ã€Azure Data Lake Storage (ADLS) Gen2
  ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã« Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

- ADLS å†…ã® Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã® OneLake ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

- Power BI ã‚’ä½¿ç”¨ã—ã¦ã€ADLS ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆçµŒç”±ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¾ã™ã€‚

- Azure Databricks ã‚’ä½¿ç”¨ã—ã¦ã€OneLake å†…ã® Delta
  ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’èª­ã¿å–ã£ã¦å¤‰æ›´ã—ã¾ã™ã€‚

# æ‰‹é † 1: Bringing your sample data to Lakehouse

In this æ‰‹é †, you'll go through the process of creating a lakehouse and
loading data into it using Microsoft Fabric.

Task:trail

## **ã‚¿ã‚¹ã‚¯ 1: Fabricãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ä½œæˆ**

ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€Fabric
ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã«ã¯ã€ã“ã®Lakehouse
ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã«å¿…è¦ãªã™ã¹ã¦ã®é …ç›® (Lakehouseã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã€Data
Factory ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã€Power BI ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ¬ãƒãƒ¼ãƒˆãªã©)
ãŒå«ã¾ã‚Œã¦ã„ã¾ã™

1.  ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ãã€ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒãƒ¼ã«ç§»å‹•ã—ã¦ã€æ¬¡ã®URLã‚’å…¥åŠ›ã¾ãŸã¯è²¼ã‚Šä»˜ã‘ã¦
    [https://app.fabric.microsoft.com/](https://app.fabric.microsoft.com/,)
    ã‹ã‚‰**Enterãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã€‚**

> ![A search engine window with a red box Description automatically
> generated with medium confidence](./media/image1.png)

2.  **Power BIã‚¦ã‚£ãƒ³ãƒ‰ã«æˆ»ã‚Šã¾ã™ã€‚**Power BIãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã®å·¦å´
    ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã§**Workspacesã«ç§»å‹•ã—ã€ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã€‚**

![](./media/image2.png)

3.  In the Workspacesãƒšãƒ¼ãƒ³ã«**+**Â **New
    workspace**ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹**ã€‚**

> ![](./media/image3.png)

4.  å³å´ã«è¡¨ç¤ºã™ã‚‹**Create a
    workspaceãƒšãƒ¼ãƒ³ã§æ¬¡ã®è©³ç´°ã‚’å…¥åŠ›ã—ã¦Applyãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã€‚**

[TABLE]

> ![](./media/image4.png)

![A screenshot of a computer Description automatically
generated](./media/image5.png)

5.  å±•é–‹ãŒå®Œäº†ã™ã‚‹ã¾ã§ã«å¾…ã¡ã¾ã™ã€‚å®Œäº†ã™ã‚‹ã«ã¯2-3åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚

![](./media/image6.png)

## **ã‚¿ã‚¹ã‚¯ 2: Lakehouseã‚’ä½œæˆ**

1.  In the **Power BI Fabric Lakehouse
    Tutorial-XXãƒšãƒ¼ã‚¸ã«å·¦ä¸‹ã«ã‚ã‚‹Power BIã‚¢ã‚¤ã‚³ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€Data
    Engineering**ã‚’é¸æŠã™ã‚‹ã€‚

> ![](./media/image7.png)

2.  **Synapse**Â **Data
    Engineering**Â **Homeãƒšãƒ¼ã‚¸ã«Lakehouseã‚’é¸æŠã—ã¦ä½œæˆã™ã‚‹ã€‚**Â 

![](./media/image8.png)

![A screenshot of a computer Description automatically
generated](./media/image9.png)

3.  **New
    lakehouseãƒ€ã‚¤ã‚¢ãƒ­ã‚°ãƒœãƒƒã‚¯ã‚¹ã«Nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«wwilakehouse**ã‚’å…¥åŠ›ã—ã¦ã€**Createãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã“ã¨ã§æ–°ã—ã„**lakehouseãŒé–‹ãã€‚

> **æ³¨æ„: wwilakehouseã®å‰ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ã™ã‚‹ã‚ˆã†ç¢ºèªã™ã‚‹**
>
> ![](./media/image10.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image11.png)
>
> ![](./media/image12.png)

4.  **Successfully created SQL endpoint**.ã‚’è¿°ã¹ã‚‹é€šçŸ¥ã‚’è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

> ![](./media/image13.png)

# æ‰‹é † 2: Azure Databricksã‚’ä½¿ç”¨ã™ã‚‹Medallion Architectureã‚’å®Ÿåœ°

# ã‚¿ã‚¹ã‚¯ 1: ãƒ–ãƒ©ã‚¦ã‚ºãƒ¬ãƒ¼ãƒ¤ã®è¨­å®š

1.  In the **wwilakehouseãƒšãƒ¼ã‚¸ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¨ªã«ã‚ã‚‹**More
    ã‚¢ã‚¤ã‚³ãƒ³(â€¦)next to the files (â€¦), and select **New subfolder**

![](./media/image14.png)

2.  On the pop-uã§ãƒ•ã‚©ãƒ«ãƒ€åã«**bronzeã‚’å…¥åŠ›ã—ã¦ã€**Createã‚’é¸æŠã™ã‚‹ã€‚

![A screenshot of a computer Description automatically
generated](./media/image15.png)

3.  ãã‚Œã§ã¯ã€ãƒ–ãƒ­ãƒ³ã‚ºãƒ•ã‚¡ã‚¤ãƒ«(â€¦)ã®æ¨ªã®Moreã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠã—ã¦ã‹ã‚‰**Upload**
    ã¨æ¬¡ã«**upload files**ã‚’é¸æŠã™ã‚‹ã€‚

![A screenshot of a computer Description automatically
generated](./media/image16.png)

4.  **Upload fileãƒšãƒ¼ãƒ³ä¸Šã§Upload fileãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã‚’é¸æŠã™ã‚‹ã€‚Browse
    buttonã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€C:\LabFiles**ã«ç§»å‹•ã—ã€å¿…è¦ãªsales data files
    (2019, 2020, 2021) ã‚’é¸æŠã—ã¦**Open**ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹**ã€‚
    ãã‚Œã‹ã‚‰ã€Upload**ã‚’é¸æŠã—ã€Lakehouseã«ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°ã—ã„ãƒ–ãƒ­ãƒ³ã‚ºãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚

![A screenshot of a computer Description automatically
generated](./media/image17.png)

> ![A screenshot of a computer Description automatically
> generated](./media/image18.png)

5.  Click on
    **bronzeãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ãŒé€šå¸¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒåçœã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã™ã‚‹**

![A screenshot of a computer Description automatically
generated](./media/image19.png)

# æ‰‹é † 3: Apache Spark ã§ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ã€ãƒ¡ãƒ€ãƒªã‚ªãƒ³ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ SQL ã§ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã™ã‚‹

## **ã‚¿ã‚¹ã‚¯ 1: ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ã¦ã€ã‚·ãƒ«ãƒãƒ¼ãƒ‡ãƒ«ã‚¿ã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚**

**wwilakehouse**ãƒšãƒ¼ã‚¸ã«ã‚³ãƒãƒ³ãƒ‰ãƒãƒ¼**ã«Open
notebook**ãƒ‰ãƒ­ãƒƒãƒ—ã«ç§»å‹•ã—ã¦ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€**New notebook** ã‚’é¸æŠã™ã‚‹ã€‚

![A screenshot of a computer Description automatically
generated](./media/image20.png)

1.  Select the first cell (which is currently aÂ *code*Â cell), and then
    in the dynamic tool bar at its top-right, use theÂ **Mâ†“**Â button to
    **convert the cell to aÂ markdownÂ cell**.

![A screenshot of a computer Description automatically
generated](./media/image21.png)

2.  ã‚»ãƒ«ãŒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
    ã‚»ãƒ«ã«å¤‰æ›´ã•ã‚Œã‚‹ã¨ã€ãã“ã«å«ã¾ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã•ã‚Œã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image22.png)

3.  ã‚»ãƒ¼ãƒ«ã‚’**ğŸ–‰**Â (ç·¨é›†)ãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨ã—ã€ã‚»ãƒ«ã‚’ã‚¨ãƒ‡ã‚£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã€å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç½®ãæ›ãˆã¦ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’æ¬¡ã®ç”¨ã«å¤‰æ›´ã—ã¾ã™ã€‚

CodeCopy

\# Sales order data exploration

ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«ã‚ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã€ã‚»ãƒ¼ãƒ«ã‚¹ã‚ªãƒ¼ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã™ã‚‹ã€‚

![A screenshot of a computer Description automatically
generated](./media/image23.png)

![A screenshot of a computer Description automatically
generated](./media/image24.png)

4.  ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ã‚»ãƒ«ã®å¤–å´ã®ä»»æ„ã®å ´æ‰€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ç·¨é›†ã‚’åœæ­¢ã—ã€ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image25.png)

5.  ã‚»ãƒ«ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã®ä¸‹ã«ã‚ã‚‹ +
    Codeã‚¢ã‚¤ã‚³ãƒ³ã‚’ä½¿ç”¨ã—ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«æ–°ã—ã„ã‚»ãƒ«ã‚’è¿½åŠ ã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image26.png)

6.  ä»Šã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã€ãƒ–ãƒ­ãƒ³ã‚ºãƒ¬ãƒ¼ãƒ¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’Spark
    DataFrameã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã®æ—¢å­˜ã®ã‚»ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚ãã“ã«ã¯ã€ã„ãã¤ã‹ã®ç°¡å˜ãªã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
    ã“ã‚Œã‚‰ 2 è¡Œã‚’å¼·èª¿è¡¨ç¤ºã—ã¦å‰Šé™¤ã—ã¾ã™ã€‚ã“ã®ã‚³ãƒ¼ãƒ‰ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“ã€‚

*æ³¨:
ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€Pythonã€Scalaã€SQLãªã©ã€ã•ã¾ã–ã¾ãªè¨€èªã§ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚ã“ã®æ‰‹é †ã§ã¯ã€PySparkã¨SQLã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã¾ãŸã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚»ãƒ«ã‚’è¿½åŠ ã—ã¦ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚„ç”»åƒã‚’è¡¨ç¤ºã—ã€ã‚³ãƒ¼ãƒ‰ã‚’è¨˜è¿°ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚*

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ã€ŒRunã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„*ã€‚*

CodeCopy

from pyspark.sql.types import \*

\# Create the schema for the table

orderSchema = StructType(\[

StructField("SalesOrderNumber", StringType()),

StructField("SalesOrderLineNumber", IntegerType()),

StructField("OrderDate", DateType()),

StructField("CustomerName", StringType()),

StructField("Email", StringType()),

StructField("Item", StringType()),

StructField("Quantity", IntegerType()),

StructField("UnitPrice", FloatType()),

StructField("Tax", FloatType())

\])

\# Import all files from bronze folder of lakehouse

df = spark.read.format("csv").option("header",
"true").schema(orderSchema).load("Files/bronze/\*.csv")

\# Display the first 10 rows of the dataframe to preview your data

display(df.head(10))

![](./media/image27.png)

***æ³¨:
ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§Sparkã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã®ã¯åˆã‚ã¦ãªã®ã§ã€Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã®ãŸã‚ã€æœ€åˆã®å®Ÿè¡Œã«ã¯1åˆ†ã»ã©ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚æ¬¡å›ã®å®Ÿè¡Œã¯ã‚ˆã‚ŠçŸ­æ™‚é–“ã§å®Œäº†ã—ã¾ã™ã€‚***

![A screenshot of a computer Description automatically
generated](./media/image28.png)

7.  å®Ÿè¡Œã—ãŸã‚³ãƒ¼ãƒ‰ã¯ã€ãƒ‡ãƒ¼ã‚¿ãŒ**bronze**
    ãƒ•ã‚©ãƒ«ãƒ€å†…ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Sparkãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤ºã—ã¾ã—ãŸã€‚ã€€

> **æ³¨:
> å‡ºåŠ›ãƒšã‚¤ãƒ³ã®å·¦ä¸Šã«ã‚ã‚‹â€¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’é¸æŠã™ã‚‹ã¨ã€ã‚»ãƒ«å‡ºåŠ›ã®å†…å®¹ã‚’ã‚¯ãƒªã‚¢ã€éè¡¨ç¤ºã€è‡ªå‹•ã‚µã‚¤ã‚ºå¤‰æ›´ã§ãã¾ã™ã€‚**

8.  æ¬¡ã«ã€PySparkãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ãŸã‚ã®åˆ—ã‚’è¿½åŠ ã—ã€æ—¢å­˜ã®åˆ—ã®å€¤ã‚’æ›´æ–°ã—ã¾ã™ã€‚+ãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨ã—ã¦æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ã—ã€æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’ã‚»ãƒ«ã«è¿½åŠ ã—ã¾ã™:

> CodeCopy
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> \# Add columns IsFlagged, CreatedTS and ModifiedTS
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \<
> '2019-08-01',True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()).withColumn("ModifiedTS",
> current_timestamp())
>
> \# Update CustomerName to "Unknown" if CustomerName null or empty
>
> df = df.withColumn("CustomerName", when((col("CustomerName").isNull()
> |
> (col("CustomerName")=="")),lit("Unknown")).otherwise(col("CustomerName")))
>
> ã‚³ãƒ¼ãƒ‰ã®æœ€åˆã®è¡Œã¯ã€PySparkã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æ–°ã—ã„åˆ—ã‚’è¿½åŠ ã—ã¦ã€ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åã€æ³¨æ–‡ãŒå¯¾è±¡ä¼šè¨ˆå¹´åº¦ä»¥å‰ã®ã‚‚ã®ã¨ã—ã¦ãƒ•ãƒ©ã‚°ä»˜ã‘ã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã€è¡ŒãŒã„ã¤ä½œæˆãŠã‚ˆã³å¤‰æ›´ã•ã‚ŒãŸã‹ã‚’è¿½è·¡ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
>
> æœ€å¾Œã«ã€CustomerName åˆ—ãŒ null ã¾ãŸã¯ç©ºã®å ´åˆã¯ "Unknown"
> ã«æ›´æ–°ã—ã¾ã™ã€‚
>
> Then, Run the cell to execute the code using theÂ æ¬¡ã«ã€**\*\*â–·**Â (*Run
> cell*)\*\*ãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨ã—ã€ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã€ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ ã€‚

![A screenshot of a computer Description automatically
generated](./media/image29.png)

![A screenshot of a computer Description automatically
generated](./media/image30.png)

9.  æ¬¡ã«ã€Delta
    Lakeå½¢å¼ã‚’ä½¿ç”¨ã—ã¦ã€salesãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®sales_silverãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ã—ã¾ã™ã€‚æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œæˆã—ã€ã‚»ãƒ«ã«æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¾ã™:

> CodeCopy

from pyspark.sql.types import \*

from delta.tables import \*

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

Â  Â  StructField("SalesOrderNumber", StringType(), True),

Â  Â  StructField("SalesOrderLineNumber", IntegerType(), True),

Â  Â  StructField("OrderDate", DateType(), True),

Â  Â  StructField("CustomerName", StringType(), True),

Â  Â  StructField("Email", StringType(), True),

Â  Â  StructField("Item", StringType(), True),

Â  Â  StructField("Quantity", IntegerType(), True),

Â  Â  StructField("UnitPrice", FloatType(), True),

Â  Â  StructField("Tax", FloatType(), True),

Â  Â  StructField("FileName", StringType(), True),

Â  Â  StructField("IsFlagged", BooleanType(), True),

Â  Â  StructField("CreatedTS", TimestampType(), True),

Â  Â  StructField("ModifiedTS", TimestampType(), True)

\])

\# Create or replace the sales_silver table with the defined schema

DeltaTable.createIfNotExists(spark) \\

Â  Â  .tableName("wwilakehouse.sales_silver") \\

Â  Â  .addColumns(silver_table_schema) \\

Â  Â  .execute()

Â  Â 

10. **\*\*â–·**Â (*Run cell*)\*\*
    ãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã€ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

11. Lakehouse explorerãƒšãƒ¼ãƒ³ã«Tablesã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚ã‚‹**â€¦**
    ã‚’é¸æŠã—ã¦**Refresh**ã‚’é¸æŠã™ã‚‹ã€‚ã“ã‚Œã§ã€ãƒªã‚¹ãƒˆã•ã‚ŒãŸæ–°ã—ã„**sales_silverãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¡¨ç¤ºã§ãã¾ã™ã€‚**Â **â–²**Â (ä¸‰è§’ã‚¢ã‚¤ã‚³ãƒ³)ãŒã€Deltaãƒ†ãƒ¼ãƒ–ãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’æŒ‡å®šã—ã¾ã™ã€‚.

> **æ³¨:
> æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã¯ã€æ•°ç§’å¾…ã£ã¦ã‹ã‚‰ã‚‚ã†ä¸€åº¦\[Refresh\]ã‚’é¸æŠã™ã‚‹ã‹ã€ãƒ–ãƒ©ã‚¦ã‚¶ã‚¿ãƒ–å…¨ä½“ã‚’æ›´æ–°ã—ã¾ã™ã€‚**.
>
> ![A screenshot of a computer Description automatically
> generated](./media/image31.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image32.png)

1.  æ¬¡ã«ã€ Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¯¾ã—ã¦ **upsert
    æ“ä½œ**ã‚’å®Ÿè¡Œã—ã€ç‰¹å®šã®æ¡ä»¶ã«åŸºã¥ã„ã¦æ—¢å­˜ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°ã—ã€ä¸€è‡´ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŒ¿å…¥ã—ã¾ã™ã€‚æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ã—ã€æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è²¼ã‚Šä»˜ã‘ã¾ã™:

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> from delta.tables import \*
>
> \# Define the schema for the source data
>
> orderSchema = StructType(\[
>
> StructField("SalesOrderNumber", StringType(), True),
>
> StructField("SalesOrderLineNumber", IntegerType(), True),
>
> StructField("OrderDate", DateType(), True),
>
> StructField("CustomerName", StringType(), True),
>
> StructField("Email", StringType(), True),
>
> StructField("Item", StringType(), True),
>
> StructField("Quantity", IntegerType(), True),
>
> StructField("UnitPrice", FloatType(), True),
>
> StructField("Tax", FloatType(), True)
>
> \])
>
> \# Read data from the bronze folder into a DataFrame
>
> df = spark.read.format("csv").option("header",
> "true").schema(orderSchema).load("Files/bronze/\*.csv")
>
> \# Add additional columns
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \< '2019-08-01',
> True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()) \\
>
> .withColumn("ModifiedTS", current_timestamp()) \\
>
> .withColumn("CustomerName", when((col("CustomerName").isNull()) |
> (col("CustomerName") == ""),
> lit("Unknown")).otherwise(col("CustomerName")))
>
> \# Define the path to the Delta table
>
> deltaTablePath = "Tables/sales_silver"
>
> \# Create a DeltaTable object for the existing Delta table
>
> deltaTable = DeltaTable.forPath(spark, deltaTablePath)
>
> \# Perform the merge (upsert) operation
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> df.alias('updates'),
>
> 'silver.SalesOrderNumber = updates.SalesOrderNumber AND \\
>
> silver.OrderDate = updates.OrderDate AND \\
>
> silver.CustomerName = updates.CustomerName AND \\
>
> silver.Item = updates.Item'
>
> ) \\
>
> .whenMatchedUpdate(set = {
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "Email": "updates.Email",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .whenNotMatchedInsert(values = {
>
> "SalesOrderNumber": "updates.SalesOrderNumber",
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "OrderDate": "updates.OrderDate",
>
> "CustomerName": "updates.CustomerName",
>
> "Email": "updates.Email",
>
> "Item": "updates.Item",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "CreatedTS": "current_timestamp()",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .execute()

12. ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã€**\*\*â–·**(*Run
    cell*)\*\*ãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™.

![A screenshot of a computer Description automatically
generated](./media/image33.png)

ã“ã®æ“ä½œã¯ã€ç‰¹å®šã®åˆ—ã®å€¤ã«åŸºã¥ã„ã¦ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã®æ—¢å­˜ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°ã—ã€ä¸€è‡´ã™ã‚‹ã‚‚ã®ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŒ¿å…¥ã§ãã‚‹ãŸã‚é‡è¦ã§ã™ã€‚ã“ã‚Œã¯ã€æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨æ–°è¦ãƒ¬ã‚³ãƒ¼ãƒ‰ã®æ›´æ–°ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚½ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆã«ã‚ˆãå¿…è¦ãªè¦ä»¶ã§ã™ã€‚

ã“ã‚Œã§ã€ã‚·ãƒ«ãƒãƒ¼ãƒ‡ãƒ«ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒä¿å­˜ã•ã‚Œã€ã•ã‚‰ã«å¤‰æ›ã¨ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’è¡Œã†æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚

ã“ã‚Œã§ã€Bronze ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’é€šå¸¸ã«å–å¾—ã—ã¦å¤‰æ›ã—ã€Silver Delta
ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚æ–°ã—ã„ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ã•ã‚‰ã«å¤‰æ›ã—ã€ã‚¹ã‚¿ãƒ¼
ã‚¹ã‚­ãƒ¼ãƒã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€Gold Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

*Note that you could have done all of this in a single notebook, but for
the purposes of this æ‰‹é † youâ€™re using separate notebooks to demonstrate
the process of transforming data from bronze to silver and then from
silver to gold. This can help with debugging, troubleshooting, and
reuse*.

## **ã‚¿ã‚¹ã‚¯ 2: Gold Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿**

1.  Fabric Lakehouse Tutorial-29 ãƒ›ãƒ¼ãƒ  ãƒšãƒ¼ã‚¸ã«æˆ»ã‚‹.

> ![A screenshot of a computer Description automatically
> generated](./media/image34.png)

2.  **wwilakehouseã‚’é¸æŠã™ã‚‹**

![A screenshot of a computer Description automatically
generated](./media/image35.png)

3.  In the lakehouse explorerãƒšãƒ¼ãƒ³å†…ã« ã€Explorerãƒšãƒ¼ãƒ³ã«Tables
    ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ãƒªã‚¹ãƒˆã•ã‚ŒãŸsales_silver ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image36.png)

4.  æ¬¡ã«ã€**Transform data for
    Goldã¨åå‰ã¤ã‘ã‚‰ã‚ŒãŸãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ã‚’ä½œæˆã—ã¾ã™ã€‚**ã“ã®ãŸã‚ã€ã‚³ãƒãƒ³ãƒ‰ãƒãƒ¼ã«ã‚ã‚‹**Open
    notebookã®ãƒ‰ãƒ­ãƒƒãƒ—ã«ç§»å‹•ã—ã¦ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‹ã‚‰New
    notebook**ã‚’é¸æŠï½“ã‚‹ã†ã€

![A screenshot of a computer Description automatically
generated](./media/image37.png)

> æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰
> ãƒ–ãƒ­ãƒƒã‚¯ã§ã€å®šå‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰Šé™¤ã—ã€æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ã‚¹ã‚¿ãƒ¼
> ã‚¹ã‚­ãƒ¼ãƒã®æ§‹ç¯‰ã‚’é–‹å§‹ã—ã¦å®Ÿè¡Œã—ã¾ã™ï¼š
>
> CodeCopy

\# Load data to the dataframe as a starting point to create the gold
layer

df = spark.read.table("wwilakehouse.sales_silver")

\# Display the first few rows of the dataframe to verify the data

df.show()

![A screenshot of a computer Description automatically
generated](./media/image38.png)

5.  æ¬¡ã«ã€æ–°ã—ã„ã‚³ãƒ¼ãƒ‰
    ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ã—ã€æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è²¼ã‚Šä»˜ã‘ã¦æ—¥ä»˜ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³
    ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã€å®Ÿè¡Œã—ã¾ã™:

Â from pyspark.sql.types import \*

Â from delta.tables import\*

Â  Â 

Â # Define the schema for the dimdate_gold table

Â DeltaTable.createIfNotExists(spark) \\

Â  Â  Â .tableName("wwilakehouse.dimdate_gold") \\

Â  Â  Â .addColumn("OrderDate", DateType()) \\

Â  Â  Â .addColumn("Day", IntegerType()) \\

Â  Â  Â .addColumn("Month", IntegerType()) \\

Â  Â  Â .addColumn("Year", IntegerType()) \\

Â  Â  Â .addColumn("mmmyyyy", StringType()) \\

Â  Â  Â .addColumn("yyyymm", StringType()) \\

Â  Â  Â .execute()

![A screenshot of a computer Description automatically
generated](./media/image39.png)

æ³¨:
ä½œæ¥­ã®é€²æ—çŠ¶æ³ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€ã„ã¤ã§ã‚‚display(df)Â ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚ã“ã®å ´åˆã€â€˜display(dfdimDate_gold)â€™ã‚’å®Ÿè¡Œã—ã¦ã€dimDate_goldãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å†…å®¹ã‚’ç¢ºèªã—ã¾ã™ã€‚

6.  æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã«**æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¦å®Ÿè¡Œã—**ã€æ—¥ä»˜ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³
    **dimdate_gold** ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆã—ã¾ã™:

> CodeCopy

from pyspark.sql.functions import col, dayofmonth, month, year,
date_format

Â  Â 

Â # Create dataframe for dimDate_gold

Â  Â 

dfdimDate_gold
=df.dropDuplicates(\["OrderDate"\]).select(col("OrderDate"), \\

Â  Â  Â  Â  Â dayofmonth("OrderDate").alias("Day"), \\

Â  Â  Â  Â  Â month("OrderDate").alias("Month"), \\

Â  Â  Â  Â  Â year("OrderDate").alias("Year"), \\

Â  Â  Â  Â  Â date_format(col("OrderDate"), "MMM-yyyy").alias("mmmyyyy"), \\

Â  Â  Â  Â  Â date_format(col("OrderDate"), "yyyyMM").alias("yyyymm"), \\

Â  Â  Â ).orderBy("OrderDate")

Â # Display the first 10 rows of the dataframe to preview your data

display(dfdimDate_gold.head(10))

![A screenshot of a computer Description automatically
generated](./media/image40.png)

![A screenshot of a computer Description automatically
generated](./media/image41.png)

> ã‚³ãƒ¼ãƒ‰ã‚’æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã™ã‚‹éš›ã«ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã§ä½•ãŒèµ·ã“ã£ã¦ã„ã‚‹ã‹ã‚’æŠŠæ¡ã—ã€ç›£è¦–ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚åˆ¥ã®æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã«ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒå…¥åŠ›ã•ã‚ŒãŸã¨ãã«æ—¥ä»˜ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‚’æ›´æ–°ã™ã‚‹**æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¦å®Ÿè¡Œ**ã—ã¾ã™ã€‚CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')
>
> dfUpdates = dfdimDate_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "OrderDate": "updates.OrderDate",
>
> "Day": "updates.Day",
>
> "Month": "updates.Month",
>
> "Year": "updates.Year",
>
> "mmmyyyy": "updates.mmmyyyy",
>
> "yyyymm": "yyyymm"
>
> }
>
> ) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image42.png)

> æ—¥ä»˜ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸã€‚

![A screenshot of a computer Description automatically
generated](./media/image43.png)

## **ã‚¿ã‚¹ã‚¯ 3: é¡§å®¢ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ã€‚**

1.  é¡§å®¢ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€**æ–°ã—ã„ã‚³ãƒ¼ãƒ‰
    ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ã—**ã€æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è²¼ã‚Šä»˜ã‘ã¦å®Ÿè¡Œã—ã¾ã™:

> CodeCopy

Â from pyspark.sql.types import \*

Â from delta.tables import \*

Â  Â 

Â # Create customer_gold dimension delta table

Â DeltaTable.createIfNotExists(spark) \\

Â  Â  Â .tableName("wwilakehouse.dimcustomer_gold") \\

Â  Â  Â .addColumn("CustomerName", StringType()) \\

Â  Â  Â .addColumn("Email", Â StringType()) \\

Â  Â  Â .addColumn("First", StringType()) \\

Â  Â  Â .addColumn("Last", StringType()) \\

Â  Â  Â .addColumn("CustomerID", LongType()) \\

Â  Â  Â .execute()

![A screenshot of a computer Description automatically
generated](./media/image44.png)

![A screenshot of a computer Description automatically
generated](./media/image45.png)

2.  æ–°ã—ã„ã‚³ãƒ¼ãƒ‰
    ãƒ–ãƒ­ãƒƒã‚¯ã«**æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¦å®Ÿè¡Œ**ã—ã€é‡è¤‡ã™ã‚‹é¡§å®¢ã‚’å‰Šé™¤ã—ã€ç‰¹å®šã®åˆ—ã‚’é¸æŠã—ã€ã€ŒCustomerNameã€åˆ—ã‚’åˆ†å‰²ã—ã¦ã€ŒFirstã€ã¨ã€ŒLastã€ã®åå‰åˆ—ã‚’ä½œæˆã—ã¾ã™ã€‚

> CodeCopy
>
> from pyspark.sql.functions import col, split
>
> \# Create customer_silver dataframe
>
> dfdimCustomer_silver =
> df.dropDuplicates(\["CustomerName","Email"\]).select(col("CustomerName"),col("Email"))
> \\
>
> .withColumn("First",split(col("CustomerName"), " ").getItem(0)) \\
>
> .withColumn("Last",split(col("CustomerName"), " ").getItem(1))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimCustomer_silver.head(10))

![A screenshot of a computer Description automatically
generated](./media/image46.png)

ã“ã“ã§ã¯ã€é‡è¤‡ã®å‰Šé™¤ã€ç‰¹å®šã®åˆ—ã®é¸æŠã€ã€ŒCustomerNameã€åˆ—ã‚’åˆ†å‰²ã—ã¦ã€ŒFirstã€ã¨ã€ŒLastã€ã®åˆ—ã‚’ä½œæˆã™ã‚‹ãªã©ã€ã•ã¾ã–ã¾ãªå¤‰æ›ã‚’å®Ÿè¡Œã—ã¦ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
dfdimCustomer_silver
ã‚’ä½œæˆã—ã¾ã—ãŸã€‚çµæœã¯ã€ã€ŒCustomerNameã€åˆ—ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã€ŒFirstã€ã¨ã€ŒLastã€ã®åˆ—ã‚’å«ã‚€ã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚Œæ§‹é€ åŒ–ã•ã‚ŒãŸé¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image47.png)

3.  æ¬¡ã«ã€é¡§å®¢ã®IDåˆ—ã‚’ä½œæˆã—ã¾ã™ã€‚æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã«ä»¥ä¸‹ã‚’è²¼ã‚Šä»˜ã‘ã¦å®Ÿè¡Œã—ã¾ã™:

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col,
when, coalesce, max, lit

\# Read the existing data from the Delta table

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

\# Find the maximum CustomerID or use 0 if the table is empty

MAXCustomerID =
dfdimCustomer_temp.select(coalesce(max(col("CustomerID")),
lit(0)).alias("MAXCustomerID")).first()\[0\]

\# Assume dfdimCustomer_silver is your source DataFrame with new data

\# Here, we select only the new customers by doing a left anti join

dfdimCustomer_gold = dfdimCustomer_silver.join(

Â  Â  dfdimCustomer_temp,

Â  Â  (dfdimCustomer_silver.CustomerName ==
dfdimCustomer_temp.CustomerName) &

Â  Â  (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email),

Â  Â  "left_anti"

)

\# Add the CustomerID column with unique values starting from
MAXCustomerID + 1

dfdimCustomer_gold = dfdimCustomer_gold.withColumn(

Â  Â  "CustomerID",

Â  Â  monotonically_increasing_id() + MAXCustomerID + 1

)

\# Display the first 10 rows of the dataframe to preview your data

dfdimCustomer_gold.show(10)

![](./media/image48.png)

![A screenshot of a computer Description automatically
generated](./media/image49.png)

4.  æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒå…¥ã£ã¦ãã‚‹ã¨é¡§å®¢ãƒ†ãƒ¼ãƒ–ãƒ«ãŒæœ€æ–°ã®çŠ¶æ…‹ã«ä¿ãŸã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚**æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯**ã«æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è²¼ã‚Šä»˜ã‘ã¦å®Ÿè¡Œã—ã¾ã™ã€‚:

> CodeCopy

from delta.tables import DeltaTable

\# Define the Delta table path

deltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')

\# Use dfUpdates to refer to the DataFrame with new or updated records

dfUpdates = dfdimCustomer_gold

\# Perform the merge operation to update or insert new records

deltaTable.alias('silver') \\

Â  .merge(

Â  Â  dfUpdates.alias('updates'),

Â  Â  'silver.CustomerName = updates.CustomerName AND silver.Email =
updates.Email'

Â  ) \\

Â  .whenMatchedUpdate(set =

Â  Â  {

Â  Â  Â  "CustomerName": "updates.CustomerName",

Â  Â  Â  "Email": "updates.Email",

Â  Â  Â  "First": "updates.First",

Â  Â  Â  "Last": "updates.Last",

Â  Â  Â  "CustomerID": "updates.CustomerID"

Â  Â  }

Â  ) \\

Â  .whenNotMatchedInsert(values =

Â  Â  {

Â  Â  Â  "CustomerName": "updates.CustomerName",

Â  Â  Â  "Email": "updates.Email",

Â  Â  Â  "First": "updates.First",

Â  Â  Â  "Last": "updates.Last",

Â  Â  Â  "CustomerID": "updates.CustomerID"

Â  Â  }

Â  ) \\

Â  .execute()

![](./media/image50.png)

![A screenshot of a computer Description automatically
generated](./media/image51.png)

5.  æ¬¡ã«ã€**ã“ã‚Œã‚‰ã®æ‰‹é †ã‚’ç¹°ã‚Šè¿”ã—ã¦ã€å•†å“ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¾ã™**ã€‚æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã«ã€æ¬¡ã®ã‚‚ã®ã‚’è²¼ã‚Šä»˜ã‘ã¦å®Ÿè¡Œã—ã¾ã™:

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.dimproduct_gold") \\
>
> .addColumn("ItemName", StringType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("ItemInfo", StringType()) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image52.png)

![A screenshot of a computer Description automatically
generated](./media/image53.png)

6.  **product_silver**ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«**åˆ¥ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ã—ã¾ã™ã€‚**.

> CodeCopy
>
> from pyspark.sql.functions import col, split, lit
>
> \# Create product_silver dataframe
>
> dfdimProduct_silver =
> df.dropDuplicates(\["Item"\]).select(col("Item")) \\
>
> .withColumn("ItemName",split(col("Item"), ", ").getItem(0)) \\
>
> .withColumn("ItemInfo",when((split(col("Item"), ",
> ").getItem(1).isNull() | (split(col("Item"), ",
> ").getItem(1)=="")),lit("")).otherwise(split(col("Item"), ",
> ").getItem(1)))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimProduct_silver.head(10))

![A screenshot of a computer Description automatically
generated](./media/image54.png)

![A screenshot of a computer Description automatically
generated](./media/image55.png)

7.  æ¬¡ã«ã€dimProduct_goldãƒ†ãƒ¼ãƒ–ãƒ«ã®IDã‚’ä½œæˆã—ã¾ã™ã€‚æ¬¡ã®æ§‹æ–‡ã‚’æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã«è¿½åŠ ã—ã¦å®Ÿè¡Œã—ã¾ã™:

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col, lit,
max, coalesce

\#dfdimProduct_temp = dfdimProduct_silver

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

MAXProductID =
dfdimProduct_temp.select(coalesce(max(col("ItemID")),lit(0)).alias("MAXItemID")).first()\[0\]

dfdimProduct_gold =
dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName
== dfdimProduct_temp.ItemName) & (dfdimProduct_silver.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left_anti")

dfdimProduct_gold =
dfdimProduct_gold.withColumn("ItemID",monotonically_increasing_id() +
MAXProductID + 1)

\# Display the first 10 rows of the dataframe to preview your data

display(dfdimProduct_gold.head(10))

![A screenshot of a computer Description automatically
generated](./media/image56.png)

ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã®ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦æ¬¡ã«ä½¿ç”¨å¯èƒ½ãªè£½å“ ID
ãŒè¨ˆç®—ã•ã‚Œã€ã“ã‚Œã‚‰ã®æ–°ã—ã„ ID
ãŒè£½å“ã«å‰²ã‚Šå½“ã¦ã€æ›´æ–°ã•ã‚ŒãŸè£½å“æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image57.png)

8.  ä»–ã®ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã§è¡Œã£ãŸã®ã¨åŒæ§˜ã«ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒå…¥ã£ã¦ãã‚‹ãŸã³ã«è£½å“ãƒ†ãƒ¼ãƒ–ãƒ«ãŒæœ€æ–°ã®çŠ¶æ…‹ã«ä¿ãŸã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æ–°ã—ã„ã‚³ãƒ¼ãƒ‰
    ãƒ–ãƒ­ãƒƒã‚¯ã«æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è²¼ã‚Šä»˜ã‘ã¦å®Ÿè¡Œã—ã¾ã™ï¼š

CodeCopy

from delta.tables import \*

deltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')

dfUpdates = dfdimProduct_gold

deltaTable.alias('silver') \\

.merge(

dfUpdates.alias('updates'),

'silver.ItemName = updates.ItemName AND silver.ItemInfo =
updates.ItemInfo'

) \\

.whenMatchedUpdate(set =

{

}

) \\

.whenNotMatchedInsert(values =

{

"ItemName": "updates.ItemName",

"ItemInfo": "updates.ItemInfo",

"ItemID": "updates.ItemID"

}

) \\

.execute()

![A screenshot of a computer Description automatically
generated](./media/image58.png)

![A screenshot of a computer Description automatically
generated](./media/image59.png)

**ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãŒæ§‹ç¯‰ã•ã‚ŒãŸã®ã§ã€æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ãƒ•ã‚¡ã‚¯ãƒˆ
ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã™ã€‚**

1.  **æ–°ã—ã„ã‚³ãƒ¼ãƒ‰
    ãƒ–ãƒ­ãƒƒã‚¯ã«**ã€æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è²¼ã‚Šä»˜ã‘ã¦å®Ÿè¡Œã—ã€**ãƒ•ã‚¡ã‚¯ãƒˆ
    ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¾ã™**ã€‚

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.factsales_gold") \\
>
> .addColumn("CustomerID", LongType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("OrderDate", DateType()) \\
>
> .addColumn("Quantity", IntegerType()) \\
>
> .addColumn("UnitPrice", FloatType()) \\
>
> .addColumn("Tax", FloatType()) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image60.png)

![A screenshot of a computer Description automatically
generated](./media/image61.png)

9.  **æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ ãƒ–ãƒ­ãƒƒã‚¯**ã«æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è²¼ã‚Šä»˜ã‘ã¦å®Ÿè¡Œã—ã€é¡§å®¢
    IDã€å•†å“
    IDã€æ³¨æ–‡æ—¥ã€æ•°é‡ã€å˜ä¾¡ã€ç¨é‡‘ãªã©ã®é¡§å®¢æƒ…å ±ã¨å•†å“æƒ…å ±ã‚’çµåˆã™ã‚‹**æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ **ã‚’ä½œæˆã—ã¾ã™:

CodeCopy

from pyspark.sql import SparkSession

from pyspark.sql.functions import split, col, when, lit

from pyspark.sql.types import StructType, StructField, StringType,
IntegerType, DateType, FloatType, BooleanType, TimestampType

\# Initialize Spark session

spark = SparkSession.builder \\

Â  Â  .appName("DeltaTableUpsert") \\

Â  Â  .config("spark.sql.extensions",
"io.delta.sql.DeltaSparkSessionExtension") \\

Â  Â  .config("spark.sql.catalog.spark_catalog",
"org.apache.spark.sql.delta.catalog.DeltaCatalog") \\

Â  Â  .getOrCreate()

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

Â  Â  StructField("SalesOrderNumber", StringType(), True),

Â  Â  StructField("SalesOrderLineNumber", IntegerType(), True),

Â  Â  StructField("OrderDate", DateType(), True),

Â  Â  StructField("CustomerName", StringType(), True),

Â  Â  StructField("Email", StringType(), True),

Â  Â  StructField("Item", StringType(), True),

Â  Â  StructField("Quantity", IntegerType(), True),

Â  Â  StructField("UnitPrice", FloatType(), True),

Â  Â  StructField("Tax", FloatType(), True),

Â  Â  StructField("FileName", StringType(), True),

Â  Â  StructField("IsFlagged", BooleanType(), True),

Â  Â  StructField("CreatedTS", TimestampType(), True),

Â  Â  StructField("ModifiedTS", TimestampType(), True)

\])

\# Define the path to the Delta table (ensure this path is correct)

delta_table_path =
"abfss://\<container\>@\<storage-account\>.dfs.core.windows.net/path/to/wwilakehouse/sales_silver"

\# Create a DataFrame with the defined schema

empty_df = spark.createDataFrame(\[\], silver_table_schema)

\# Register the Delta table in the Metastore

spark.sql(f"""

Â  Â  CREATE TABLE IF NOT EXISTS wwilakehouse.sales_silver

Â  Â  USING DELTA

Â  Â  LOCATION '{delta_table_path}'

""")

\# Load data into DataFrame

df = spark.read.table("wwilakehouse.sales_silver")

\# Perform transformations on df

df = df.withColumn("ItemName", split(col("Item"), ", ").getItem(0)) \\

Â  Â  .withColumn("ItemInfo", when(

Â  Â  Â  Â  (split(col("Item"), ", ").getItem(1).isNull()) |
(split(col("Item"), ", ").getItem(1) == ""),

Â  Â  Â  Â  lit("")

Â  Â  ).otherwise(split(col("Item"), ", ").getItem(1)))

\# Load additional DataFrames for joins

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

\# Create Sales_gold dataframe

dffactSales_gold = df.alias("df1").join(dfdimCustomer_temp.alias("df2"),
(df.CustomerName == dfdimCustomer_temp.CustomerName) & (df.Email ==
dfdimCustomer_temp.Email), "left") \\

Â  Â  .join(dfdimProduct_temp.alias("df3"), (df.ItemName ==
dfdimProduct_temp.ItemName) & (df.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left") \\

Â  Â  .select(

Â  Â  Â  Â  col("df2.CustomerID"),

Â  Â  Â  Â  col("df3.ItemID"),

Â  Â  Â  Â  col("df1.OrderDate"),

Â  Â  Â  Â  col("df1.Quantity"),

Â  Â  Â  Â  col("df1.UnitPrice"),

Â  Â  Â  Â  col("df1.Tax")

Â  Â  ).orderBy(col("df1.OrderDate"), col("df2.CustomerID"),
col("df3.ItemID"))

\# Show the result

dffactSales_gold.show()

![A screenshot of a computer Description automatically
generated](./media/image62.png)

![A screenshot of a computer Description automatically
generated](./media/image63.png)

![A screenshot of a computer Description automatically
generated](./media/image64.png)

1.  æ¬¡ã«ã€**æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯**ã§æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ã€å£²ä¸Šãƒ‡ãƒ¼ã‚¿ãŒæœ€æ–°ã®çŠ¶æ…‹ã«ä¿ãŸã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™:

> CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')
>
> dfUpdates = dffactSales_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate AND silver.CustomerID =
> updates.CustomerID AND silver.ItemID = updates.ItemID'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "CustomerID": "updates.CustomerID",
>
> "ItemID": "updates.ItemID",
>
> "OrderDate": "updates.OrderDate",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax"
>
> }
>
> ) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image65.png)

ã“ã“ã§ã¯ã€Delta
Lakeã®ãƒãƒ¼ã‚¸æ“ä½œã‚’ä½¿ç”¨ã—ã¦ã€factsales_goldãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ–°ã—ã„å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆdffactSales_goldï¼‰ã¨åŒæœŸãƒ»æ›´æ–°ã—ã¦ã„ã¾ã™ã€‚ã“ã®æ“ä½œã§ã¯ã€æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆsilverãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrameã‚’æ›´æ–°ï¼‰ã®æ³¨æ–‡æ—¥ã€é¡§å®¢IDã€å•†å“IDã‚’æ¯”è¼ƒã—ã€ä¸€è‡´ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°ã—ã€å¿…è¦ã«å¿œã˜ã¦æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŒ¿å…¥ã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image66.png)

ã“ã‚Œã§ã€ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚„åˆ†æã«ä½¿ç”¨ã§ãã‚‹ã€ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚Œãƒ¢ãƒ‡ãƒ«åŒ–ã•ã‚ŒãŸ**ã‚´ãƒ¼ãƒ«ãƒ‰**ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå®Œæˆã—ã¾ã—ãŸã€‚

# æ‰‹é † 4: Azure Databricks ã¨ Azure Data Lake Storage (ADLS) Gen 2 é–“ã®æ¥ç¶šã‚’ç¢ºç«‹ã™ã‚‹

ãã‚Œã§ã¯ã€Azure Databricksã‚’ä½¿ç”¨ã—ã¦Azure Data Lake Storage (ADLS)
Gen2ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§Deltaãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚æ¬¡ã«ã€ADLSã®Deltaãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®OneLakeã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚’ä½œæˆã—ã€Power
BIã‚’ä½¿ç”¨ã—ã¦ADLSã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆçµŒç”±ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¾ã™ã€‚

**ã‚¿ã‚¹ã‚¯ 0: Azure ãƒ‘ã‚¹ã‚’åˆ©ç”¨ã—ã¦ Azure ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹ã«ã™ã‚‹**

## 

1.  æ¬¡ã®ãƒªãƒ³ã‚¯ !!https://www.microsoftazurepass.com/!!
    ã«ç§»å‹•ã—ã€\[Start\] ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚

![](./media/image67.png)

2.  Microsoft ã‚µã‚¤ãƒ³ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã§ãƒ†ãƒŠãƒ³ãƒˆ ID ã‚’å…¥åŠ›ã—ã€\[Next\]
    ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚

![](./media/image68.png)

1.  æ¬¡ã®ãƒšãƒ¼ã‚¸ã§ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã€\[**ã‚µã‚¤ãƒ³ã‚¤ãƒ³**\]ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚

![](./media/image69.png)

![A screenshot of a computer error Description automatically
generated](./media/image70.png)

3.  ãƒ­ã‚°ã‚¤ãƒ³ã—ãŸã‚‰ã€Microsoft Azure ãƒšãƒ¼ã‚¸ã§ \[Microsoft
    ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ç¢ºèª\] ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚

![](./media/image71.png)

4.  æ¬¡ã®ãƒšãƒ¼ã‚¸ã§ã€ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰ã€ã‚­ãƒ£ãƒ—ãƒãƒ£æ–‡å­—ã‚’å…¥åŠ›ã—ã€\[Submit\]ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™
    **ã€‚**

![A screenshot of a computer Description automatically
generated](./media/image72.png)

![A screenshot of a computer error Description automatically
generated](./media/image73.png)

5.  \[Your
    profile\]ãƒšãƒ¼ã‚¸ã§ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã®è©³ç´°ã‚’å…¥åŠ›ã—ã€\[Sign-up\]ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™
    **ã€‚**

6.  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€å¤šè¦ç´ èªè¨¼ã«ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—ã—ã€æ¬¡ã®ãƒªãƒ³ã‚¯ã«ç§»å‹•ã—ã¦
    Azure ãƒãƒ¼ã‚¿ãƒ«ã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ !! <https://portal.azure.com/#home>!!

![](./media/image74.png)

7.  æ¤œç´¢ãƒãƒ¼ã«ã€Œã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã€ã¨å…¥åŠ›ã—ã€\[Service\]ã®ä¸‹ã«ã‚ã‚‹\[Subscription\]ã‚¢ã‚¤ã‚³ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™
    **.**

![A screenshot of a computer Description automatically
generated](./media/image75.png)

8.  Azure ãƒ‘ã‚¹ã®å¼•ãæ›ãˆã«æˆåŠŸã™ã‚‹ã¨ã€ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ ID
    ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚

![](./media/image76.png)

## **ã‚¿ã‚¹ã‚¯ 1: Azure Data Storage ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹**

1.  Azure ã®è³‡æ ¼æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã€Azure portal ã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³ã—ã¾ã™ã€‚

2.  ãƒ›ãƒ¼ãƒ  ãƒšãƒ¼ã‚¸ã§ã€å·¦å´ã®ãƒãƒ¼ã‚¿ãƒ« ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ \[**Storage
    accounts**\] ã‚’é¸æŠã—ã¦ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
    ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ãƒãƒ¼ã‚¿ãƒ«
    ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã¯ã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼
    ãƒœã‚¿ãƒ³ã‚’é¸æŠã—ã¦ã‚ªãƒ³ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™.

![A screenshot of a computer Description automatically
generated](./media/image77.png)

3.  \[**Storage accounts**\] ãƒšãƒ¼ã‚¸ã§ã€ \[**Create\] ã‚’é¸æŠã—ã¾ã™**.

![A screenshot of a computer Description automatically
generated](./media/image78.png)

4.  \[åŸºæœ¬\] ã‚¿ãƒ–ã§ãƒªã‚½ãƒ¼ã‚¹ ã‚°ãƒ«ãƒ¼ãƒ—ã‚’é¸æŠã—ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
    ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å¿…é ˆæƒ…å ±ã‚’å…¥åŠ›ã—ã¾ã™ï¼š

[TABLE]

ä»–ã®è¨­å®šã¯ãã®ã¾ã¾ã«ã—ã¦ã€\[**Review+create**\]
ã‚’é¸æŠã—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å—ã‘å…¥ã‚Œã¦ã€ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æ¤œè¨¼ã¨ä½œæˆã«é€²ã¿ã¾ã™ã€‚

æ³¨: ãƒªã‚½ãƒ¼ã‚¹ ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ã¾ã ä½œæˆã—ã¦ã„ãªã„å ´åˆã¯ã€\[**Create new**\]
ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æ–°ã—ã„ãƒªã‚½ãƒ¼ã‚¹ã‚’ä½œæˆã§ãã¾ã™.

![](./media/image79.png)

![](./media/image80.png)

![A screenshot of a computer Description automatically
generated](./media/image81.png)

![A screenshot of a computer Description automatically
generated](./media/image82.png)

![A screenshot of a computer Description automatically
generated](./media/image83.png)

ã€ŒReview+createã€ã‚¿ãƒ–ã«ç§»å‹•ã™ã‚‹ã¨ã€Azure ã¯é¸æŠã—ãŸã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
ã‚¢ã‚«ã‚¦ãƒ³ãƒˆè¨­å®šã®æ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚æ¤œè¨¼ã«åˆæ ¼ã™ã‚‹ã¨ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ä½œæˆã«é€²ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚æ¤œè¨¼ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€ãƒãƒ¼ã‚¿ãƒ«ã«å¤‰æ›´ãŒå¿…è¦ãªè¨­å®šãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚.

![A screenshot of a computer Description automatically
generated](./media/image84.png)

![A screenshot of a computer Description automatically
generated](./media/image85.png)

ã“ã‚Œã§ã€Azure ãƒ‡ãƒ¼ã‚¿ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚

5.  ãƒšãƒ¼ã‚¸ä¸Šéƒ¨ã®æ¤œç´¢ãƒãƒ¼ã§æ¤œç´¢ã—ã¦ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒšãƒ¼ã‚¸ã«ç§»å‹•ã—ã€æ–°ã—ãä½œæˆã—ãŸã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’é¸æŠã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image86.png)

6.  ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ ãƒšãƒ¼ã‚¸ã§ã€å·¦å´ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ ãƒšã‚¤ãƒ³ã®
    \[**Data storage**\] ã®ä¸‹ã® \[**Containers**\]
    ã«ç§»å‹•ã—ã€!!medalion1!!
    ã¨ã„ã†åå‰ã®æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒŠãƒ¼ã‚’ä½œæˆã—ã¦ã€\[**Create**\]
    ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚Â 

Â 

![A screenshot of a computer Description automatically
generated](./media/image87.png)

7.  **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒšãƒ¼ã‚¸ã«æˆ»ã‚Šã€å·¦å´ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€Œ**ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**ã€ã‚’é¸æŠã—ã¾ã™ã€‚ä¸‹ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ãƒ—ãƒ©ã‚¤ãƒãƒªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**Primary
    endpoint**ã®URLã‚’ã‚³ãƒ”ãƒ¼ã—ã€ãƒ¡ãƒ¢å¸³ã«è²¼ã‚Šä»˜ã‘ã¾ã™ã€‚ã“ã‚Œã¯ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹éš›ã«å½¹ç«‹ã¡ã¾ã™ã€‚

![](./media/image88.png)

8.  åŒæ§˜ã«ã€åŒã˜ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ãƒãƒ«ã®**ã‚¢ã‚¯ã‚»ã‚¹ã‚­ãƒ¼**ã«ç§»å‹•ã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image89.png)

## **ã‚¿ã‚¹ã‚¯ 2: Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã€ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚’ä½œæˆã—ã€Lakehouse å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¾ã™**

1.  Lakehouseã§ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¨ªã«ã‚ã‚‹**(...)** ã‚’é¸æŠã—ã€ \[**New
    shortcut\]** ã‚’é¸æŠã—ã¾ã™ã€‚

![](./media/image90.png)

2.  **New shortcutç”»é¢ã«Azure Data Lake Storage Gen2**ã‚¿ã‚¤ãƒ«ã‚’é¸æŠã™ã‚‹ã€‚

![Screenshot of the tile options in the New shortcut
screen.](./media/image91.png)

3.  ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã®æ¥ç¶šã®è©³ç´°ã‚’æŒ‡å®šã™ã‚‹:

[TABLE]

4.  **Next**ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹

![A screenshot of a computer Description automatically
generated](./media/image92.png)

5.  ã“ã‚Œã«ã‚ˆã‚Šã€Azure ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
    ã‚³ãƒ³ãƒ†ãƒŠãƒ¼ã¨ã®ãƒªãƒ³ã‚¯ãŒç¢ºç«‹ã•ã‚Œã¾ã™ã€‚ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’é¸æŠã—ã€\[**æ¬¡ã¸**\]
    ãƒœã‚¿ãƒ³ã‚’é¸æŠã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image93.png)

![A screenshot of a computer Description automatically
generated](./media/image94.png)![A screenshot of a computer Description
automatically generated](./media/image95.png)

6.  ã‚¦ã‚£ã‚¶ãƒ¼ãƒ‰ãŒèµ·å‹•ã—ãŸã‚‰ã€ã€Œ**Files**ã€ã‚’é¸æŠã—ã€**ãƒ–ãƒ­ãƒ³ã‚º**ãƒ•ã‚¡ã‚¤ãƒ«ã®ã€Œâ€¦ã€ã‚’é¸æŠã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image96.png)

7.  **Load to tablesã¨new table**ã‚’é¸æŠã™ã‚‹ã€‚

![](./media/image97.png)

8.  ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã®åå‰**ã‚’ bronze_01** ã¨ã—ã¦æŒ‡å®š
    ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¨®é¡ã¨ã—ã¦ **parquet** ã‚’é¸æŠã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image98.png)

![A screenshot of a computer Description automatically
generated](./media/image99.png)

![A screenshot of a computer Description automatically
generated](./media/image100.png)

![A screenshot of a computer Description automatically
generated](./media/image101.png)

9.  ãƒ•ã‚¡ã‚¤ãƒ«**bronze_01**ãŒãƒ•ã‚¡ã‚¤ãƒ«å†…ã«è¡¨ç¤ºã•ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

![A screenshot of a computer Description automatically
generated](./media/image102.png)

10. æ¬¡ã«ã€**ãƒ–ãƒ­ãƒ³ã‚º**ãƒ•ã‚¡ã‚¤ãƒ«ã®ã€Œâ€¦ã€ã‚’é¸æŠã—ã¾ã™ã€‚ã€Œ**Load to
    tables**ã€ã¨ã€Œ**Existing table**ã€ã‚’é¸æŠã—ã¾ã™

![A screenshot of a computer Description automatically
generated](./media/image103.png)

11. æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’**dimcustomer_gold**ã¨ã—ã¦æä¾›ã™ã‚‹ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¨®é¡ã¯**parquet**ã¨é¸æŠã—ã€**load**ã‚’é¸æŠã—ã¾ã™**.**

![A screenshot of a computer Description automatically
generated](./media/image104.png)

![A screenshot of a computer Description automatically
generated](./media/image105.png)

## **ã‚¿ã‚¹ã‚¯ 3: ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã‚´ãƒ¼ãƒ«ãƒ‰ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹**

ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã§ã¯ã€ã‚´ãƒ¼ãƒ«ãƒ‰ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã€ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹å†…ã§ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€ãƒ¬ãƒãƒ¼ãƒˆç”¨ã®ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—ã¨ãƒ¡ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆã§ãã¾ã™ã€‚

*Lakehouseã‚’ä½œæˆã™ã‚‹ã¨è‡ªå‹•çš„ã«ä½œæˆã•ã‚Œã‚‹**ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«**ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚Lakehouse
explorerã‹ã‚‰ã€ã“ã®ãƒ©ãƒœã§ä½œæˆã—ãŸã‚´ãƒ¼ãƒ«ãƒ‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å«ã‚€æ–°ã—ã„ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ã€ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚*

1.  ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã§ã€**wwilakehouse** Lakehouseã«ç§»å‹•ã—ã¾ã™ã€‚æ¬¡ã«
    **ã€**Lakehouseã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ãƒ¼ãƒ“ãƒ¥ãƒ¼ã®ãƒªãƒœãƒ³ã‹ã‚‰\[**New semantic
    model**\] ã‚’é¸æŠã—ã¾ã™.

![A screenshot of a computer Description automatically
generated](./media/image106.png)

2.  ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã§ã€ **æ–°**ã—ã„ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ãƒ¢ãƒ‡ãƒ«ã«
    **DatabricksTutorial** ã¨ã„ã†åå‰ã‚’å‰²ã‚Šå½“ã¦ã€ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’
    **Fabric Lakehouse Tutorial-29** ã¨ã—ã¦é¸æŠã—ã¾ã™ã€‚

![](./media/image107.png)

3.  æ¬¡ã«ã€ä¸‹ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯
    ãƒ¢ãƒ‡ãƒ«ã«å«ã‚ã‚‹ã™ã¹ã¦ã‚’é¸æŠã—ã€ \[Confirm\] ã‚’é¸æŠã—ã¾ã™ .

ã“ã‚Œã«ã‚ˆã‚Šã€Fabric ã§ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯
ãƒ¢ãƒ‡ãƒ«ãŒé–‹ãã€æ¬¡ã«ç¤ºã™ã‚ˆã†ã«ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—ã¨ãƒ¡ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆã§ãã¾ã™:

![A screenshot of a computer Description automatically
generated](./media/image108.png)

ã“ã“ã‹ã‚‰ã€ã‚ãªãŸã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ãƒ ã®ä»–ã®ãƒ¡ãƒ³ãƒãƒ¼ã¯ã€Lakehouseå†…ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒ¬ãƒãƒ¼ãƒˆã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆã§ãã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¬ãƒãƒ¼ãƒˆã¯Lakehouseã®ã‚´ãƒ¼ãƒ«ãƒ‰ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ç›´æ¥æ¥ç¶šã•ã‚Œã‚‹ãŸã‚ã€å¸¸ã«æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ãŒåæ˜ ã•ã‚Œã¾ã™.

# æ‰‹é † 5: Azure Databricks ã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã®å–ã‚Šè¾¼ã¿ã¨åˆ†æ

1.  Power BI ã‚µãƒ¼ãƒ“ã‚¹ã§Lakehouseã«ç§»å‹•ã—ã€ \[**Get data**\] ã‚’é¸æŠã—ã€
    \[**New data pipeline\]** ã‚’é¸æŠã—ã¾ã™ã€‚

![Screenshot showing how to navigate to new data pipeline option from
within the UI.](./media/image109.png)

**New Pipeline**ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã€æ–°ã—ã„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åå‰ã€Œ**Ingest Data
pipeline
01**ã€ã‚’å…¥åŠ›ã—ã€ã€Œ**Create**ã€ã‚’é¸æŠã—ã¾ã™ã€‚![](./media/image110.png)

1.  ã“ã®æ‰‹é †ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ ã‚½ãƒ¼ã‚¹ã¨ã—ã¦ **NYC Taxi - Green** ã‚µãƒ³ãƒ—ãƒ«
    ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image111.png)

2.  ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”»é¢ã§ã€\[**Next**\] ã‚’é¸æŠã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image112.png)

3.  \[**Data destination**\] ã§ã€OneLake Delta ãƒ†ãƒ¼ãƒ–ãƒ«
    ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«ä½¿ç”¨ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®åå‰ã‚’é¸æŠã—ã¾ã™ã€‚æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¸æŠã™ã‚‹ã‹ã€æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã§ãã¾ã™ã€‚ã“ã®ãƒ©ãƒœã§ã¯ã€
    \[**Load to new table\]** ã‚’é¸æŠã—ã€ \[**Next**\] ã‚’é¸æŠã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image113.png)

4.  On theÂ **Review + Saveç”»é¢ã§Start data transfer
    immediatelyã‚’é¸æŠã—ã¦ã‹ã‚‰Save + Run**ã‚’é¸æŠã™ã‚‹ã€‚

![Screenshot showing how to enter table name.](./media/image114.png)

![A screenshot of a computer Description automatically
generated](./media/image115.png)

5.  ã‚¸ãƒ§ãƒ–ãŒå®Œäº†ã—ãŸã‚‰ã€Lakehouseã«ç§»å‹•ã—ã€/Tables
    ã®ä¸‹ã«ãƒªã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ«ã‚¿ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image116.png)

6.  Azure Blob Filesystem (ABFS) ãƒ‘ã‚¹ã‚’ãƒ‡ãƒ«ã‚¿
    ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚³ãƒ”ãƒ¼ã™ã‚‹ãŸã‚ã«ã€ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ãƒ¼
    ãƒ“ãƒ¥ãƒ¼ã§ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’å³ã‚¯ãƒªãƒƒã‚¯ã—ã€ \[**ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£**\] ã‚’é¸æŠã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image117.png)

7.  Azure Databricks ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’é–‹ãã€ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

olsPath = "**abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample**"

df=spark.read.format('delta').option("inferSchema","true").load(olsPath)

df.show(5)

*æ³¨:å¤ªå­—ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼ã—ãŸã‚‚ã®ã«ç½®ãæ›ãˆã‚‹ã€‚*

![](./media/image118.png)

8.  ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å€¤ã‚’å¤‰æ›´ã—ã¦ Delta ãƒ†ãƒ¼ãƒ–ãƒ« ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹.

%sql

update delta.\`abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample\` set vendorID = 99999 where vendorID
= 1;

*æ³¨:å¤ªå­—ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼ã—ãŸã‚‚ã®ã«ç½®ãæ›ãˆã¾ã™ã€‚*

![A screenshot of a computer Description automatically
generated](./media/image119.png)

# æ‰‹é † 6: ãƒªã‚½ãƒ¼ã‚¹ã®å‰Šé™¤

ã“ã®æ‰‹é †ã§ã¯ã€Microsoft Fabric Lakehouseã§ãƒ¡ãƒ€ãƒªã‚ªãƒ³
ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’å­¦ç¿’ã—ã¾ã—ãŸã€‚

Lakehouseã®æ¢ç´¢ãŒçµ‚äº†ã—ãŸã‚‰ã€ã“ã®æ‰‹é †ç”¨ã«ä½œæˆã—ãŸãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ã§ãã¾ã™ã€‚

1.  å·¦å´ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ (**Fabric Lakehouse
    Tutorial-29)** ã‚’é¸æŠã—ã¾ã™ã€‚ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹é …ç›®ãƒ“ãƒ¥ãƒ¼ãŒé–‹ãã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image120.png)

2.  ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹åã®ä¸‹ã®ã€Œ...ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã€ã€Œ**Workspace
    settings**ã€ã‚’é¸æŠã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image121.png)

3.  ä¸€ç•ªä¸‹ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€ \[ **Remove this workspace\]**
    ã‚’é¸æŠã—ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image122.png)

4.  ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã™ã‚‹è­¦å‘Šã§**\[Delete**\]ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚

![A white background with black text Description automatically
generated](./media/image123.png)

5.  ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãŒå‰Šé™¤ã•ã‚ŒãŸã¨ã„ã†é€šçŸ¥ã‚’å¾…ã£ã¦ã‹ã‚‰ã€æ¬¡ã®ãƒ©ãƒœã«é€²ã¿ã¾ã™ã€‚

![A screenshot of a computer Description automatically
generated](./media/image124.png)

**è¦ç´„**:

ã“ã®ãƒ©ãƒœã§ã¯ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦Microsoft
FabricLakehouseã§ãƒ¡ãƒ€ãƒªã‚ªãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’å­¦ç¿’ã—ã¾ã™ã€‚ä¸»ãªæ‰‹é †ã¨ã—ã¦ã¯ã€ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®è¨­å®šã€Lakehouseã®æ§‹ç¯‰ã€åˆæœŸå–ã‚Šè¾¼ã¿ã®ãŸã‚ã®ãƒ–ãƒ­ãƒ³ã‚ºãƒ¬ã‚¤ãƒ¤ãƒ¼ã¸ã®ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€æ§‹é€ åŒ–å‡¦ç†ã®ãŸã‚ã®ã‚·ãƒ«ãƒãƒ¼Deltaãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®å¤‰æ›ã€é«˜åº¦ãªåˆ†æã®ãŸã‚ã®ã‚´ãƒ¼ãƒ«ãƒ‰Deltaãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®å¤‰æ›ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®æ¢ç´¢ã€ãã—ã¦æ´å¯Ÿã«å¯Œã‚“ã åˆ†æã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—ã®ä½œæˆãªã©ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚

## 
