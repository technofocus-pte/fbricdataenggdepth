# Caso de uso 04: AnÃ¡lisis moderno a escala en la nube con Azure Databricks y Microsoft Fabric

**IntroducciÃ³n**

En este laboratorio, explorarÃ¡ la integraciÃ³n de Azure Databricks con
Microsoft Fabric para crear y gestionar un lakehouse utilizando la
arquitectura Medallion, crear una tabla Delta con la ayuda de su cuenta
de Azure Data Lake Storage (ADLS) Gen2 utilizando Azure Databricks e
Ingestar datos con Azure Databricks. Esta guÃ­a prÃ¡ctica le guiarÃ¡ a
travÃ©s de los pasos necesarios para crear un lakehouse, cargar datos en
Ã©l y explorar las capas de datos estructurados para facilitar el
anÃ¡lisis de datos y la generaciÃ³n de informes eficientes.

La arquitectura Medallion consta de tres capas (o zonas) distintas.

- Bronze: TambiÃ©n conocida como zona bruta, esta primera capa almacena
  los datos de origen en su formato original. Los datos en esta capa son
  tÃ­picamente append-only e inmutables.

- Silver: TambiÃ©n conocida como zona enriquecida, esta capa almacena
  datos procedentes de la capa bronze. Los datos brutos se han limpiado
  y normalizado, y ahora estÃ¡n estructurados en forma de tablas (filas y
  columnas). TambiÃ©n podrÃ­a integrarse con otros datos para proporcionar
  una visiÃ³n empresarial de todas las entidades de negocio, como el
  cliente, el producto y otros.

- Gold: TambiÃ©n conocida como la zona curada, esta capa final almacena
  los datos procedentes de la capa silver. Los datos se refinan para
  satisfacer requisitos empresariales y analÃ­ticos especÃ­ficos
  posteriores. Las tablas suelen ajustarse al diseÃ±o de esquema en
  estrella, que apoya el desarrollo de modelos de datos optimizados para
  el rendimiento y la facilidad de uso.

**Objetivos**:

- Comprender los principios de la arquitectura Medallion dentro de
  Microsoft Fabric Lakehouse.

- Implementar un proceso de gestiÃ³n de datos estructurado utilizando
  capas Medallion (Bronze, Silver, Gold).

- Transformar datos sin procesar en datos validados y enriquecidos para
  anÃ¡lisis e informes avanzados.

- Aprender las mejores prÃ¡cticas para la seguridad de datos, CI/CD, y la
  consulta eficiente de datos.

- Cargar datos en OneLake con el explorador de archivos de OneLake.

- Utilizar un notebook Fabric para leer datos en OneLake y escribir de
  nuevo como una tabla Delta.

- Analizar y transformar datos con Spark utilizando un Fabric notebook.

- Consultar una copia de los datos en OneLake con SQL.

- Crear una tabla Delta en su cuenta Azure Data Lake Storage (ADLS) Gen2
  utilizando Azure Databricks.

- Crear un shortcut de OneLake a una tabla Delta en ADLS.

- Utilizar Power BI para analizar datos a travÃ©s del shortcut ADLS.

- Leer y modificar una tabla Delta en OneLake con Azure Databricks.

# Ejercicio 1: Introducir datos de muestra en Lakehouse

En este ejercicio, usted irÃ¡ a travÃ©s del proceso de creaciÃ³n de un
lakehouse y la carga de datos en Ã©l utilizando Microsoft Fabric.

Tarea:ruta

**Tarea 1: Crear un workspace de Fabric**

En esta tarea, se crea un workspace de Fabric. El workspace contiene
todos los elementos necesarios para este tutorial de lakehouse, que
incluye lakehouse, dataflows, pipelines de Data Factory, los notebooks,
los conjuntos de datos de Power BI y los informes.

1.  Abra su navegador, vaya a la barra de direcciones y escriba o pegue
    la siguiente URL:
    [https://app.fabric.microsoft.com/](https://app.fabric.microsoft.com/,)
    luego presione la tecla **Enter**.

> ![A search engine window with a red box Description automatically
> generated with medium confidence](./media/image1.png)

2.  Vuelva a la ventana de **Power**Â **BI**. En el menÃº de navegaciÃ³n de
    la izquierda de la pÃ¡gina de inicio de Power BI, navegue y haga clic
    enÂ **Workspaces**.

![](./media/image2.png)

3.  En el panel Workspaces, haga clic en elÂ botÃ³nÂ **+**Â **New
    workspace.**

> ![](./media/image3.png)

4.  En el panelÂ **Create** **a workspace**Â que aparece a la derecha,
    introduzca los siguientes datos y haga clic en el botÃ³nÂ **Apply**.

[TABLE]

> ![](./media/image4.png)

![A screenshot of a computer Description automatically
generated](./media/image5.png)

5.  Espere a que finalice la implementaciÃ³n. Tarda de 2 a 3 minutos en
    completarse.

![](./media/image6.png)

## **Tarea 2: Crear un lakehouse**

1.  En la pÃ¡ginaÂ **Power BI Fabric Lakehouse Tutorial-XX**, haga clic en
    el iconoÂ de **Power BIÂ **situado en la parte inferior izquierda y
    seleccioneÂ **Data Engineering**.

> ![](./media/image7.png)

2.  En la pÃ¡gina **Synapse**Â **Data Engineering**Â **Home**,
    seleccioneÂ **Lakehouse**Â para crear un lakehouse.

![](./media/image8.png)

![A screenshot of a computer Description automatically
generated](./media/image9.png)

3.  En el cuadro de diÃ¡logoÂ **New lakehouse**, ingrese **wwilakehouse**
    en el campo **Name**, haga clic en el botÃ³n **Create** button y abra
    el nuevo lakehouse.

> **Nota**: AsegÃºrese de eliminar el espacio antes de **wwilakehouse**.
>
> ![](./media/image10.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image11.png)
>
> ![](./media/image12.png)

4.  VerÃ¡ una notificaciÃ³n que dice **Successfully created SQL
    endpoint**.

> ![](./media/image13.png)

# Ejercicio 2: ImplementaciÃ³n de la arquitectura Medallion utilizando Azure Databricks 

## **Tarea 1: ConfiguraciÃ³n de la capa Bronze**

1.  En la pÃ¡gina **wwilakehouse**, seleccione el icono More junto a los
    archivos (â€¦), y seleccione **New subfolder**

![](./media/image14.png)

2.  En la ventana emergente proporcione el nombre de la carpeta
    comoÂ **bronze**, y seleccione Create.

![A screenshot of a computer Description automatically
generated](./media/image15.png)

3.  Ahora, seleccione el icono More junto a los archivos bronze (...), y
    seleccioneÂ **Upload**Â y luego,Â **upload files**.

![A screenshot of a computer Description automatically
generated](./media/image16.png)

4.  En el panel **upload file**, seleccione el botÃ³n de opciÃ³nÂ **Upload
    file**. Haga clic en el botÃ³n **Browse** y vaya **C:\LabFiles**,
    luego seleccione los archivos de datos de ventas requeridos (2019,
    2020, 2021) y haga clic en el botÃ³n **Open**.

Y a continuaciÃ³n, seleccione **Upload** para cargar los archivos en la
nueva carpeta â€˜bronzeâ€™ de su Lakehouse.

![A screenshot of a computer Description automatically
generated](./media/image17.png)

> ![A screenshot of a computer Description automatically
> generated](./media/image18.png)

5.  Haga clic en carpetaÂ **bronzeÂ **para validar que los archivos se han
    cargado correctamente y los archivos estÃ¡n reflejando.

![A screenshot of a computer Description automatically
generated](./media/image19.png)

# Ejercicio 3: TransformaciÃ³n de datos con Apache Spark y consulta con SQL en arquitectura Medallion

## **Tarea 1: Transformar datos y cargarlos en la tabla Silver Delta** 

En la pÃ¡ginaÂ **wwilakehouse**, navegue y haga clic enÂ **Open
notebook**Â desplegable en la barra de comandos, y luego seleccioneÂ **New
notebook**.

![A screenshot of a computer Description automatically
generated](./media/image20.png)

1.  Seleccione la primera celda (que actualmente es una celda
    deÂ *cÃ³digo*) y, a continuaciÃ³n, en la barra de herramientas dinÃ¡mica
    situada en la parte superior derecha, utilice el
    botÃ³nÂ **Mâ†“**Â paraÂ **convertir la celda en una celda de marcado**.

![A screenshot of a computer Description automatically
generated](./media/image21.png)

2.  Cuando la celda cambia a una celda markdown, el texto que contiene
    se renderiza.

![A screenshot of a computer Description automatically
generated](./media/image22.png)

3.  Utilice el botÃ³n **ğŸ–‰**Â (Editar) para cambiar la celda al modo de
    ediciÃ³n, sustituya todo el texto y, a continuaciÃ³n, modifique el
    markdown como sigue:

CodeCopy

\# Sales order data exploration

Utilice el cÃ³digo de este notebook para explorar los datos de los
pedidos de venta.

![A screenshot of a computer Description automatically
generated](./media/image23.png)

![A screenshot of a computer Description automatically
generated](./media/image24.png)

4.  Haga clic en cualquier parte del notebook fuera de la celda para
    dejar de editarla y ver el markdown renderizado.

![A screenshot of a computer Description automatically
generated](./media/image25.png)

5.  Utilice el icono + Code debajo de la salida de la celda para aÃ±adir
    una nueva celda de cÃ³digo al notebook.

![A screenshot of a computer Description automatically
generated](./media/image26.png)

6.  Ahora, utilice el notebook para cargar los datos de la capa bronze
    en un DataFrame de Spark.

Seleccione la celda existente en el notebook, que contiene algÃºn cÃ³digo
simple comentado. Resalte y elimine estas dos lÃ­neas - no necesitarÃ¡
este cÃ³digo.

*Nota: Los notebooks le permiten ejecutar cÃ³digo en una variedad de
lenguajes, incluyendo Python, Scala y SQL. En este ejercicio, utilizarÃ¡
PySpark y SQL. TambiÃ©n puede aÃ±adir celdas markdown para proporcionar
texto formateado e imÃ¡genes para documentar su cÃ³digo.*

Para ello, ingrese el siguiente cÃ³digo en Ã©l y haga clic en **Run**.

CodeCopy

from pyspark.sql.types import \*

\# Create the schema for the table

orderSchema = StructType(\[

StructField("SalesOrderNumber", StringType()),

StructField("SalesOrderLineNumber", IntegerType()),

StructField("OrderDate", DateType()),

StructField("CustomerName", StringType()),

StructField("Email", StringType()),

StructField("Item", StringType()),

StructField("Quantity", IntegerType()),

StructField("UnitPrice", FloatType()),

StructField("Tax", FloatType())

\])

\# Import all files from bronze folder of lakehouse

df = spark.read.format("csv").option("header",
"true").schema(orderSchema).load("Files/bronze/\*.csv")

\# Display the first 10 rows of the dataframe to preview your data

display(df.head(10))

![](./media/image27.png)

***Nota**: Dado que es la primera vez que ejecuta cÃ³digo Spark en este
notebook, debe iniciarse una sesiÃ³n Spark. Esto significa que la primera
ejecuciÃ³n puede tardar mÃ¡s o menos un minuto en completarse. Las
ejecuciones posteriores serÃ¡n mÃ¡s rÃ¡pidas.*

![A screenshot of a computer Description automatically
generated](./media/image28.png)

7.  El cÃ³digo que ejecutÃ³ cargÃ³ los datos de los archivos CSV de la
    carpetaÂ **bronze**Â en un dataframe de Spark y, a continuaciÃ³n,
    mostrÃ³ las primeras filas del dataframe.

> **Nota**: Puede borrar, ocultar y redimensionar automÃ¡ticamente el
> contenido de la salida de celdas seleccionando el menÃºÂ **...** en la
> parte superior izquierda del panel de salida.

8.  AhoraÂ **aÃ±adirÃ¡ columnas para la validaciÃ³n y limpieza de datos**,
    utilizando un dataframe PySpark para aÃ±adir columnas y actualizar
    los valores de algunas de las columnas existentes. Utilice el
    botÃ³n + paraÂ **aÃ±adir un nuevo bloque de cÃ³digo**Â y aÃ±ada el
    siguiente cÃ³digo a la celda:

> CodeCopy
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> \# Add columns IsFlagged, CreatedTS and ModifiedTS
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \<
> '2019-08-01',True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()).withColumn("ModifiedTS",
> current_timestamp())
>
> \# Update CustomerName to "Unknown" if CustomerName null or empty
>
> df = df.withColumn("CustomerName", when((col("CustomerName").isNull()
> |
> (col("CustomerName")=="")),lit("Unknown")).otherwise(col("CustomerName")))
>
> La primera lÃ­nea del cÃ³digo importa las funciones necesarias de
> PySpark. A continuaciÃ³n, aÃ±ade nuevas columnas al dataframe para poder
> rastrear el nombre del archivo de origen, si el pedido se marcÃ³ como
> anterior al ejercicio de interÃ©s y cuÃ¡ndo se creÃ³ y modificÃ³ la fila..
>
> Por Ãºltimo, estÃ¡ actualizando la columna CustomerName a â€œUnknownâ€ si
> es nula o estÃ¡ vacÃ­a.
>
> A continuaciÃ³n, ejecute la celda para ejecutar el cÃ³digo utilizando el
> botÃ³nÂ **\*\*â–·**Â (*Ejecutar celda*)\*\*.

![A screenshot of a computer Description automatically
generated](./media/image29.png)

![A screenshot of a computer Description automatically
generated](./media/image30.png)

9.  A continuaciÃ³n, definirÃ¡ el esquema de la tablaÂ **sales_silver**Â de
    la base de datos de ventas utilizando el formato Delta Lake. Cree un
    nuevo bloque de cÃ³digo y aÃ±ada el siguiente cÃ³digo a la celda:

> CodeCopy

from pyspark.sql.types import \*

from delta.tables import \*

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

Â  Â  StructField("SalesOrderNumber", StringType(), True),

Â  Â  StructField("SalesOrderLineNumber", IntegerType(), True),

Â  Â  StructField("OrderDate", DateType(), True),

Â  Â  StructField("CustomerName", StringType(), True),

Â  Â  StructField("Email", StringType(), True),

Â  Â  StructField("Item", StringType(), True),

Â  Â  StructField("Quantity", IntegerType(), True),

Â  Â  StructField("UnitPrice", FloatType(), True),

Â  Â  StructField("Tax", FloatType(), True),

Â  Â  StructField("FileName", StringType(), True),

Â  Â  StructField("IsFlagged", BooleanType(), True),

Â  Â  StructField("CreatedTS", TimestampType(), True),

Â  Â  StructField("ModifiedTS", TimestampType(), True)

\])

\# Create or replace the sales_silver table with the defined schema

DeltaTable.createIfNotExists(spark) \\

Â  Â  .tableName("wwilakehouse.sales_silver") \\

Â  Â  .addColumns(silver_table_schema) \\

Â  Â  .execute()

Â  Â 

10. Ejecute laÂ *celda*Â para ejecutar el cÃ³digo utilizando el
    botÃ³nÂ **\*\*â–·**Â (*Ejecutar celda*)\*\*.

11. SeleccioneÂ **...** en la secciÃ³n Tables del panel del explorador de
    lakehouse y seleccioneÂ **Refresh**. Ahora deberÃ­a ver la nueva
    tablaÂ **sales_silver**Â en la lista. ElÂ **â–²**Â (icono del triÃ¡ngulo)
    indica que se trata de una tabla Delta.

> **Nota**: Si no ve la nueva tabla, espere unos segundos y
> seleccioneÂ **RefreshÂ **de nuevo, o actualice toda la pestaÃ±a del
> navegador.
>
> ![A screenshot of a computer Description automatically
> generated](./media/image31.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image32.png)

12. Ahora va a realizar unaÂ **operaciÃ³n upsert**Â en una tabla Delta,
    actualizando los registros existentes basÃ¡ndose en condiciones
    especÃ­ficas e insertando nuevos registros cuando no se encuentre
    ninguna coincidencia. AÃ±ada un nuevo bloque de cÃ³digo y pegue el
    siguiente cÃ³digo:

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> from delta.tables import \*
>
> \# Define the schema for the source data
>
> orderSchema = StructType(\[
>
> StructField("SalesOrderNumber", StringType(), True),
>
> StructField("SalesOrderLineNumber", IntegerType(), True),
>
> StructField("OrderDate", DateType(), True),
>
> StructField("CustomerName", StringType(), True),
>
> StructField("Email", StringType(), True),
>
> StructField("Item", StringType(), True),
>
> StructField("Quantity", IntegerType(), True),
>
> StructField("UnitPrice", FloatType(), True),
>
> StructField("Tax", FloatType(), True)
>
> \])
>
> \# Read data from the bronze folder into a DataFrame
>
> df = spark.read.format("csv").option("header",
> "true").schema(orderSchema).load("Files/bronze/\*.csv")
>
> \# Add additional columns
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \< '2019-08-01',
> True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()) \\
>
> .withColumn("ModifiedTS", current_timestamp()) \\
>
> .withColumn("CustomerName", when((col("CustomerName").isNull()) |
> (col("CustomerName") == ""),
> lit("Unknown")).otherwise(col("CustomerName")))
>
> \# Define the path to the Delta table
>
> deltaTablePath = "Tables/sales_silver"
>
> \# Create a DeltaTable object for the existing Delta table
>
> deltaTable = DeltaTable.forPath(spark, deltaTablePath)
>
> \# Perform the merge (upsert) operation
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> df.alias('updates'),
>
> 'silver.SalesOrderNumber = updates.SalesOrderNumber AND \\
>
> silver.OrderDate = updates.OrderDate AND \\
>
> silver.CustomerName = updates.CustomerName AND \\
>
> silver.Item = updates.Item'
>
> ) \\
>
> .whenMatchedUpdate(set = {
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "Email": "updates.Email",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .whenNotMatchedInsert(values = {
>
> "SalesOrderNumber": "updates.SalesOrderNumber",
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "OrderDate": "updates.OrderDate",
>
> "CustomerName": "updates.CustomerName",
>
> "Email": "updates.Email",
>
> "Item": "updates.Item",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "CreatedTS": "current_timestamp()",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .execute()

13. Ejecute laÂ *celda*Â para ejecutar el cÃ³digo utilizando el
    botÃ³nÂ **\*\*â–·**Â (*Ejecutar celda*)\*\*.

![A screenshot of a computer Description automatically
generated](./media/image33.png)

Esta operaciÃ³n es importante porque le permite actualizar los registros
existentes en la tabla basÃ¡ndose en los valores de columnas especÃ­ficas,
e insertar nuevos registros cuando no se encuentra ninguna coincidencia.
Se trata de un requisito habitual cuando se cargan datos de un sistema
fuente que puede contener actualizaciones de registros existentes y
nuevos.

Ahora tiene datos en su tabla delta silver que estÃ¡n listos para su
posterior transformaciÃ³n y modelizaciÃ³n.

Ha tomado con Ã©xito los datos de su capa bronze, los ha transformado y
los ha cargado en una tabla Delta silver. Ahora utilizarÃ¡ un nuevo
notebook para transformar aÃºn mÃ¡s los datos, modelarlos en un esquema de
estrella y cargarlos en tablas Delta gold.

*Tenga en cuenta que podrÃ­a haber hecho todo esto en un Ãºnico notebook,
pero a efectos de este ejercicio va a utilizar notebooks separados para
demostrar el proceso de transformaciÃ³n de datos de bronze a silver y
luego de silver a gold. Esto puede ayudar a depurar, solucionar
problemas y reutilizar*.

## **Tarea 2: Cargar datos en tablas Gold Delta** 

1.  Volver a la pÃ¡gina de inicio de Fabric Lakehouse Tutorial-29.

> ![A screenshot of a computer Description automatically
> generated](./media/image34.png)

2.  Seleccione **wwilakehouse.**

![A screenshot of a computer Description automatically
generated](./media/image35.png)

3.  En el panel del explorador de lakehouse, deberÃ­a ver la
    tablaÂ **sales_silver**Â en la secciÃ³nÂ **Tables**Â del panel del
    explorador.

![A screenshot of a computer Description automatically
generated](./media/image36.png)

4.  Ahora, cree un nuevo notebook llamadoÂ **Transform data for Gold**.
    Para ello, navegue y haga clic enÂ **Open notebook** en la barra de
    comandos, luego seleccioneÂ **New notebook**.

![A screenshot of a computer Description automatically
generated](./media/image37.png)

5.  En el bloque de cÃ³digo existente, elimine el texto repetitivo
    yÂ **aÃ±ada el siguiente cÃ³digo**Â para cargar los datos en su
    dataframe y empezar a construir su esquema estrella, luego
    ejecÃºtelo:

> CodeCopy

\# Load data to the dataframe as a starting point to create the gold
layer

df = spark.read.table("wwilakehouse.sales_silver")

\# Display the first few rows of the dataframe to verify the data

df.show()

![A screenshot of a computer Description automatically
generated](./media/image38.png)

6.  **A continuaciÃ³n, aÃ±ada un nuevo bloque de cÃ³digo**Â y pegue el
    siguiente cÃ³digo para crear su tabla de dimensiones de fecha y
    ejecÃºtela:

Â from pyspark.sql.types import \*

Â from delta.tables import\*

Â  Â 

Â # Define the schema for the dimdate_gold table

Â DeltaTable.createIfNotExists(spark) \\

Â  Â  Â .tableName("wwilakehouse.dimdate_gold") \\

Â  Â  Â .addColumn("OrderDate", DateType()) \\

Â  Â  Â .addColumn("Day", IntegerType()) \\

Â  Â  Â .addColumn("Month", IntegerType()) \\

Â  Â  Â .addColumn("Year", IntegerType()) \\

Â  Â  Â .addColumn("mmmyyyy", StringType()) \\

Â  Â  Â .addColumn("yyyymm", StringType()) \\

Â  Â  Â .execute()

![A screenshot of a computer Description automatically
generated](./media/image39.png)

**Nota**: Puede ejecutar el comando display(df)Â  en cualquier momento
para comprobar el progreso de su trabajo. En este caso, ejecutarÃ­a
'display(dfdimDate_gold)' para ver el contenido del dataframe
dimDate_gold.

7.  En un nuevo bloque de cÃ³digo,Â **aÃ±ada y ejecute el siguiente
    cÃ³digo**Â para crear un dataframe para su dimensiÃ³n de
    fecha,Â **dimdate_gold**:

> CodeCopy

from pyspark.sql.functions import col, dayofmonth, month, year,
date_format

Â  Â 

Â # Create dataframe for dimDate_gold

Â  Â 

dfdimDate_gold
=df.dropDuplicates(\["OrderDate"\]).select(col("OrderDate"), \\

Â  Â  Â  Â  Â dayofmonth("OrderDate").alias("Day"), \\

Â  Â  Â  Â  Â month("OrderDate").alias("Month"), \\

Â  Â  Â  Â  Â year("OrderDate").alias("Year"), \\

Â  Â  Â  Â  Â date_format(col("OrderDate"), "MMM-yyyy").alias("mmmyyyy"), \\

Â  Â  Â  Â  Â date_format(col("OrderDate"), "yyyyMM").alias("yyyymm"), \\

Â  Â  Â ).orderBy("OrderDate")

Â # Display the first 10 rows of the dataframe to preview your data

display(dfdimDate_gold.head(10))

![A screenshot of a computer Description automatically
generated](./media/image40.png)

![A screenshot of a computer Description automatically
generated](./media/image41.png)

8.  EstÃ¡ separando el cÃ³digo en nuevos bloques de cÃ³digo para que pueda
    entender y observar lo que ocurre en el notebook a medida que
    transforma los datos. En otro nuevo bloque de cÃ³digo,Â **aÃ±ada y
    ejecute el siguiente cÃ³digo**Â para actualizar la dimensiÃ³n fecha a
    medida que entren nuevos datos:

> CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')
>
> dfUpdates = dfdimDate_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "OrderDate": "updates.OrderDate",
>
> "Day": "updates.Day",
>
> "Month": "updates.Month",
>
> "Year": "updates.Year",
>
> "mmmyyyy": "updates.mmmyyyy",
>
> "yyyymm": "yyyymm"
>
> }
>
> ) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image42.png)

> Su dimensiÃ³n de fecha estÃ¡ completamente configurada.

![A screenshot of a computer Description automatically
generated](./media/image43.png)

## **Tarea 3: CreaciÃ³n de la dimensiÃ³n de cliente**

1.  Para construir la tabla de dimensiones de clientes,Â **aÃ±ada un nuevo
    bloque de**Â cÃ³digo , pegue y ejecute el siguiente cÃ³digo:

> CodeCopy

Â from pyspark.sql.types import \*

Â from delta.tables import \*

Â  Â 

Â # Create customer_gold dimension delta table

Â DeltaTable.createIfNotExists(spark) \\

Â  Â  Â .tableName("wwilakehouse.dimcustomer_gold") \\

Â  Â  Â .addColumn("CustomerName", StringType()) \\

Â  Â  Â .addColumn("Email", Â StringType()) \\

Â  Â  Â .addColumn("First", StringType()) \\

Â  Â  Â .addColumn("Last", StringType()) \\

Â  Â  Â .addColumn("CustomerID", LongType()) \\

Â  Â  Â .execute()

![A screenshot of a computer Description automatically
generated](./media/image44.png)

![A screenshot of a computer Description automatically
generated](./media/image45.png)

2.  En un nuevo bloque de cÃ³digo,Â **aÃ±ada y ejecute el siguiente
    cÃ³digo**Â para eliminar los clientes duplicados, seleccionar columnas
    especÃ­ficas y dividir la columna â€œCustomerNameâ€ para crear las
    columnas â€œFirstâ€ y â€œLastâ€ name:

> CodeCopy
>
> from pyspark.sql.functions import col, split
>
> \# Create customer_silver dataframe
>
> dfdimCustomer_silver =
> df.dropDuplicates(\["CustomerName","Email"\]).select(col("CustomerName"),col("Email"))
> \\
>
> .withColumn("First",split(col("CustomerName"), " ").getItem(0)) \\
>
> .withColumn("Last",split(col("CustomerName"), " ").getItem(1))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimCustomer_silver.head(10))

![A screenshot of a computer Description automatically
generated](./media/image46.png)

AquÃ­ ha creado un nuevo dataFrame dfdimCustomer_silver realizando varias
transformaciones como eliminar duplicados, seleccionar columnas
especÃ­ficas y dividir la columna â€œCustomerNameâ€ para crear columnas de
nombre â€œFirstâ€ y â€œLastâ€. El resultado es un dataFrame con los datos de
los clientes limpios y estructurados, incluidas las columnas separadas
â€œFirstâ€œ y â€Last â€œ extraÃ­das de la columna â€CustomerNameâ€.

![A screenshot of a computer Description automatically
generated](./media/image47.png)

3.  A continuaciÃ³n,Â **crearemos la columna ID para nuestros clientes**.
    En un nuevo bloque de cÃ³digo, pegue y ejecute lo siguiente:

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col,
when, coalesce, max, lit

\# Read the existing data from the Delta table

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

\# Find the maximum CustomerID or use 0 if the table is empty

MAXCustomerID =
dfdimCustomer_temp.select(coalesce(max(col("CustomerID")),
lit(0)).alias("MAXCustomerID")).first()\[0\]

\# Assume dfdimCustomer_silver is your source DataFrame with new data

\# Here, we select only the new customers by doing a left anti join

dfdimCustomer_gold = dfdimCustomer_silver.join(

Â  Â  dfdimCustomer_temp,

Â  Â  (dfdimCustomer_silver.CustomerName ==
dfdimCustomer_temp.CustomerName) &

Â  Â  (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email),

Â  Â  "left_anti"

)

\# Add the CustomerID column with unique values starting from
MAXCustomerID + 1

dfdimCustomer_gold = dfdimCustomer_gold.withColumn(

Â  Â  "CustomerID",

Â  Â  monotonically_increasing_id() + MAXCustomerID + 1

)

\# Display the first 10 rows of the dataframe to preview your data

dfdimCustomer_gold.show(10)

![](./media/image48.png)

![A screenshot of a computer Description automatically
generated](./media/image49.png)

4.  Ahora se asegurarÃ¡ de que su tabla de clientes se mantiene
    actualizada a medida que llegan nuevos datos.Â **En un nuevo bloque
    de cÃ³digo**, pegue y ejecute lo siguiente:

> CodeCopy

from delta.tables import DeltaTable

\# Define the Delta table path

deltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')

\# Use dfUpdates to refer to the DataFrame with new or updated records

dfUpdates = dfdimCustomer_gold

\# Perform the merge operation to update or insert new records

deltaTable.alias('silver') \\

Â  .merge(

Â  Â  dfUpdates.alias('updates'),

Â  Â  'silver.CustomerName = updates.CustomerName AND silver.Email =
updates.Email'

Â  ) \\

Â  .whenMatchedUpdate(set =

Â  Â  {

Â  Â  Â  "CustomerName": "updates.CustomerName",

Â  Â  Â  "Email": "updates.Email",

Â  Â  Â  "First": "updates.First",

Â  Â  Â  "Last": "updates.Last",

Â  Â  Â  "CustomerID": "updates.CustomerID"

Â  Â  }

Â  ) \\

Â  .whenNotMatchedInsert(values =

Â  Â  {

Â  Â  Â  "CustomerName": "updates.CustomerName",

Â  Â  Â  "Email": "updates.Email",

Â  Â  Â  "First": "updates.First",

Â  Â  Â  "Last": "updates.Last",

Â  Â  Â  "CustomerID": "updates.CustomerID"

Â  Â  }

Â  ) \\

Â  .execute()

![](./media/image50.png)

![A screenshot of a computer Description automatically
generated](./media/image51.png)

5.  Ahora **repetirÃ¡ esos pasos para crear la dimensiÃ³n de su
    producto**. En un nuevo bloque de cÃ³digo, pegue y ejecute lo
    siguiente:

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.dimproduct_gold") \\
>
> .addColumn("ItemName", StringType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("ItemInfo", StringType()) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image52.png)

![A screenshot of a computer Description automatically
generated](./media/image53.png)

6.  **AÃ±ada otro bloque de cÃ³digo** para crear el dataframe
    **product_silver**.

> CodeCopy
>
> from pyspark.sql.functions import col, split, lit
>
> \# Create product_silver dataframe
>
> dfdimProduct_silver =
> df.dropDuplicates(\["Item"\]).select(col("Item")) \\
>
> .withColumn("ItemName",split(col("Item"), ", ").getItem(0)) \\
>
> .withColumn("ItemInfo",when((split(col("Item"), ",
> ").getItem(1).isNull() | (split(col("Item"), ",
> ").getItem(1)=="")),lit("")).otherwise(split(col("Item"), ",
> ").getItem(1)))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimProduct_silver.head(10))

![A screenshot of a computer Description automatically
generated](./media/image54.png)

![A screenshot of a computer Description automatically
generated](./media/image55.png)

7.  Ahora crearÃ¡ IDs para suÂ **tabla dimProduct_gold**. AÃ±ada la
    siguiente sintaxis a un nuevo bloque de cÃ³digo y ejecÃºtelo:

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col, lit,
max, coalesce

\#dfdimProduct_temp = dfdimProduct_silver

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

MAXProductID =
dfdimProduct_temp.select(coalesce(max(col("ItemID")),lit(0)).alias("MAXItemID")).first()\[0\]

dfdimProduct_gold =
dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName
== dfdimProduct_temp.ItemName) & (dfdimProduct_silver.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left_anti")

dfdimProduct_gold =
dfdimProduct_gold.withColumn("ItemID",monotonically_increasing_id() +
MAXProductID + 1)

\# Display the first 10 rows of the dataframe to preview your data

display(dfdimProduct_gold.head(10))

![A screenshot of a computer Description automatically
generated](./media/image56.png)

Esto calcula el siguiente ID de producto disponible basÃ¡ndose en los
datos actuales de la tabla, asigna estos nuevos ID a los productos y, a
continuaciÃ³n, muestra la informaciÃ³n actualizada del producto.

![A screenshot of a computer Description automatically
generated](./media/image57.png)

8.  De forma similar a lo que ha hecho con sus otras dimensiones, debe
    asegurarse de que su tabla de productos se mantiene actualizada a
    medida que llegan nuevos datos.Â **En un nuevo bloque de cÃ³digo**,
    pegue y ejecute lo siguiente:

CodeCopy

from delta.tables import \*

deltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')

dfUpdates = dfdimProduct_gold

deltaTable.alias('silver') \\

.merge(

dfUpdates.alias('updates'),

'silver.ItemName = updates.ItemName AND silver.ItemInfo =
updates.ItemInfo'

) \\

.whenMatchedUpdate(set =

{

}

) \\

.whenNotMatchedInsert(values =

{

"ItemName": "updates.ItemName",

"ItemInfo": "updates.ItemInfo",

"ItemID": "updates.ItemID"

}

) \\

.execute()

![A screenshot of a computer Description automatically
generated](./media/image58.png)

![A screenshot of a computer Description automatically
generated](./media/image59.png)

**Ahora que tiene sus dimensiones construidas, el paso final es crear la
tabla de hechos.**

9.  **En un nuevo bloque de cÃ³digo,** pegue y ejecute el siguiente
    cÃ³digo para crear la**Â tabla de hechos.**

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.factsales_gold") \\
>
> .addColumn("CustomerID", LongType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("OrderDate", DateType()) \\
>
> .addColumn("Quantity", IntegerType()) \\
>
> .addColumn("UnitPrice", FloatType()) \\
>
> .addColumn("Tax", FloatType()) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image60.png)

![A screenshot of a computer Description automatically
generated](./media/image61.png)

10. **En un nuevo bloque de cÃ³digo,** pegue y ejecute el siguiente
    cÃ³digo para crear un **nuevo dataframe** que combine los datos de
    ventas con la informaciÃ³n del cliente y del producto, incluyendo el
    ID del cliente, el ID del elemento, la fecha del pedido, la
    cantidad, el precio unitario y los impuestos:

CodeCopy

from pyspark.sql import SparkSession

from pyspark.sql.functions import split, col, when, lit

from pyspark.sql.types import StructType, StructField, StringType,
IntegerType, DateType, FloatType, BooleanType, TimestampType

\# Initialize Spark session

spark = SparkSession.builder \\

Â  Â  .appName("DeltaTableUpsert") \\

Â  Â  .config("spark.sql.extensions",
"io.delta.sql.DeltaSparkSessionExtension") \\

Â  Â  .config("spark.sql.catalog.spark_catalog",
"org.apache.spark.sql.delta.catalog.DeltaCatalog") \\

Â  Â  .getOrCreate()

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

Â  Â  StructField("SalesOrderNumber", StringType(), True),

Â  Â  StructField("SalesOrderLineNumber", IntegerType(), True),

Â  Â  StructField("OrderDate", DateType(), True),

Â  Â  StructField("CustomerName", StringType(), True),

Â  Â  StructField("Email", StringType(), True),

Â  Â  StructField("Item", StringType(), True),

Â  Â  StructField("Quantity", IntegerType(), True),

Â  Â  StructField("UnitPrice", FloatType(), True),

Â  Â  StructField("Tax", FloatType(), True),

Â  Â  StructField("FileName", StringType(), True),

Â  Â  StructField("IsFlagged", BooleanType(), True),

Â  Â  StructField("CreatedTS", TimestampType(), True),

Â  Â  StructField("ModifiedTS", TimestampType(), True)

\])

\# Define the path to the Delta table (ensure this path is correct)

delta_table_path =
"abfss://\<container\>@\<storage-account\>.dfs.core.windows.net/path/to/wwilakehouse/sales_silver"

\# Create a DataFrame with the defined schema

empty_df = spark.createDataFrame(\[\], silver_table_schema)

\# Register the Delta table in the Metastore

spark.sql(f"""

Â  Â  CREATE TABLE IF NOT EXISTS wwilakehouse.sales_silver

Â  Â  USING DELTA

Â  Â  LOCATION '{delta_table_path}'

""")

\# Load data into DataFrame

df = spark.read.table("wwilakehouse.sales_silver")

\# Perform transformations on df

df = df.withColumn("ItemName", split(col("Item"), ", ").getItem(0)) \\

Â  Â  .withColumn("ItemInfo", when(

Â  Â  Â  Â  (split(col("Item"), ", ").getItem(1).isNull()) |
(split(col("Item"), ", ").getItem(1) == ""),

Â  Â  Â  Â  lit("")

Â  Â  ).otherwise(split(col("Item"), ", ").getItem(1)))

\# Load additional DataFrames for joins

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

\# Create Sales_gold dataframe

dffactSales_gold = df.alias("df1").join(dfdimCustomer_temp.alias("df2"),
(df.CustomerName == dfdimCustomer_temp.CustomerName) & (df.Email ==
dfdimCustomer_temp.Email), "left") \\

Â  Â  .join(dfdimProduct_temp.alias("df3"), (df.ItemName ==
dfdimProduct_temp.ItemName) & (df.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left") \\

Â  Â  .select(

Â  Â  Â  Â  col("df2.CustomerID"),

Â  Â  Â  Â  col("df3.ItemID"),

Â  Â  Â  Â  col("df1.OrderDate"),

Â  Â  Â  Â  col("df1.Quantity"),

Â  Â  Â  Â  col("df1.UnitPrice"),

Â  Â  Â  Â  col("df1.Tax")

Â  Â  ).orderBy(col("df1.OrderDate"), col("df2.CustomerID"),
col("df3.ItemID"))

\# Show the result

dffactSales_gold.show()

![A screenshot of a computer Description automatically
generated](./media/image62.png)

![A screenshot of a computer Description automatically
generated](./media/image63.png)

![A screenshot of a computer Description automatically
generated](./media/image64.png)

1.  Ahora se asegurarÃ¡ de que los datos de ventas permanezcan
    actualizados ejecutando el siguiente cÃ³digo en unÂ **nuevo bloque de
    cÃ³digo**:

> CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')
>
> dfUpdates = dffactSales_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate AND silver.CustomerID =
> updates.CustomerID AND silver.ItemID = updates.ItemID'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "CustomerID": "updates.CustomerID",
>
> "ItemID": "updates.ItemID",
>
> "OrderDate": "updates.OrderDate",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax"
>
> }
>
> ) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image65.png)

AquÃ­ estÃ¡ utilizando la operaciÃ³n de fusiÃ³n de Delta Lake para
sincronizar y actualizar la tabla factsales_gold con los nuevos datos de
ventas (dffactSales_gold). La operaciÃ³n compara la fecha del pedido, el
ID del cliente y el ID del elemento entre los datos existentes (tabla
silver) y los nuevos datos (actualiza dataframe), actualizando los
registros coincidentes e insertando nuevos registros segÃºn sea
necesario.

![A screenshot of a computer Description automatically
generated](./media/image66.png)

Ahora dispone de una capaÂ **gold**Â curada y modelada que puede
utilizarse para informes y anÃ¡lisis.

# Ejercicio 4: Establecimiento de la conectividad entre Azure Databricks y Azure Data Lake Storage (ADLS) Gen 2

Ahora vamos a crear una tabla Delta con la ayuda de su cuenta Azure Data
Lake Storage (ADLS) Gen2 utilizando Azure Databricks. A continuaciÃ³n,
crearÃ¡ un shortcut de OneLake a una tabla Delta en ADLS y utilizarÃ¡
Power BI para analizar los datos a travÃ©s del shortcut ADLS.

## **Tarea 0: Canjear un pase Azure y activar la suscripciÃ³n Azure**

1.  Navegue en el siguiente enlace
    !!https://www.microsoftazurepass.com/!! y haga clic en el
    botÃ³nÂ **Start**.

![](./media/image67.png)

2.  En la pÃ¡gina de inicio de sesiÃ³n de Microsoft introduzca elÂ **ID de
    tenant,Â **haga clic enÂ **Next.**

![](./media/image68.png)

3.  En la pÃ¡gina siguiente ingrese su contraseÃ±a y haga clic enÂ **Sign
    In**.

![](./media/image69.png)

![A screenshot of a computer error Description automatically
generated](./media/image70.png)

4.  Una vez iniciada la sesiÃ³n, en la pÃ¡gina de Microsoft Azure, haga
    clic en la pestaÃ±aÂ **Confirm Microsoft Account**.

![](./media/image71.png)

5.  En la pÃ¡gina siguiente, ingrese el cÃ³digo de promociÃ³n, los
    caracteres Captcha y haga clic enÂ **Submit.**

![A screenshot of a computer Description automatically
generated](./media/image72.png)

![A screenshot of a computer error Description automatically
generated](./media/image73.png)

6.  En la pÃ¡gina Your profile ingrese los datos de su perfil y haga clic
    enÂ **Sign up.**

7.  Si se le solicita, regÃ­strese para la autenticaciÃ³n multifactor y, a
    continuaciÃ³n, inicie sesiÃ³n en el portal Azure navegando hasta el
    siguiente enlace!! <https://portal.azure.com/#home>!!

![](./media/image74.png)

8.  En la barra de bÃºsqueda escriba Subscriptions y haga clic en el
    icono Subscriptions bajoÂ **Services.**

![A screenshot of a computer Description automatically
generated](./media/image75.png)

9.  DespuÃ©s de canjear con Ã©xito el pase Azure se generarÃ¡ un ID de
    suscripciÃ³n.

![](./media/image76.png)

## **Tarea 1: Crear una cuenta Azure Data Storage**

1.  Inicie sesiÃ³n en su portal Azure, utilizando sus credenciales azure.

2.  En la pÃ¡gina de inicio, en el menÃº izquierdo del portal,
    seleccioneÂ **Storage accounts**Â para ver una lista de sus cuentas de
    almacenamiento. Si el menÃº del portal no estÃ¡ visible, seleccione el
    botÃ³n del menÃº para activarlo.

![A screenshot of a computer Description automatically
generated](./media/image77.png)

3.  En la pÃ¡ginaÂ **Storage accounts**, seleccione **Create**.

![A screenshot of a computer Description automatically
generated](./media/image78.png)

4.  En la pestaÃ±a Basics, al seleccionar un grupo de recursos,
    proporcione la informaciÃ³n esencial para su cuenta de
    almacenamiento:

[TABLE]

Deje los demÃ¡s ajustes como estÃ¡n y seleccioneÂ **Review + createÂ **para
aceptar las opciones por defecto y proceder a validar y crear la cuenta.

Nota: Si aÃºn no tiene un grupo de recursos creado, puede hacer clic en
**â€œCreate newâ€** y crear un nuevo recurso para su cuenta de
almacenamiento.

.

![](./media/image79.png)

![](./media/image80.png)

![A screenshot of a computer Description automatically
generated](./media/image81.png)

![A screenshot of a computer Description automatically
generated](./media/image82.png)

![A screenshot of a computer Description automatically
generated](./media/image83.png)

5.  Cuando navega a la pestaÃ±aÂ **Review + create**, Azure ejecuta la
    validaciÃ³n de la configuraciÃ³n de la cuenta de almacenamiento que ha
    elegido. Si la validaciÃ³n pasa, puede proceder a crear la cuenta de
    almacenamiento.

Si la validaciÃ³n falla, el portal indica quÃ© ajustes deben modificarse.

![A screenshot of a computer Description automatically
generated](./media/image84.png)

![A screenshot of a computer Description automatically
generated](./media/image85.png)

Ahora ha creado correctamente su cuenta de almacenamiento de datos
Azure.

6.  Navegue a la pÃ¡gina de cuentas de almacenamiento buscando en la
    barra de bÃºsqueda de la parte superior de la pÃ¡gina, seleccione la
    cuenta de almacenamiento reciÃ©n creada.

![A screenshot of a computer Description automatically
generated](./media/image86.png)

7.  En la pÃ¡gina de la cuenta de almacenamiento, vaya
    aÂ **Containers**Â enÂ **Data storage**Â en el panel de navegaciÃ³n de la
    izquierda, cree un nuevo contenedor con el nombre !!medalion1!! y
    haga clic en el botÃ³nÂ **Create**.Â 

Â 

![A screenshot of a computer Description automatically
generated](./media/image87.png)

8.  Ahora vuelva a la pÃ¡ginaÂ **storage account**Â y
    seleccioneÂ **EndpointsÂ **en el menÃº de navegaciÃ³n de la izquierda.
    DesplÃ¡cese hacia abajo y copie laÂ **URL del endpoint principal**Â y
    pÃ©guela en un bloc de notas. Esto le serÃ¡ Ãºtil mientras crea el
    shortcut.

![](./media/image88.png)

9.  Del mismo modo, navegue hastaÂ **Access keys** en el mismo panel de
    navegaciÃ³n.

![A screenshot of a computer Description automatically
generated](./media/image89.png)

## **Tarea 2: Cree una tabla Delta, cree un shortcut y analice los datos en su Lakehouse**

1.  En su lakehouse, seleccione las elipsesÂ **(...)**Â junto a los
    archivos y, a continuaciÃ³n, seleccioneÂ **New shortcut**.

![](./media/image90.png)

2.  En la pantallaÂ **New shortcut**, seleccione el mosaicoÂ **Azure Data
    Lake Storage Gen2**.

![Screenshot of the tile options in the New shortcut
screen.](./media/image91.png)

3.  Especifique los detalles de conexiÃ³n para el shortcut:

[TABLE]

4.  Y haga clic en **Next**.

![A screenshot of a computer Description automatically
generated](./media/image92.png)

5.  Esto establecerÃ¡ un enlace con su contenedor de almacenamiento
    Azure. Seleccione el almacenamiento y haga clic en el
    botÃ³nÂ **Next**.

![A screenshot of a computer Description automatically
generated](./media/image93.png)

![A screenshot of a computer Description automatically
generated](./media/image94.png)![A screenshot of a computer Description
automatically generated](./media/image95.png)

6.  Una vez iniciado el Wizard, seleccioneÂ **FilesÂ **y seleccione
    elÂ **â€œ...â€Â **en el archivo**Â bronze**.

![A screenshot of a computer Description automatically
generated](./media/image96.png)

7.  Seleccione **load to tables** y **new table**.

![](./media/image97.png)

8.  En la ventana emergente indique el nombre de su tabla
    comoÂ **bronze_01**Â y seleccione el tipo de archivo comoÂ **parquet**.

![A screenshot of a computer Description automatically
generated](./media/image98.png)

![A screenshot of a computer Description automatically
generated](./media/image99.png)

![A screenshot of a computer Description automatically
generated](./media/image100.png)

![A screenshot of a computer Description automatically
generated](./media/image101.png)

9.  El archivoÂ **bronze_01**Â es ahora visible en los archivos.

![A screenshot of a computer Description automatically
generated](./media/image102.png)

10. A continuaciÃ³n, seleccione **"..."Â **en el archivo**Â bronze**.
    SeleccioneÂ **load to tables**Â yÂ **existing table.**

![A screenshot of a computer Description automatically
generated](./media/image103.png)

11. Proporcione el nombre de la tabla existente
    comoÂ **dimcustomer_gold.**Â Seleccione el tipo de archivo
    como**Â parquetÂ **y seleccione**Â load.**

![A screenshot of a computer Description automatically
generated](./media/image104.png)

![A screenshot of a computer Description automatically
generated](./media/image105.png)

## **Tarea 3: Crear un modelo semÃ¡ntico utilizando la capa gold para crear un informe**

En su workspace, ahora puede utilizar la capa gold para crear un informe
y analizar los datos. Puede acceder al modelo semÃ¡ntico directamente en
su workspace para crear relaciones y medidas para los informes.

*Tenga en cuenta que no puede utilizar elÂ **modelo semÃ¡ntico por
defecto**Â que se crea automÃ¡ticamente al crear un lakehouse.Â Debe crear
un nuevo modelo semÃ¡ntico que incluya las tablas gold que ha creado en
este laboratorio, desde el explorador de lakehouse.*

1.  En su workspace, navegue hasta suÂ **wwilakehouseÂ **lakehouse. A
    continuaciÃ³n, seleccioneÂ **New semantic model**Â en la cinta de la
    vista del explorador de lakehouse.

![A screenshot of a computer Description automatically
generated](./media/image106.png)

2.  En la ventana emergente, asigne el nombreÂ **DatabricksTutorial**Â a
    su nuevo modelo semÃ¡ntico y seleccione el workspace comoÂ **Fabric
    Lakehouse Tutorial-29**.

![](./media/image107.png)

3.  A continuaciÃ³n, desplÃ¡cese hacia abajo y seleccione todo lo que
    desee incluir en su modelo semÃ¡ntico y seleccioneÂ **Confirm**.

Esto abrirÃ¡ el modelo semÃ¡ntico en Fabric donde podrÃ¡ crear relaciones y
medidas, como se muestra aquÃ­:

![A screenshot of a computer Description automatically
generated](./media/image108.png)

Desde aquÃ­, usted u otros miembros de su equipo de datos pueden crear
informes y cuadros de mando basados en los datos de su lakehouse. Estos
informes estarÃ¡n conectados directamente a la capa gold de su lakehouse,
por lo que siempre reflejarÃ¡n los datos mÃ¡s recientes.

# Ejercicio 5: Ingesta de datos y anÃ¡lisis con Azure Databricks

1.  Navegue hasta su lakehouse en el servicio Power BI y
    seleccioneÂ **Get data**Â y luego seleccioneÂ **New data pipeline**.

![Screenshot showing how to navigate to new data pipeline option from
within the UI.](./media/image109.png)

1.  En el prompt **New** **Pipeline**, ingrese un nombre para el nuevo
    pipeline y luego seleccione **Create**.

**IngestDatapipeline01**

![](./media/image110.png)

2.  Para este ejercicio, seleccione los datos de la muestraÂ **NYC Taxi -
    Green**Â como fuente de datos.

![A screenshot of a computer Description automatically
generated](./media/image111.png)

3.  En la pantalla de vista previa, seleccione **Next**.

![A screenshot of a computer Description automatically
generated](./media/image112.png)

4.  Para el destino de los datos, seleccione el nombre de la tabla que
    desea utilizar para almacenar los datos de la tabla OneLake Delta.
    Puede elegir una tabla existente o crear una nueva. Para el
    propÃ³sito de este laboratorio, seleccioneÂ **load into new tableÂ **y
    seleccioneÂ **Next**.

![A screenshot of a computer Description automatically
generated](./media/image113.png)

5.  En la pantallaÂ **Review + Save**, seleccioneÂ **Start data transfer
    immediately**Â y despuÃ©sÂ **Save + Run**.

![Screenshot showing how to enter table name.](./media/image114.png)

![A screenshot of a computer Description automatically
generated](./media/image115.png)

6.  Una vez finalizado el trabajo, navegue hasta su lakehouse y
    visualice la tabla delta que aparece en /Tables.

![A screenshot of a computer Description automatically
generated](./media/image116.png)

7.  Copie la ruta del Azure Blob Filesystem (ABFS) de su tabla delta a
    haciendo clic con el botÃ³n derecho del mouse en el nombre de la
    tabla en la vista del explorador y seleccionandoÂ **Properties.**

![A screenshot of a computer Description automatically
generated](./media/image117.png)

8.  Abra su notebook Azure Databricks y ejecute el cÃ³digo.

olsPath = "**abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample**"

df=spark.read.format('delta').option("inferSchema","true").load(olsPath)

df.show(5)

*Nota: Sustituya la ruta del archivo en negrita por la que ha copiado.*

![](./media/image118.png)

9.  Actualizar los datos de la tabla Delta cambiando el valor de un
    campo.

%sql

update delta.\`abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample\` set vendorID = 99999 where vendorID
= 1;

*Nota: Sustituya la ruta del archivo en negrita por la que ha copiado.*

![A screenshot of a computer Description automatically
generated](./media/image119.png)

# Ejercicio 6: LiberaciÃ³n de recursos 

En este ejercicio, ha aprendido a crear una arquitectura Medallion en un
lakehouse de Microsoft Fabric.

Si ha terminado de explorar su lakehouse, puede eliminar el workspace
que creÃ³ para este ejercicio.

1.  Seleccione su workspace, elÂ **Tutorial-29 de Fabric LakehouseÂ **en
    el menÃº de navegaciÃ³n de la izquierda. Se abrirÃ¡ la vista de
    elementos del workspace.

![A screenshot of a computer Description automatically
generated](./media/image120.png)

2.  Seleccione la opciÃ³nÂ ***...***Â bajo el nombre del workspace y
    seleccioneÂ **Workspace settings**.

![A screenshot of a computer Description automatically
generated](./media/image121.png)

3.  DesplÃ¡cese hasta la parte inferior y **Remove this workspace.**

![A screenshot of a computer Description automatically
generated](./media/image122.png)

4.  Haga clic enÂ **DeleteÂ **en la advertencia que aparece.

![A white background with black text Description automatically
generated](./media/image123.png)

5.  Espere a recibir una notificaciÃ³n de que el workspace ha sido
    eliminado, antes de pasar al siguiente laboratorio.

![A screenshot of a computer Description automatically
generated](./media/image124.png)

**Resumen**:

Este laboratorio guÃ­a a los participantes a travÃ©s de la construcciÃ³n de
una arquitectura Medallion en un lakehouse Microsoft Fabric utilizando
notebooks. Los pasos clave incluyen la configuraciÃ³n de un workspace, el
establecimiento de un lakehouse, la carga de datos en la capa bronze
para la ingestiÃ³n inicial, su transformaciÃ³n en una tabla Delta silver
para el procesamiento estructurado, el refinamiento posterior en tablas
Delta gold para el anÃ¡lisis avanzado, la exploraciÃ³n de modelos
semÃ¡nticos y la creaciÃ³n de relaciones de datos para un anÃ¡lisis
profundo.

## 
