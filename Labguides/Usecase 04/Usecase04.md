# Cas d'utilisation 04 : Analytique moderne √† l'√©chelle du cloud avec Azure Databricks et Microsoft Fabric

**Introduction**

Dans cet atelier, vous allez explorer l'int√©gration d'Azure Databricks √†
Microsoft Fabric pour cr√©er et g√©rer un lakehouse √† l'aide de
l'architecture Medallion, cr√©er une table Delta √† l'aide de votre compte
Azure Data Lake Storage (ADLS) Gen2 √† l'aide d'Azure Databricks et
Ing√©rer des donn√©es avec Azure Databricks. Ce guide pratique vous
guidera √† travers les √©tapes n√©cessaires √† la cr√©ation d'un lakehouse, y
charger des donn√©es et explorer les couches de donn√©es structur√©es pour
faciliter l'analyse des donn√©es et la cr√©ation de rapports efficaces.

L'architecture des m√©daillons se compose de trois couches (ou zones)
distinctes.

- Bronze : Aussi appel√©e zone brute, cette premi√®re couche stocke les
  donn√©es sources dans leur format d'origine. Les donn√©es de cette
  couche sont g√©n√©ralement immuables et immuables.

- Argent : √âgalement connue sous le nom de zone enrichie, cette couche
  stocke les donn√©es provenant de la couche bronze. Les donn√©es brutes
  ont √©t√© nettoy√©es et standardis√©es, et elles sont maintenant
  structur√©es sous forme de tables (lignes et colonnes). Elles peuvent
  √©galement √™tre int√©gr√©es √† d'autres donn√©es pour fournir une vue
  d'entreprise de toutes les entit√©s commerciales, telles que le client,
  le produit, etc.

- Or : √âgalement connue sous le nom de zone organis√©e, cette derni√®re
  couche stocke les donn√©es provenant de la couche argent√©e. Les donn√©es
  sont affin√©es pour r√©pondre aux exigences sp√©cifiques de l'activit√© et
  de l'analytique en aval. Les tables sont g√©n√©ralement conformes √† la
  conception de sch√©ma en √©toile, qui prend en charge le d√©veloppement
  de mod√®les de donn√©es optimis√©s pour les performances et la
  convivialit√©.

**Objectives**:

- Comprendre les principes de l'architecture Medallion dans Microsoft
  Fabric Lakehouse.

- Mettez en place un processus de gestion des donn√©es structur√© √† l'aide
  de couches Medallion (Bronze, Argent, Or).

- Transformez les donn√©es brutes en donn√©es valid√©es et enrichies pour
  des analyses et des rapports avanc√©s.

- D√©couvrez les bonnes pratiques en mati√®re de s√©curit√© des donn√©es, de
  CI/CD et d'interrogation efficace des donn√©es.

- T√©l√©chargez des donn√©es sur OneLake √† l'aide de l'explorateur de
  fichiers OneLake.

- Utilisez un Notebook Fabric pour lire les donn√©es sur OneLake et les
  r√©√©crire sous forme de table Delta.

- Analysez et transformez des donn√©es avec Spark √† l'aide d'un notebook
  Fabric.

- Interrogez une copie de donn√©es sur OneLake avec SQL.

- Cr√©ez une table Delta dans votre compte Azure Data Lake Storage (ADLS)
  Gen2 √† l'aide d'Azure Databricks.

- Cr√©ez un raccourci OneLake vers une table Delta dans ADLS.

- Utilisez Power BI pour analyser les donn√©es via le raccourci ADLS.

- Lire et modifier une table Delta dans OneLake avec Azure Databricks.

# Exercice 1 : Importer vos exemples de donn√©es dans Lakehouse

Dans cet exercice, vous allez passer par le processus de cr√©ation d'un
lakehouse et d'y charger des donn√©es √† l'aide de Microsoft Fabric.

T√¢che¬†: sentier

## **T√¢che 1 : Cr√©er un espace de travail Fabric**

Dans cette t√¢che, vous allez cr√©er un espace de travail Fabric. L'espace
de travail contient tous les √©l√©ments n√©cessaires √† ce didacticiel
lakehouse, notamment lakehouse, les flux de donn√©es, les pipelines Data
Factory, les notebooks, les jeux de donn√©es Power BI et les rapports.

1.  Ouvrez votre navigateur, acc√©dez √† la barre d'adresse et tapez ou
    collez l'URL suivante :
    [https://app.fabric.microsoft.com/](https://app.fabric.microsoft.com/,)
    puis appuyez sur le bouton **Enter**.

> ![Une fen√™tre de moteur de recherche avec une bo√Æte rouge Description
> g√©n√©r√©e automatiquement avec un niveau de confiance
> moyen](./media/image1.png)

2.  Revenez √† la fen√™tre **Power BI**. Dans le menu de navigation de
    gauche de la page d'accueil de Power BI, naviguez et cliquez sur
    **Workspaces**.

![](./media/image2.png)

3.  Dans le volet Workspaces, cliquez sur le bouton **+**¬†**New
    workspace.**

> ![](./media/image3.png)

4.  Dans le volet **Create a workspace** qui s'affiche sur le c√¥t√©
    droit, entrez les d√©tails suivants, puis cliquez sur le bouton
    **Apply**.

[TABLE]

5.  

> ![](./media/image4.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image5.png)

6.  Attendez la fin du d√©ploiement. Cela prend 2-3 minutes √† compl√©ter.

![](./media/image6.png)

## **T√¢che 2 : Cr√©er une maison au bord du lac**

1.  Dans la page **Power BI Fabric Lakehouse Tutorial-XX**, cliquez sur
    l'ic√¥ne **Power BI** situ√©e en bas √† gauche et s√©lectionnez **Data
    Engineering**.

> ![](./media/image7.png)

2.  Sur la page d'accueil de **Synapse Data Engineering** , s√©lectionnez
    **Lakehouse** pour cr√©er un lakehouse.

![](./media/image8.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image9.png)

3.  Dans la bo√Æte de dialogue New lakehouse, entrez **wwilakehouse**
    dans le champ **Name**, cliquez sur le bouton **Create** et ouvrez
    le New lakehouse.

> **Remarque**¬†: Assurez-vous de supprimer l'espace avant
> **wwilakehouse**.
>
> ![](./media/image10.png)
>
> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image11.png)
>
> ![](./media/image12.png)

4.  Une notification indiquant **Successfully created SQL endpoint**,
    s'affiche.

> ![](./media/image13.png)

# Exercice 2 : Impl√©mentation de l'architecture Medallion √† l'aide d'Azure Databricks

## **T√¢che 1 : Mise en place de la couche de bronze**

1.  Sur la **page wwilakehouse**, s√©lectionnez l'ic√¥ne Plus √† c√¥t√© des
    fichiers (...), puis s√©lectionnez **New subfolder**

![](./media/image14.png)

2.  Dans la fen√™tre contextuelle, indiquez le nom du dossier bronze,
    puis s√©lectionnez Cr√©er.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image15.png)

3.  Maintenant, s√©lectionnez l'ic√¥ne Plus √† c√¥t√© des fichiers bronze
    (...), puis s√©lectionnez **Upload**, puis **upload files**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image16.png)

4.  Dans le volet **upload file**, s√©lectionnez la case d'option
    **Upload file**. Cliquez sur le bouton **Browse** et acc√©dez √†
    **C¬†:\LabFiles**, puis s√©lectionnez le fichier de fichiers de
    donn√©es de vente requis (2019, 2020, 2021) et cliquez sur le bouton
    Open.

Ensuite, s√©lectionnez **Upload** pour t√©l√©charger les fichiers dans le
nouveau dossier ¬´ bronze ¬ª de votre Lakehouse.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image17.png)

> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image18.png)

5.  Cliquez sur le dossier **bronze** pour v√©rifier que les fichiers ont
    √©t√© t√©l√©charg√©s avec succ√®s et qu'ils sont refl√©t√©s.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image19.png)

# Exercice 3 : Transformation de donn√©es avec Apache Spark et requ√™te avec SQL dans l'architecture m√©daillon

## **T√¢che 1 : Transformer les donn√©es et les charger dans la table Delta Silver**

Dans la page **wwilakehouse**, naviguez et cliquez sur **Open notebook**
dans la barre de commandes, puis s√©lectionnez **New notebook**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image20.png)

1.  S√©lectionnez la premi√®re cellule (qui est actuellement une *cellule
    de code*), puis dans la barre d'outils dynamique en haut √† droite,
    utilisez le bouton **M‚Üì** pour **convertir la cellule en cellule
    Markdown (convert the cell to a markdown cell)**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image21.png)

2.  Lorsque la cellule se transforme en cellule Markdown, le texte
    qu'elle contient est affich√©.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image22.png)

3.  Utilisez le **üñâ** bouton (Modifier) pour passer la cellule en mode
    √©dition, remplacez tout le texte puis modifiez la d√©marque comme
    suit :

CodeCopy

\# Exploration des donn√©es de commande client

Utilisez le code de ce notebook pour explorer les donn√©es de commande
client.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image23.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image24.png)

4.  Cliquez n'importe o√π dans le Notebook en dehors de la cellule pour
    arr√™ter de le modifier et voir le markdown rendu.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image25.png)

5.  Utilisez l'ic√¥ne de code + sous la sortie de la cellule pour ajouter
    une nouvelle cellule de code au Notebook.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image26.png)

6.  Maintenant, utilisez le notebook pour charger les donn√©es de la
    couche bronze dans un DataFrame Spark.

S√©lectionnez la cellule existante dans le Notebook, qui contient un code
comment√© simple. Mettez en surbrillance et supprimez ces deux lignes -
vous n'aurez pas besoin de ce code.

*Remarque : Les notebooks vous permettent d'Run du code dans divers
langages, notamment Python, Scala et SQL. Dans cet exercice, vous allez
utiliser PySpark et SQL. Vous pouvez √©galement ajouter des cellules
Markdown pour fournir du texte et des images format√©s afin de documenter
votre code.*

Pour cela, entrez le code suivant et cliquez sur **Run**.

CodeCopy

from pyspark.sql.types import \*

\# Create the schema for the table

orderSchema = StructType(\[

StructField("SalesOrderNumber", StringType()),

StructField("SalesOrderLineNumber", IntegerType()),

StructField("OrderDate", DateType()),

StructField("CustomerName", StringType()),

StructField("Email", StringType()),

StructField("Item", StringType()),

StructField("Quantity", IntegerType()),

StructField("UnitPrice", FloatType()),

StructField("Tax", FloatType())

\])

\# Import all files from bronze folder of lakehouse

df = spark.read.format("csv").option("header",
"true").schema(orderSchema).load("Files/bronze/\*.csv")

\# Display the first 10 rows of the dataframe to preview your data

display(df.head(10))

![](./media/image27.png)

***Remarque** : Comme c'est la premi√®re fois que vous ex√©cutez du code
Spark dans ce notebook, une session Spark doit √™tre d√©marr√©e. Cela
signifie que la premi√®re ex√©cution peut prendre environ une minute. Les
ex√©cutions suivantes seront plus rapides.*

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image28.png)

7.  Le code que vous avez ex√©cut√© a charg√© les donn√©es des fichiers CSV
    dans le dossier **bronze** dans une trame de donn√©es Spark, puis a
    affich√© les premi√®res lignes de la trame de donn√©es.

> **Remarque** : Vous pouvez effacer, masquer et redimensionner
> automatiquement le contenu de la sortie de cellule en s√©lectionnant
> l'ic√¥ne **...** en haut √† gauche du volet de sortie.

8.  Vous allez maintenant **add columns for data validation and
    cleanup** des donn√©es, √† l'aide d'une trame de donn√©es PySpark pour
    ajouter des colonnes et mettre √† jour les valeurs de certaines
    colonnes existantes. Utilisez le bouton + pour **add a new code
    block** et ajoutez le code suivant √† la cellule :

> CodeCopy
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> \# Add columns IsFlagged, CreatedTS and ModifiedTS
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \<
> '2019-08-01',True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()).withColumn("ModifiedTS",
> current_timestamp())
>
> \# Update CustomerName to "Unknown" if CustomerName null or empty
>
> df = df.withColumn("CustomerName", when((col("CustomerName").isNull()
> |
> (col("CustomerName")=="")),lit("Unknown")).otherwise(col("CustomerName")))
>
> La premi√®re ligne du code importe les fonctions n√©cessaires de
> PySpark. Vous ajoutez ensuite de nouvelles colonnes √† la trame de
> donn√©es afin de pouvoir suivre le nom du fichier source, si la
> commande a √©t√© marqu√©e comme √©tant avant l'exercice fiscal qui vous
> int√©resse et quand la ligne a √©t√© cr√©√©e et modifi√©e.
>
> Enfin, vous mettez √† jour la colonne CustomerName sur ‚ÄúUnknown‚Äù si
> elle est null ou vide.
>
> Ensuite, ex√©cutez la cellule pour Run le code √† l'aide du bouton
> **\*\*‚ñ∑** (*Run cell*)\*\*.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image29.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image30.png)

9.  Ensuite, vous allez d√©finir le sch√©ma de la table **sales_silver**
    dans la base de donn√©es de vente au format Delta Lake. Cr√©ez un
    nouveau bloc de code et ajoutez le code suivant √† la cellule :

> CodeCopy

from pyspark.sql.types import \*

from delta.tables import \*

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

¬† ¬† StructField("SalesOrderNumber", StringType(), True),

¬† ¬† StructField("SalesOrderLineNumber", IntegerType(), True),

¬† ¬† StructField("OrderDate", DateType(), True),

¬† ¬† StructField("CustomerName", StringType(), True),

¬† ¬† StructField("Email", StringType(), True),

¬† ¬† StructField("Item", StringType(), True),

¬† ¬† StructField("Quantity", IntegerType(), True),

¬† ¬† StructField("UnitPrice", FloatType(), True),

¬† ¬† StructField("Tax", FloatType(), True),

¬† ¬† StructField("FileName", StringType(), True),

¬† ¬† StructField("IsFlagged", BooleanType(), True),

¬† ¬† StructField("CreatedTS", TimestampType(), True),

¬† ¬† StructField("ModifiedTS", TimestampType(), True)

\])

\# Create or replace the sales_silver table with the defined schema

DeltaTable.createIfNotExists(spark) \\

¬† ¬† .tableName("wwilakehouse.sales_silver") \\

¬† ¬† .addColumns(silver_table_schema) \\

¬† ¬† .execute()

¬† ¬†

10. Ex√©cutez la cellule pour Run le code √† l'aide du bouton **\*\*‚ñ∑**
    (*Run cell*)\*\*.

11. S√©lectionnez l'ic√¥ne **...** dans la section Tables du volet de
    l'explorateur lakehouse et s√©lectionnez **Refresh**. Vous devriez
    maintenant voir la nouvelle table **sales_silver** r√©pertori√©e. Le
    **‚ñ≤** (ic√¥ne en forme de triangle) indique qu'il s'agit d'une table
    Delta.

> **Remarque** : Si vous ne voyez pas le nouveau tableau, patientez
> quelques secondes, puis s√©lectionnez √† nouveau **Refresh** ou
> actualisez l'ensemble de l'onglet du navigateur.
>
> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image31.png)
>
> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image32.png)

12. Vous allez maintenant effectuer une **op√©ration de mise √† niveau
    (upsert operation)** sur une table Delta, en mettant √† jour les
    enregistrements existants en fonction de conditions sp√©cifiques et
    en ins√©rant de nouveaux enregistrements lorsqu'aucune correspondance
    n'est trouv√©e. Ajoutez un nouveau bloc de code et collez le code
    suivant :

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> from delta.tables import \*
>
> \# Define the schema for the source data
>
> orderSchema = StructType(\[
>
> StructField("SalesOrderNumber", StringType(), True),
>
> StructField("SalesOrderLineNumber", IntegerType(), True),
>
> StructField("OrderDate", DateType(), True),
>
> StructField("CustomerName", StringType(), True),
>
> StructField("Email", StringType(), True),
>
> StructField("Item", StringType(), True),
>
> StructField("Quantity", IntegerType(), True),
>
> StructField("UnitPrice", FloatType(), True),
>
> StructField("Tax", FloatType(), True)
>
> \])
>
> \# Read data from the bronze folder into a DataFrame
>
> df = spark.read.format("csv").option("header",
> "true").schema(orderSchema).load("Files/bronze/\*.csv")
>
> \# Add additional columns
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \< '2019-08-01',
> True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()) \\
>
> .withColumn("ModifiedTS", current_timestamp()) \\
>
> .withColumn("CustomerName", when((col("CustomerName").isNull()) |
> (col("CustomerName") == ""),
> lit("Unknown")).otherwise(col("CustomerName")))
>
> \# Define the path to the Delta table
>
> deltaTablePath = "Tables/sales_silver"
>
> \# Create a DeltaTable object for the existing Delta table
>
> deltaTable = DeltaTable.forPath(spark, deltaTablePath)
>
> \# Perform the merge (upsert) operation
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> df.alias('updates'),
>
> 'silver.SalesOrderNumber = updates.SalesOrderNumber AND \\
>
> silver.OrderDate = updates.OrderDate AND \\
>
> silver.CustomerName = updates.CustomerName AND \\
>
> silver.Item = updates.Item'
>
> ) \\
>
> .whenMatchedUpdate(set = {
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "Email": "updates.Email",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .whenNotMatchedInsert(values = {
>
> "SalesOrderNumber": "updates.SalesOrderNumber",
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "OrderDate": "updates.OrderDate",
>
> "CustomerName": "updates.CustomerName",
>
> "Email": "updates.Email",
>
> "Item": "updates.Item",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "CreatedTS": "current_timestamp()",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .execute()

13. Ex√©cutez la cellule pour Run le code √† l'aide du bouton **\*\*‚ñ∑**
    (*Run cell*)\*\*.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image33.png)

Cette op√©ration est importante car elle vous permet de mettre √† jour les
enregistrements existants dans la table en fonction des valeurs de
colonnes sp√©cifiques et d'ins√©rer de nouveaux enregistrements
lorsqu'aucune correspondance n'est trouv√©e. Il s'agit d'une exigence
courante lorsque vous chargez des donn√©es √† partir d'un syst√®me source
qui peut contenir des mises √† jour d'enregistrements existants et
nouveaux.

Vous avez maintenant des donn√©es dans votre table delta Silver qui sont
pr√™tes pour une transformation et une mod√©lisation ult√©rieure.

Vous avez r√©ussi √† prendre des donn√©es de votre couche bronze, √† les
transformer et √† les charger dans une table Delta argent√©e. Vous allez
maintenant utiliser un nouveau notebook pour transformer davantage les
donn√©es, les mod√©liser dans un sch√©ma en √©toile et les charger dans des
tables Delta dor√©es.

*Notez que vous auriez pu effectuer tout cela dans un seul Notebook,
mais pour les besoins de cet exercice, vous utilisez des Notebook
distincts pour illustrer le processus de transformation des donn√©es de
bronze √† argent, puis d'argent √† or. Cela peut faciliter le d√©bogage, le
d√©pannage et la r√©utilisation*.

## **T√¢che 2 : Charger des donn√©es dans des tables Gold Delta**

1.  Retournez √† la page d'accueil de Fabric Lakehouse Tutorial-29.

> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image34.png)

2.  S√©lectionnez **wwilakehouse.**

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image35.png)

3.  Dans le volet de l'explorateur Lakehouse, vous devez voir la **table
    sales_silver** r√©pertori√©e dans la section **Tables** du volet de
    l'explorateur.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image36.png)

4.  Maintenant, cr√©ez un Notebook appel√© **Transform data for Gold**.
    Pour cela, naviguez et cliquez sur **Open notebook** dans la barre
    de commandes, puis s√©lectionnez **New notebook**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image37.png)

5.  Dans le bloc de code existant, supprimez le texte standard et
    **ajoutez le code suivant (add the following code)** pour charger
    les donn√©es dans votre trame de donn√©es et commencer √† cr√©er votre
    sch√©ma en √©toile, puis ex√©cutez-le :

> CodeCopy

\# Load data to the dataframe as a starting point to create the gold
layer

df = spark.read.table("wwilakehouse.sales_silver")

\# Display the first few rows of the dataframe to verify the data

df.show()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image38.png)

6.  Ensuite**, Add a new code block** et collez le code suivant pour
    cr√©er votre table de dimension de date et ex√©cutez-la :

¬†from pyspark.sql.types import \*

¬†from delta.tables import\*

¬† ¬†

¬†# Define the schema for the dimdate_gold table

¬†DeltaTable.createIfNotExists(spark) \\

¬† ¬† ¬†.tableName("wwilakehouse.dimdate_gold") \\

¬† ¬† ¬†.addColumn("OrderDate", DateType()) \\

¬† ¬† ¬†.addColumn("Day", IntegerType()) \\

¬† ¬† ¬†.addColumn("Month", IntegerType()) \\

¬† ¬† ¬†.addColumn("Year", IntegerType()) \\

¬† ¬† ¬†.addColumn("mmmyyyy", StringType()) \\

¬† ¬† ¬†.addColumn("yyyymm", StringType()) \\

¬† ¬† ¬†.execute()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image39.png)

**Remarque** : Vous pouvez Run la **commande** display(df) √† tout moment
pour v√©rifier l'avancement de votre travail. Dans ce cas, vous devez Run
'display(dfdimDate_gold) pour voir le contenu de la trame de donn√©es
dimDate_gold.

7.  Dans un nouveau bloc de code, **add and run the following code**
    pour cr√©er une trame de donn√©es pour votre dimension de date,
    **dimdate_gold** :

> CodeCopy

from pyspark.sql.functions import col, dayofmonth, month, year,
date_format

¬† ¬†

¬†# Create dataframe for dimDate_gold

¬† ¬†

dfdimDate_gold
=df.dropDuplicates(\["OrderDate"\]).select(col("OrderDate"), \\

¬† ¬† ¬† ¬† ¬†dayofmonth("OrderDate").alias("Day"), \\

¬† ¬† ¬† ¬† ¬†month("OrderDate").alias("Month"), \\

¬† ¬† ¬† ¬† ¬†year("OrderDate").alias("Year"), \\

¬† ¬† ¬† ¬† ¬†date_format(col("OrderDate"), "MMM-yyyy").alias("mmmyyyy"), \\

¬† ¬† ¬† ¬† ¬†date_format(col("OrderDate"), "yyyyMM").alias("yyyymm"), \\

¬† ¬† ¬†).orderBy("OrderDate")

¬†# Display the first 10 rows of the dataframe to preview your data

display(dfdimDate_gold.head(10))

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image40.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image41.png)

8.  Vous s√©parez le code en nouveaux blocs de code afin de pouvoir
    comprendre et observer ce qui se passe dans le Notebook pendant que
    vous transformez les donn√©es. Dans un autre nouveau bloc de code,
    **ajoutez et ex√©cutez le code suivant  
    (add and run the following code)** pour mettre √† jour la dimension
    de date √† mesure que de nouvelles donn√©es arrivent :

> CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')
>
> dfUpdates = dfdimDate_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "OrderDate": "updates.OrderDate",
>
> "Day": "updates.Day",
>
> "Month": "updates.Month",
>
> "Year": "updates.Year",
>
> "mmmyyyy": "updates.mmmyyyy",
>
> "yyyymm": "yyyymm"
>
> }
>
> ) \\
>
> .execute()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image42.png)

> Votre dimension de date est configur√©e.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image43.png)

## **T√¢che 3 : Cr√©ez votre dimension client.**

1.  Pour cr√©er la table de dimension client, **add a new code block**,
    collez et ex√©cutez le code suivant :

> CodeCopy

¬†from pyspark.sql.types import \*

¬†from delta.tables import \*

¬† ¬†

¬†# Create customer_gold dimension delta table

¬†DeltaTable.createIfNotExists(spark) \\

¬† ¬† ¬†.tableName("wwilakehouse.dimcustomer_gold") \\

¬† ¬† ¬†.addColumn("CustomerName", StringType()) \\

¬† ¬† ¬†.addColumn("Email", ¬†StringType()) \\

¬† ¬† ¬†.addColumn("First", StringType()) \\

¬† ¬† ¬†.addColumn("Last", StringType()) \\

¬† ¬† ¬†.addColumn("CustomerID", LongType()) \\

¬† ¬† ¬†.execute()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image44.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image45.png)

2.  Dans un nouveau bloc de code, **ajoutez et ex√©cutez le code
    suivant** (**add and run the following code)** pour supprimer les
    clients en double, s√©lectionnez des colonnes sp√©cifiques et
    fractionnez la colonne ¬´ CustomerName ¬ª pour cr√©er des colonnes ¬´
    First ¬ª et ¬´ Last ¬ª name :

> CodeCopy
>
> from pyspark.sql.functions import col, split
>
> \# Create customer_silver dataframe
>
> dfdimCustomer_silver =
> df.dropDuplicates(\["CustomerName","Email"\]).select(col("CustomerName"),col("Email"))
> \\
>
> .withColumn("First",split(col("CustomerName"), " ").getItem(0)) \\
>
> .withColumn("Last",split(col("CustomerName"), " ").getItem(1))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimCustomer_silver.head(10))
>
> ![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
> automatiquement](./media/image46.png)

Ici, vous avez cr√©√© un nouveau DataFrame dfdimCustomer_silver en
effectuant diverses transformations telles que la suppression des
doublons, la s√©lection de colonnes sp√©cifiques et la division de la
colonne ¬´ CustomerName ¬ª pour cr√©er des colonnes ¬´ First ¬ª et ¬´ Last
name ¬ª. Le r√©sultat est un DataFrame avec des donn√©es client nettoy√©es
et structur√©es, y compris des colonnes de nom de famille et de pr√©nom
distinctes extraites de la colonne de type ‚ÄúCustomerName‚Äù.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image47.png)

3.  Ensuite, nous allons **create the ID column for our customers**.
    Dans un nouveau bloc de code, collez et ex√©cutez ce qui suit :

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col,
when, coalesce, max, lit

\# Read the existing data from the Delta table

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

\# Find the maximum CustomerID or use 0 if the table is empty

MAXCustomerID =
dfdimCustomer_temp.select(coalesce(max(col("CustomerID")),
lit(0)).alias("MAXCustomerID")).first()\[0\]

\# Assume dfdimCustomer_silver is your source DataFrame with new data

\# Here, we select only the new customers by doing a left anti join

dfdimCustomer_gold = dfdimCustomer_silver.join(

¬† ¬† dfdimCustomer_temp,

¬† ¬† (dfdimCustomer_silver.CustomerName ==
dfdimCustomer_temp.CustomerName) &

¬† ¬† (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email),

¬† ¬† "left_anti"

)

\# Add the CustomerID column with unique values starting from
MAXCustomerID + 1

dfdimCustomer_gold = dfdimCustomer_gold.withColumn(

¬† ¬† "CustomerID",

¬† ¬† monotonically_increasing_id() + MAXCustomerID + 1

)

\# Display the first 10 rows of the dataframe to preview your data

dfdimCustomer_gold.show(10)

![](./media/image48.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image49.png)

4.  Vous vous assurez d√©sormais que votre table de clients reste √† jour
    au fur et √† mesure que de nouvelles donn√©es arrivent. **Dans un
    nouveau bloc de code (In a new code block)**, collez et ex√©cutez ce
    qui suit :

> CodeCopy

from delta.tables import DeltaTable

\# Define the Delta table path

deltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')

\# Use dfUpdates to refer to the DataFrame with new or updated records

dfUpdates = dfdimCustomer_gold

\# Perform the merge operation to update or insert new records

deltaTable.alias('silver') \\

¬† .merge(

¬† ¬† dfUpdates.alias('updates'),

¬† ¬† 'silver.CustomerName = updates.CustomerName AND silver.Email =
updates.Email'

¬† ) \\

¬† .whenMatchedUpdate(set =

¬† ¬† {

¬† ¬† ¬† "CustomerName": "updates.CustomerName",

¬† ¬† ¬† "Email": "updates.Email",

¬† ¬† ¬† "First": "updates.First",

¬† ¬† ¬† "Last": "updates.Last",

¬† ¬† ¬† "CustomerID": "updates.CustomerID"

¬† ¬† }

¬† ) \\

¬† .whenNotMatchedInsert(values =

¬† ¬† {

¬† ¬† ¬† "CustomerName": "updates.CustomerName",

¬† ¬† ¬† "Email": "updates.Email",

¬† ¬† ¬† "First": "updates.First",

¬† ¬† ¬† "Last": "updates.Last",

¬† ¬† ¬† "CustomerID": "updates.CustomerID"

¬† ¬† }

¬† ) \\

¬† .execute()

![](./media/image50.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image51.png)

5.  Vous allez maintenant **repeat those steps to create your product
    dimension**. Dans un nouveau bloc de code, collez et ex√©cutez ce qui
    suit :

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.dimproduct_gold") \\
>
> .addColumn("ItemName", StringType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("ItemInfo", StringType()) \\
>
> .execute()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image52.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image53.png)

6.  **Add another code block** pour cr√©er le **product_silver**
    dataframe.

> CodeCopy
>
> from pyspark.sql.functions import col, split, lit
>
> \# Create product_silver dataframe
>
> dfdimProduct_silver =
> df.dropDuplicates(\["Item"\]).select(col("Item")) \\
>
> .withColumn("ItemName",split(col("Item"), ", ").getItem(0)) \\
>
> .withColumn("ItemInfo",when((split(col("Item"), ",
> ").getItem(1).isNull() | (split(col("Item"), ",
> ").getItem(1)=="")),lit("")).otherwise(split(col("Item"), ",
> ").getItem(1)))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimProduct_silver.head(10))

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image54.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image55.png)

7.  Vous allez maintenant cr√©er des ID pour votre **table
    dimProduct_gold**. Ajoutez la syntaxe suivante √† un nouveau bloc de
    code et ex√©cutez-le :

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col, lit,
max, coalesce

\#dfdimProduct_temp = dfdimProduct_silver

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

MAXProductID =
dfdimProduct_temp.select(coalesce(max(col("ItemID")),lit(0)).alias("MAXItemID")).first()\[0\]

dfdimProduct_gold =
dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName
== dfdimProduct_temp.ItemName) & (dfdimProduct_silver.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left_anti")

dfdimProduct_gold =
dfdimProduct_gold.withColumn("ItemID",monotonically_increasing_id() +
MAXProductID + 1)

\# Display the first 10 rows of the dataframe to preview your data

display(dfdimProduct_gold.head(10))

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image56.png)

Cela calcule le prochain ID de produit disponible en fonction des
donn√©es actuelles de la table, attribue ces nouveaux ID aux produits,
puis affiche les informations sur le produit mises √† jour.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image57.png)

8.  √Ä l'instar de ce que vous avez fait avec vos autres dimensions, vous
    devez vous assurer que votre tableau de produits reste √† jour au fur
    et √† mesure que de nouvelles donn√©es arrivent. **In a new code
    block**, collez et ex√©cutez ce qui suit :

CodeCopy

from delta.tables import \*

deltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')

dfUpdates = dfdimProduct_gold

deltaTable.alias('silver') \\

.merge(

dfUpdates.alias('updates'),

'silver.ItemName = updates.ItemName AND silver.ItemInfo =
updates.ItemInfo'

) \\

.whenMatchedUpdate(set =

{

}

) \\

.whenNotMatchedInsert(values =

{

"ItemName": "updates.ItemName",

"ItemInfo": "updates.ItemInfo",

"ItemID": "updates.ItemID"

}

) \\

.execute()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image58.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image59.png)

**Maintenant que vous avez d√©fini vos dimensions, la derni√®re √©tape
consiste √† cr√©er la table de faits.**

9.  **In a new code block**, collez et ex√©cutez le code suivant pour
    cr√©er **la table de faits (fact table)**:

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.factsales_gold") \\
>
> .addColumn("CustomerID", LongType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("OrderDate", DateType()) \\
>
> .addColumn("Quantity", IntegerType()) \\
>
> .addColumn("UnitPrice", FloatType()) \\
>
> .addColumn("Tax", FloatType()) \\
>
> .execute()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image60.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image61.png)

10. **In a new code block**, collez et ex√©cutez le code suivant pour
    cr√©er un **new dataframe** afin de combiner les donn√©es de vente
    avec les informations sur le client et le produit, notamment l'ID
    client, l'ID de l'article, la date de commande, la quantit√©, le prix
    unitaire et les taxes :

CodeCopy

from pyspark.sql import SparkSession

from pyspark.sql.functions import split, col, when, lit

from pyspark.sql.types import StructType, StructField, StringType,
IntegerType, DateType, FloatType, BooleanType, TimestampType

\# Initialize Spark session

spark = SparkSession.builder \\

¬† ¬† .appName("DeltaTableUpsert") \\

¬† ¬† .config("spark.sql.extensions",
"io.delta.sql.DeltaSparkSessionExtension") \\

¬† ¬† .config("spark.sql.catalog.spark_catalog",
"org.apache.spark.sql.delta.catalog.DeltaCatalog") \\

¬† ¬† .getOrCreate()

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

¬† ¬† StructField("SalesOrderNumber", StringType(), True),

¬† ¬† StructField("SalesOrderLineNumber", IntegerType(), True),

¬† ¬† StructField("OrderDate", DateType(), True),

¬† ¬† StructField("CustomerName", StringType(), True),

¬† ¬† StructField("Email", StringType(), True),

¬† ¬† StructField("Item", StringType(), True),

¬† ¬† StructField("Quantity", IntegerType(), True),

¬† ¬† StructField("UnitPrice", FloatType(), True),

¬† ¬† StructField("Tax", FloatType(), True),

¬† ¬† StructField("FileName", StringType(), True),

¬† ¬† StructField("IsFlagged", BooleanType(), True),

¬† ¬† StructField("CreatedTS", TimestampType(), True),

¬† ¬† StructField("ModifiedTS", TimestampType(), True)

\])

\# Define the path to the Delta table (ensure this path is correct)

delta_table_path =
"abfss://\<container\>@\<storage-account\>.dfs.core.windows.net/path/to/wwilakehouse/sales_silver"

\# Create a DataFrame with the defined schema

empty_df = spark.createDataFrame(\[\], silver_table_schema)

\# Register the Delta table in the Metastore

spark.sql(f"""

¬† ¬† CREATE TABLE IF NOT EXISTS wwilakehouse.sales_silver

¬† ¬† USING DELTA

¬† ¬† LOCATION '{delta_table_path}'

""")

\# Load data into DataFrame

df = spark.read.table("wwilakehouse.sales_silver")

\# Perform transformations on df

df = df.withColumn("ItemName", split(col("Item"), ", ").getItem(0)) \\

¬† ¬† .withColumn("ItemInfo", when(

¬† ¬† ¬† ¬† (split(col("Item"), ", ").getItem(1).isNull()) |
(split(col("Item"), ", ").getItem(1) == ""),

¬† ¬† ¬† ¬† lit("")

¬† ¬† ).otherwise(split(col("Item"), ", ").getItem(1)))

\# Load additional DataFrames for joins

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

\# Create Sales_gold dataframe

dffactSales_gold = df.alias("df1").join(dfdimCustomer_temp.alias("df2"),
(df.CustomerName == dfdimCustomer_temp.CustomerName) & (df.Email ==
dfdimCustomer_temp.Email), "left") \\

¬† ¬† .join(dfdimProduct_temp.alias("df3"), (df.ItemName ==
dfdimProduct_temp.ItemName) & (df.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left") \\

¬† ¬† .select(

¬† ¬† ¬† ¬† col("df2.CustomerID"),

¬† ¬† ¬† ¬† col("df3.ItemID"),

¬† ¬† ¬† ¬† col("df1.OrderDate"),

¬† ¬† ¬† ¬† col("df1.Quantity"),

¬† ¬† ¬† ¬† col("df1.UnitPrice"),

¬† ¬† ¬† ¬† col("df1.Tax")

¬† ¬† ).orderBy(col("df1.OrderDate"), col("df2.CustomerID"),
col("df3.ItemID"))

\# Show the result

dffactSales_gold.show()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image62.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image63.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image64.png)

1.  Vous allez maintenant vous assurer que les donn√©es de vente restent
    √† jour en ex√©cutant le code suivant dans un **new code block** :

> CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')
>
> dfUpdates = dffactSales_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate AND silver.CustomerID =
> updates.CustomerID AND silver.ItemID = updates.ItemID'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "CustomerID": "updates.CustomerID",
>
> "ItemID": "updates.ItemID",
>
> "OrderDate": "updates.OrderDate",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax"
>
> }
>
> ) \\
>
> .execute()

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image65.png)

Ici, vous utilisez l'op√©ration de fusion de Delta Lake pour synchroniser
et mettre √† jour la table factsales_gold avec de nouvelles donn√©es de
vente (dffactSales_gold). L'op√©ration compare la date de commande, l'ID
client et l'ID d'article entre les donn√©es existantes (table argent√©e)
et les nouvelles donn√©es (mise √† jour DataFrame), en mettant √† jour les
enregistrements correspondants et en ins√©rant de nouveaux
enregistrements si n√©cessaire.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image66.png)

Vous disposez maintenant d‚Äôune couche **gold** organis√©e et mod√©lis√©e
qui peut √™tre utilis√©e pour la cr√©ation de rapports et l'analyse.

# Exercice 4 : √âtablissement de la connectivit√© entre Azure Databricks et Azure Data Lake Storage (ADLS) Gen 2

Nous allons maintenant cr√©er une table Delta √† l'aide de votre compte
Azure Data Lake Storage (ADLS) Gen2 √† l'aide d'Azure Databricks.
Ensuite, vous allez cr√©er un raccourci OneLake vers une table Delta dans
ADLS et utiliser Power BI pour analyser les donn√©es via le raccourci
ADLS.

## **T√¢che 0 : Utiliser un pass Azure et activer l'abonnement Azure**

1.  Naviguez sur le lien suivant !!
    https://www.microsoftazurepass.com/¬†!! et cliquez sur le bouton
    **D√©marrer**.

![](./media/image67.png)

2.  Sur la page de connexion Microsoft, entrez Tenant ID**,** cliquez
    sur **Next**.

![](./media/image68.png)

3.  Sur la page suivante, entrez votre mot de passe et cliquez sur
    **Sign In**.

![](./media/image69.png)

![Une capture d'√©cran d'une erreur informatique Description g√©n√©r√©e
automatiquement](./media/image70.png)

4.  Une fois connect√©, sur la page Microsoft Azure, cliquez sur l‚Äôonglet
    **Confirm Microsoft Account**

![](./media/image71.png)

5.  Sur la page suivante, entrez le code promotionnel, les caract√®res
    Captcha et cliquez sur **Submit.**

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image72.png)

![Une capture d'√©cran d'une erreur informatique Description g√©n√©r√©e
automatiquement](./media/image73.png)

6.  Sur la page Votre profil, entrez les d√©tails de votre profil et
    cliquez sur **SIgn up.**

7.  si vous y √™tes invit√©, inscrivez-vous √† l'authentification
    multifacteur, puis connectez-vous au portail Azure en naviguant
    jusqu'au lien suivant !! <https://portal.azure.com/#home>¬†!!

![](./media/image74.png)

8.  Dans la barre de recherche, tapez Abonnement et cliquez sur l'ic√¥ne
    Abonnement sous **Services.**

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image75.png)

9.  Une fois l'√©change r√©ussi d'Azure Pass r√©ussi, un ID d'abonnement
    est g√©n√©r√©.

![](./media/image76.png)

## **T√¢che 1 : Cr√©er un compte Azure Data Storage**

1.  Connectez-vous √† votre portail Azure √† l'aide de vos informations
    d'identification Azure.

2.  Sur la page d'accueil, dans le menu du portail de gauche,
    s√©lectionnez **Storage accounts** pour afficher la liste de vos
    comptes de stockage. Si le menu du portail n'est pas visible,
    s√©lectionnez le bouton de menu pour l'activer.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image77.png)

3.  Sur la page **Storage accounts**, s√©lectionnez **Create**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image78.png)

4.  Sous l'onglet Informations de base, apr√®s avoir s√©lectionn√© un
    groupe de ressources, fournissez les informations essentielles pour
    votre compte de stockage :

[TABLE]

Laissez les autres param√®tres tels quels et s√©lectionnez **Review +
create** pour accepter les options par d√©faut et proc√©der √† la
validation et √† la cr√©ation du compte.

Remarque : Si vous n'avez pas encore cr√©√© de groupe de ressources, vous
pouvez cliquer sur ‚Äú**Create new**‚Äù et cr√©er une ressource pour votre
compte de stockage.

![](./media/image79.png)

![](./media/image80.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image81.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image82.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image83.png)

5.  Lorsque vous acc√©dez √† l'onglet **Review + create**, Azure ex√©cute
    la validation sur les param√®tres du compte de stockage que vous avez
    choisis. Si la validation r√©ussit, vous pouvez proc√©der √† la
    cr√©ation du compte de stockage.

En cas d'√©chec de la validation, le portail indique les param√®tres √†
modifier.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image84.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image85.png)

Vous avez maintenant cr√©√© votre compte de stockage de donn√©es Azure.

6.  Acc√©dez √† la page des comptes de stockage en effectuant une
    recherche dans la barre de recherche en haut de la page, puis
    s√©lectionnez le compte de stockage nouvellement cr√©√©.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image86.png)

7.  Sur la page du compte de stockage, acc√©dez √† **Containers** sous
    **Data storage** dans le volet de navigation de gauche, cr√©ez un
    conteneur avec le nom !! m√©daillon1¬†!! et cliquez sur le bouton
    **Create**.

¬†

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image87.png)

8.  Revenez maintenant sur la page du **storage account**, s√©lectionnez
    **Endpoints** dans le menu de navigation de gauche. Faites d√©filer
    l'√©cran vers le bas et copiez l'URL du **Primary endpoint URL** et
    collez-la sur un Notebook. Cela vous sera utile lors de la cr√©ation
    du raccourci.

![](./media/image88.png)

9.  De m√™me, acc√©dez aux **Access keys** dans le m√™me panneau de
    navigation.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image89.png)

## **T√¢che 2 : Cr√©ez une table Delta, cr√©ez un raccourci et analysez les donn√©es de votre Lakehouse**

1.  Dans votre lakehouse, s√©lectionnez les (...) √† c√¥t√© de fichiers,
    puis s√©lectionnez **New shortcut**.

![](./media/image90.png)

2.  Dans l'√©cran **New shortcut**, s√©lectionnez la vignette **Azure Data
    Lake Storage Gen2**.

![Capture d'√©cran des options de vignette dans l'√©cran Nouveau
raccourci.](./media/image91.png)

3.  Sp√©cifiez les d√©tails de connexion du raccourci :

[TABLE]

4.  Et cliquez sur **Next**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image92.png)

5.  Cela √©tablira un lien avec votre conteneur de stockage Azure.
    S√©lectionnez le stockage, puis s√©lectionnez le bouton **Next**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image93.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image94.png)![Une capture d'√©cran d'un
ordinateur Description g√©n√©r√©e automatiquement](./media/image95.png)

6.  Une fois l'Assistant lanc√©, s√©lectionnez **Files** et s√©lectionnez
    l'ic√¥ne **"... ¬ª** sur la lime en **bronze**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image96.png)

7.  S√©lectionnez **load to tables** et **new table**.

![](./media/image97.png)

8.  Dans la fen√™tre contextuelle, indiquez le nom de votre table sous la
    forme **bronze_01** et s√©lectionnez le type de fichier **Parquet**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image98.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image99.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image100.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image101.png)

9.  Le fichier **bronze_01** est maintenant visible dans les fichiers.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image102.png)

10. Ensuite, s√©lectionnez l'ic√¥ne **"... ¬ª** sur la lime en **bronze**.
    S√©lectionnez **load to tables** et **existing table..**

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image103.png)

11. Indiquez le nom de la table existante sous la forme
    **dimcustomer_gold.** S√©lectionnez le type de fichier **Parquet** et
    s√©lectionnez **load.**

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image104.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image105.png)

## **T√¢che 3 : Cr√©ation d'un mod√®le s√©mantique Utilisation de la couche d'or pour cr√©er un rapport**

Dans votre espace de travail, vous pouvez d√©sormais utiliser la couche
or pour cr√©er un rapport et analyser les donn√©es. Vous pouvez acc√©der au
mod√®le s√©mantique directement dans votre espace de travail pour cr√©er
des relations et des mesures pour la cr√©ation de rapports.

*Notez que vous ne pouvez pas utiliser le ¬†**default semantic model**
qui est cr√©√© automatiquement lorsque vous cr√©ez une Lakehouse. Vous
devez cr√©er un mod√®le s√©mantique qui inclut les tables d'or que vous
avez cr√©√©es dans cet atelier, √† partir de l'explorateur lakehouse.*

1.  Dans votre espace de travail, acc√©dez √† votre lakehouse
    **wwilakehouse**. S√©lectionnez ensuite **New semantic model** dans
    le ruban de la vue Lakehouse Explorer.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image106.png)

2.  Dans la fen√™tre contextuelle, attribuez le nom
    **DatabricksTutorial** √† votre nouveau mod√®le s√©mantique et
    s√©lectionnez l'espace de travail Fabric **Lakehouse Tutorial-29**.

![](./media/image107.png)

3.  Ensuite, faites d√©filer vers le bas et s√©lectionnez tout ce √†
    inclure dans votre mod√®le s√©mantique, puis s√©lectionnez **Confirm**.

Cela ouvrira le mod√®le s√©mantique dans Fabric o√π vous pourrez cr√©er des
relations et des mesures, comme illustr√© ici :

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image108.png)

√Ä partir de l√†, vous ou d'autres membres de votre √©quipe de donn√©es
pouvez cr√©er des rapports et des tableaux de bord bas√©s sur les donn√©es
de votre lakehouse. Ces rapports seront connect√©s directement √† la
couche d'or de votre lakehouse, de sorte qu'ils refl√©teront toujours les
derni√®res donn√©es.

# Exercice 5 : Ingestion de donn√©es et analyse avec Azure Databricks

1.  Acc√©dez √† votre lakehouse dans le service Power BI et s√©lectionnez
    **Obtenir des donn√©es**, puis Nouveau **pipeline de donn√©es**.

![Capture d'√©cran montrant comment acc√©der √† la nouvelle option de
pipeline de donn√©es √† partir de l'interface
utilisateur.](./media/image109.png)

1.  Dans l'invite **New pipeline**, entrez un nom pour le nouveau
    pipeline, puis s√©lectionnez **Create**. **IngestionDatapipeline01**

![](./media/image110.png)

2.  Pour cet exercice, s√©lectionnez l‚Äôexemple de donn√©es **NYC Taxi -
    Green** comme source de donn√©es.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image111.png)

3.  Sur l'√©cran d'aper√ßu, s√©lectionnez **Next**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image112.png)

4.  Pour Destination des donn√©es, s√©lectionnez le nom de la table que
    vous souhaitez utiliser pour stocker les donn√©es de la table OneLake
    Delta. Vous pouvez choisir une table existante ou en cr√©er une
    nouvelle. Pour les besoins de cet atelier, s√©lectionnez **load into
    new table**, puis s√©lectionnez **Next**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image113.png)

5.  Sur l'√©cran **Review + Save**, s√©lectionnez **Start data transfer
    immediately**, puis **Save** **+ Run**.

![Capture d'√©cran montrant comment entrer le nom de la
table.](./media/image114.png)

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image115.png)

6.  Une fois le travail termin√©, acc√©dez √† votre lakehouse et affichez
    la table delta r√©pertori√©e sous /Tables.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image116.png)

7.  Copiez le chemin d'acc√®s au syst√®me de fichiers Azure Blob (ABFS)
    dans votre table delta en cliquant avec le bouton droit sur le nom
    de la table dans la vue de l'Explorateur et en s√©lectionnant
    **Properties**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image117.png)

8.  Ouvrez votre notebook Azure Databricks et ex√©cutez le code.

olsPath = "**abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample**"

df=spark.read.format('delta').option("inferSchema","true").load(olsPath)

df.show(5)

*Remarque : Remplacez le chemin d'acc√®s du fichier en gras par celui que
vous avez copi√©.*

![](./media/image118.png)

9.  Mettez √† jour les donn√©es de la table Delta en modifiant une valeur
    de champ.

%sql

update delta.\`abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample\` set vendorID = 99999 where vendorID
= 1;

*Remarque : Remplacez le chemin d'acc√®s du fichier en gras par celui que
vous avez copi√©.*

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image119.png)

# Exercice 6 : Nettoyer les ressources

Dans cet exercice, vous allez apprendre √† cr√©er une architecture de
m√©daillon dans un lakehouse Microsoft Fabric.

Si vous avez termin√© d'explorer votre lakehouse, vous pouvez supprimer
l'espace de travail que vous avez cr√©√© pour cet exercice.

1.  S√©lectionnez votre espace de travail, le **Fabric Lakehouse
    Tutorial-29** dans le menu de navigation de gauche. Il ouvre la vue
    des √©l√©ments de l'espace de travail.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image120.png)

2.  S√©lectionnez l'ic√¥ne ***...*** sous le nom de l'espace de travail et
    s√©lectionnez **Workspace settings**.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image121.png)

3.  Faites d√©filer vers le bas et **Remove this workspace.**

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image122.png)

4.  Cliquez sur **Delete** dans l'avertissement qui s'affiche.

![Un fond blanc avec du texte noir Description g√©n√©r√©e
automatiquement](./media/image123.png)

5.  Attendez une notification indiquant que l'espace de travail a √©t√©
    supprim√© avant de passer au labo suivant.

![Une capture d'√©cran d'un ordinateur Description g√©n√©r√©e
automatiquement](./media/image124.png)

**R√©sum√©**¬†:

Cet atelier guide les participants dans la cr√©ation d'une architecture
de m√©daillon dans un lakehouse Microsoft Fabric √† l'aide de Notebook.
Les √©tapes cl√©s comprennent la mise en place d'un espace de travail, la
cr√©ation d'un lakehouse, le t√©l√©chargement des donn√©es dans la couche
bronze pour l'ingestion initiale, la transformation en une table Delta
argent√©e pour un traitement structur√©, l'affinement en tables Delta
dor√©es pour des analyses avanc√©es, l'exploration de mod√®les s√©mantiques
et la cr√©ation de relations entre les donn√©es pour une analyse
perspicace.

## 
