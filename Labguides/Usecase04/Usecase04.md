# ç”¨ä¾‹ 04ï¼šä½¿ç”¨ Azure Databricks å’Œ Microsoft Fabric è¿›è¡Œç°ä»£äº‘è§„æ¨¡åˆ†æ

**ä»‹ç»**

åœ¨æœ¬å®éªŒå®¤ä¸­ï¼Œä½ å°†æ¢ç´¢ Azure Databricks ä¸ Microsoft Fabric
çš„é›†æˆï¼Œä»¥ä½¿ç”¨ Medallion ä½“ç³»ç»“æ„åˆ›å»ºå’Œç®¡ç†æ¹–ä»“ä¸€ä½“ï¼Œåœ¨ Azure Data Lake
Storage ï¼ˆADLSï¼‰ Gen2 å¸æˆ·çš„å¸®åŠ©ä¸‹ä½¿ç”¨ Azure Databricks åˆ›å»º Delta
è¡¨ï¼Œä»¥åŠä½¿ç”¨ Azure Databricks
å¼•å…¥æ•°æ®ã€‚æœ¬å®è·µæŒ‡å—å°†å¼•å¯¼æ‚¨å®Œæˆåˆ›å»ºæ¹–ä»“ä¸€ä½“æ‰€éœ€çš„æ­¥éª¤ï¼Œå°†æ•°æ®åŠ è½½åˆ°å…¶ä¸­ï¼Œå¹¶æ¢ç´¢ç»“æ„åŒ–æ•°æ®å±‚ä»¥ä¿ƒè¿›é«˜æ•ˆçš„æ•°æ®åˆ†æå’ŒæŠ¥å‘Šã€‚

å¥–ç« æ¶æ„ç”±ä¸‰ä¸ªä¸åŒçš„å±‚ï¼ˆæˆ–åŒºåŸŸï¼‰ç»„æˆã€‚

- Bronzeï¼šä¹Ÿç§°ä¸ºåŸå§‹åŒºåŸŸï¼Œç¬¬ä¸€å±‚ä»¥å…¶åŸå§‹æ ¼å¼å­˜å‚¨æºæ•°æ®ã€‚æ­¤å±‚ä¸­çš„æ•°æ®é€šå¸¸æ˜¯ä»…é™„åŠ ä¸”ä¸å¯å˜çš„ã€‚

- Silverï¼šä¹Ÿç§°ä¸ºæ‰©å……åŒºåŸŸï¼Œæ­¤å±‚å­˜å‚¨æºè‡ª bronze
  å±‚çš„æ•°æ®ã€‚åŸå§‹æ•°æ®å·²ç»è¿‡æ¸…ç†å’Œæ ‡å‡†åŒ–ï¼Œç°åœ¨å…¶ç»“æ„ä¸ºè¡¨
  ï¼ˆè¡Œå’Œåˆ—ï¼‰ã€‚å®ƒè¿˜å¯èƒ½ä¸å…¶ä»–æ•°æ®é›†æˆï¼Œä»¥æä¾›æ‰€æœ‰ä¸šåŠ¡å®ä½“ï¼ˆå¦‚å®¢æˆ·ã€äº§å“ç­‰ï¼‰çš„ä¼ä¸šè§†å›¾ã€‚

- Goldï¼šä¹Ÿç§°ä¸ºç‰¹é€‰åŒºåŸŸï¼Œæ­¤æœ€åä¸€ä¸ªå±‚å­˜å‚¨æ¥è‡ªé“¶è‰²å±‚çš„æ•°æ®ã€‚æ•°æ®ç»è¿‡ä¼˜åŒ–ä»¥æ»¡è¶³ç‰¹å®šçš„ä¸‹æ¸¸ä¸šåŠ¡å’Œåˆ†æè¦æ±‚ã€‚è¡¨é€šå¸¸ç¬¦åˆæ˜Ÿå‹æ¶æ„è®¾è®¡ï¼Œè¯¥è®¾è®¡æ”¯æŒå¼€å‘é’ˆå¯¹æ€§èƒ½å’Œå¯ç”¨æ€§è¿›è¡Œä¼˜åŒ–çš„æ•°æ®æ¨¡å‹ã€‚

**ç›®æ ‡ï¼š**

- äº†è§£ Microsoft Fabric Lakehouse ä¸­çš„å¥–ç« ä½“ç³»ç»“æ„åŸåˆ™ã€‚

- ä½¿ç”¨å‹‹ç« å›¾å±‚ï¼ˆé“œç‰Œã€é“¶ç‰Œã€é‡‘ç‰Œï¼‰å®æ–½ç»“æ„åŒ–æ•°æ®ç®¡ç†æµç¨‹ã€‚

- å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºç»è¿‡éªŒè¯å’Œä¸°å¯Œçš„æ•°æ®ï¼Œä»¥è¿›è¡Œé«˜çº§åˆ†æå’ŒæŠ¥å‘Šã€‚

- äº†è§£æ•°æ®å®‰å…¨ã€CI/CD å’Œé«˜æ•ˆæ•°æ®æŸ¥è¯¢çš„æœ€ä½³å®è·µã€‚

- ä½¿ç”¨ OneLake æ–‡ä»¶èµ„æºç®¡ç†å™¨å°†æ•°æ®ä¸Šä¼ åˆ° OneLakeã€‚

- ä½¿ç”¨ Fabric ç¬”è®°æœ¬è¯»å– OneLake ä¸Šçš„æ•°æ®ï¼Œå¹¶ä½œä¸º Delta è¡¨å†™å›ã€‚

&nbsp;

- ä½¿ç”¨ Fabric ç¬”è®°æœ¬é€šè¿‡ Spark åˆ†æå’Œè½¬æ¢æ•°æ®ã€‚

- ä½¿ç”¨ SQL æŸ¥è¯¢ OneLake ä¸Šçš„ä¸€ä»½æ•°æ®å‰¯æœ¬ã€‚

- ä½¿ç”¨ Azure Databricks åœ¨ Azure Data Lake Storage ï¼ˆADLSï¼‰ Gen2
  å¸æˆ·ä¸­åˆ›å»º Delta è¡¨ã€‚

- åœ¨ ADLS ä¸­åˆ›å»º Delta è¡¨çš„ OneLake å¿«æ·æ–¹å¼ã€‚

- ä½¿ç”¨ Power BI é€šè¿‡ ADLS å¿«æ·æ–¹å¼åˆ†ææ•°æ®ã€‚

- ä½¿ç”¨ Azure Databricks è¯»å–å’Œä¿®æ”¹ OneLake ä¸­çš„ Delta è¡¨ã€‚

# ç»ƒä¹  1ï¼šå°†ç¤ºä¾‹æ•°æ®å¼•å…¥ Lakehouse

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæ‚¨å°†å®Œæˆä½¿ç”¨ Microsoft Fabric åˆ›å»º Lakehouse
å¹¶å°†æ•°æ®åŠ è½½åˆ°å…¶ä¸­çš„è¿‡ç¨‹ã€‚

Task:trail

## **ä»»åŠ¡ 1ï¼šåˆ›å»º Fabric å·¥ä½œåŒº**

åœ¨æ­¤ä»»åŠ¡ä¸­ï¼Œæ‚¨å°†åˆ›å»ºä¸€ä¸ª Fabric å·¥ä½œåŒºã€‚å·¥ä½œåŒºåŒ…å«æ­¤ Lakehouse
æ•™ç¨‹æ‰€éœ€çš„æ‰€æœ‰é¡¹ï¼Œå…¶ä¸­åŒ…æ‹¬
Lakehouseã€æ•°æ®æµã€æ•°æ®å·¥å‚ç®¡é“ã€ç¬”è®°æœ¬ã€Power BI æ•°æ®é›†å’ŒæŠ¥è¡¨ã€‚

1.  æ‰“å¼€æµè§ˆå™¨ï¼Œå¯¼èˆªåˆ°åœ°å€æ ï¼Œç„¶åé”®å…¥æˆ–ç²˜è´´ä»¥ä¸‹
    URLï¼š[https://app.fabric.microsoft.com/](https://app.fabric.microsoft.com/,)
    ç„¶åæŒ‰ **Enter** æŒ‰é’®ã€‚

> ![A search engine window with a red box Description automatically
> generated with medium confidence](./media/image1.png)

2.  è¿”å›åˆ° **Power BI** çª—å£ã€‚åœ¨ Power BI
    ä¸»é¡µçš„å·¦ä¾§å¯¼èˆªèœå•ä¸Šï¼Œå¯¼èˆªå¹¶å•å‡» **Workspaces**ã€‚

![](./media/image2.png)

3.  åœ¨ Workspaces çª—æ ¼ä¸­ï¼Œ å•å‡» **+ New workspace** æŒ‰é’®**ã€‚**

> ![](./media/image3.png)

4.  åœ¨å³ä¾§æ˜¾ç¤ºçš„ **Create a workspace**
    çª—æ ¼ä¸­ï¼Œè¾“å…¥ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯ï¼Œç„¶åå•å‡» **Apply** æŒ‰é’®ã€‚

[TABLE]

> ![](./media/image4.png)

![A screenshot of a computer Description automatically
generated](./media/image5.png)

5.  ç­‰å¾…éƒ¨ç½²å®Œæˆã€‚å®Œæˆéœ€è¦ 2-3 åˆ†é’Ÿã€‚

![](./media/image6.png)

## **ä»»åŠ¡ 2ï¼šåˆ›å»º Lakehouse**

1.  åœ¨ **Power BI Fabric Lakehouse Tutorial-XX**
    é¡µé¢ä¸­ï¼Œå•å‡»ä½äºå·¦ä¸‹è§’çš„ **Power BI** å›¾æ ‡ï¼Œç„¶åé€‰æ‹© **Data
    Engineering**ã€‚

> ![](./media/image7.png)

2.  åœ¨ **ynapseÂ Data EngineeringÂ Home** ä¸­ï¼Œé€‰æ‹© **Lakehouse** ä»¥åˆ›å»º
    Lakehouseã€‚

![](./media/image8.png)

![A screenshot of a computer Description automatically
generated](./media/image9.png)

3.  åœ¨ **New lakehouse** å¯¹è¯æ¡†ä¸­ï¼Œåœ¨ **Name** å­—æ®µä¸­è¾“å…¥
    **wwilakehouse**ï¼Œå•å‡» **Create** æŒ‰é’®å¹¶æ‰“å¼€æ–°çš„ Lakehouseã€‚

> **æ³¨æ„ï¼š**ç¡®ä¿åœ¨ **wwilakehouse** ä¹‹å‰åˆ é™¤ç©ºæ ¼ã€‚
>
> ![](./media/image10.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image11.png)
>
> ![](./media/image12.png)

4.  æ‚¨å°†çœ‹åˆ°ä¸€æ¡é€šçŸ¥ï¼ŒæŒ‡å‡º **Successfully created SQL endpoint**ã€‚

> ![](./media/image13.png)

# ç»ƒä¹  2ï¼š ä½¿ç”¨ Azure Databricks å®ç°å¥–ç« ä½“ç³»ç»“æ„

## **ä»»åŠ¡ 1ï¼šè®¾ç½®é’é“œå±‚**

1.  åœ¨ **wwilakehouse** é¡µé¢ä¸­ï¼Œ é€‰æ‹©æ–‡ä»¶æ—è¾¹çš„ More icon
    ï¼ˆ...ï¼‰ï¼Œç„¶åé€‰æ‹© **New subfolder**

![](./media/image14.png)

2.  åœ¨å¼¹å‡ºçª—å£ä¸­æä¾› Folder name ä½œä¸º **bronze**ï¼Œç„¶åé€‰æ‹© Createã€‚

![A screenshot of a computer Description automatically
generated](./media/image15.png)

3.  ç°åœ¨ï¼Œé€‰æ‹© é“œç‰Œæ–‡ä»¶æ—è¾¹çš„ æ›´å¤š å›¾æ ‡ ï¼ˆ...ï¼‰ï¼Œç„¶åé€‰æ‹© **Upload**
    ï¼Œç„¶åé€‰æ‹© **upload files**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image16.png)

4.  åœ¨ **upload file** çª—æ ¼ä¸­ï¼Œé€‰æ‹© **Upload file**
    å•é€‰æŒ‰é’®ã€‚å•å‡»**Browse button** å¹¶æµè§ˆåˆ° **Cï¼š\LabFiles**ï¼Œç„¶åé€‰æ‹©
    æ‰€éœ€çš„é”€å”®æ•°æ®æ–‡ä»¶ï¼ˆ2019ã€2020ã€2021ï¼‰æ–‡ä»¶ï¼Œç„¶åå•å‡» **Open** æŒ‰é’®ã€‚

ç„¶åï¼Œé€‰æ‹© **Upload** å°†æ–‡ä»¶ä¸Šä¼ åˆ° Lakehouse ä¸­çš„æ–°â€œbronzeâ€æ–‡ä»¶å¤¹ä¸­ã€‚

![A screenshot of a computer Description automatically
generated](./media/image17.png)

> ![A screenshot of a computer Description automatically
> generated](./media/image18.png)

5.  å•å‡» **bronze** æ–‡ä»¶å¤¹ä»¥éªŒè¯æ–‡ä»¶æ˜¯å¦å·²æˆåŠŸä¸Šä¼ ä»¥åŠæ–‡ä»¶æ˜¯å¦æ­£åœ¨åå°„ã€‚

![A screenshot of a computer Description automatically
generated](./media/image19.png)

# ç»ƒä¹  3ï¼š åœ¨ Medallion ä½“ç³»ç»“æ„ä¸­ä½¿ç”¨ Apache Spark è½¬æ¢æ•°æ®å¹¶ä½¿ç”¨ SQL è¿›è¡ŒæŸ¥è¯¢

## **ä»»åŠ¡ 1ï¼šè½¬æ¢æ•°æ®å¹¶åŠ è½½åˆ° silver Delta è¡¨**

åœ¨ **wwilakehouse** é¡µé¢ä¸­ï¼Œå¯¼èˆªå¹¶å•å‡»å‘½ä»¤æ ä¸­çš„ **Open notebook**
dropï¼Œç„¶åé€‰æ‹© **New notebook**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image20.png)

1.  é€‰æ‹©ç¬¬ä¸€ä¸ªå•å…ƒæ ¼ï¼ˆå½“å‰ä¸º*ä»£ç *å•å…ƒæ ¼ï¼‰ï¼Œç„¶ååœ¨å…¶å³ä¸Šè§’çš„åŠ¨æ€å·¥å…·æ ä¸­ï¼Œä½¿ç”¨
    **Mâ†“** æŒ‰é’® **convert the cell to aÂ markdownÂ cell.**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image21.png)

2.  å½“å•å…ƒæ ¼æ›´æ”¹ä¸º Markdown å•å…ƒæ ¼æ—¶ï¼Œå°†å‘ˆç°å®ƒåŒ…å«çš„æ–‡æœ¬ã€‚

![A screenshot of a computer Description automatically
generated](./media/image22.png)

3.  **ğŸ–‰** ä½¿ç”¨ ï¼ˆEditï¼‰
    æŒ‰é’®å°†å•å…ƒæ ¼åˆ‡æ¢åˆ°ç¼–è¾‘æ¨¡å¼ï¼Œæ›¿æ¢æ‰€æœ‰æ–‡æœ¬ï¼Œç„¶åæŒ‰å¦‚ä¸‹æ–¹å¼ä¿®æ”¹
    markdownï¼š

CodeCopy

\# Sales order data exploration

Use the code in this notebook to explore sales order data.

![A screenshot of a computer Description automatically
generated](./media/image23.png)

![A screenshot of a computer Description automatically
generated](./media/image24.png)

4.  å•å‡»ç¬”è®°æœ¬ä¸­å•å…ƒæ ¼å¤–éƒ¨çš„ä»»æ„ä½ç½®å¯åœæ­¢ç¼–è¾‘å®ƒå¹¶æŸ¥çœ‹å‘ˆç°çš„ Markdownã€‚

![A screenshot of a computer Description automatically
generated](./media/image25.png)

5.  ä½¿ç”¨å•å…ƒæ ¼è¾“å‡ºä¸‹æ–¹çš„ + Code å›¾æ ‡å°†æ–°çš„ä»£ç å•å…ƒæ ¼æ·»åŠ åˆ°ç¬”è®°æœ¬ä¸­ã€‚

![A screenshot of a computer Description automatically
generated](./media/image26.png)

6.  ç°åœ¨ï¼Œä½¿ç”¨ç¬”è®°æœ¬å°†æ•°æ®ä» bronze å±‚åŠ è½½åˆ° Spark DataFrame ä¸­ã€‚

é€‰æ‹© Notebook
ä¸­çš„ç°æœ‰å•å…ƒæ ¼ï¼Œå…¶ä¸­åŒ…å«ä¸€äº›ç®€å•çš„æ³¨é‡Šæ‰çš„ä»£ç ã€‚çªå‡ºæ˜¾ç¤ºå¹¶åˆ é™¤è¿™ä¸¤è¡Œ -
æ‚¨å°†ä¸éœ€è¦æ­¤ä»£ç ã€‚

*æ³¨æ„ï¼šç¬”è®°æœ¬ä½¿æ‚¨èƒ½å¤Ÿä½¿ç”¨å¤šç§è¯­è¨€è¿è¡Œä»£ç ï¼ŒåŒ…æ‹¬ Pythonã€Scala å’Œ
SQLã€‚åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨ PySpark å’Œ SQLã€‚æ‚¨è¿˜å¯ä»¥æ·»åŠ  Markdown
å•å…ƒæ ¼ä»¥æä¾›æ ¼å¼åŒ–çš„æ–‡æœ¬å’Œå›¾åƒæ¥è®°å½•æ‚¨çš„ä»£ç ã€‚*

ä¸ºæ­¤ï¼Œè¯·åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹ä»£ç ï¼Œç„¶åå•å‡» **Run**.

CodeCopy

from pyspark.sql.types import \*

\# Create the schema for the table

orderSchema = StructType(\[

StructField("SalesOrderNumber", StringType()),

StructField("SalesOrderLineNumber", IntegerType()),

StructField("OrderDate", DateType()),

StructField("CustomerName", StringType()),

StructField("Email", StringType()),

StructField("Item", StringType()),

StructField("Quantity", IntegerType()),

StructField("UnitPrice", FloatType()),

StructField("Tax", FloatType())

\])

\# Import all files from bronze folder of lakehouse

df = spark.read.format("csv").option("header",
"true").schema(orderSchema).load("Files/bronze/\*.csv")

\# Display the first 10 rows of the dataframe to preview your data

display(df.head(10))

![](./media/image27.png)

***æ³¨æ„**ï¼šç”±äºè¿™æ˜¯æ‚¨ç¬¬ä¸€æ¬¡åœ¨æ­¤ç¬”è®°æœ¬ä¸­è¿è¡Œä»»ä½• Spark ä»£ç ï¼Œå› æ­¤å¿…é¡»å¯åŠ¨
Spark
ä¼šè¯ã€‚è¿™æ„å‘³ç€ç¬¬ä¸€æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸€åˆ†é’Ÿå·¦å³æ‰èƒ½å®Œæˆã€‚åç»­è¿è¡Œä¼šæ›´å¿«ã€‚*

![A screenshot of a computer Description automatically
generated](./media/image28.png)

7.  æ‚¨è¿è¡Œçš„ä»£ç å°† bronze æ–‡ä»¶å¤¹ä¸­çš„ CSV æ–‡ä»¶ä¸­çš„æ•°æ®åŠ è½½åˆ° Spark
    æ•°æ®å¸§ä¸­ï¼Œç„¶åæ˜¾ç¤ºæ•°æ®å¸§çš„å‰å‡ è¡Œã€‚

> **å¤‡æ³¨**ï¼šæ‚¨å¯ä»¥é€šè¿‡é€‰æ‹© **...** Outputï¼ˆè¾“å‡ºï¼‰çª—æ ¼å·¦ä¸Šè§’çš„èœå•ã€‚

8.  ç°åœ¨ï¼Œæ‚¨å°† **add columns for data validation and cleanup**ï¼Œä½¿ç”¨
    PySpark æ•°æ®å¸§æ·»åŠ åˆ—å¹¶æ›´æ–°ä¸€äº›ç°æœ‰åˆ—çš„å€¼ã€‚ä½¿ç”¨ + æŒ‰é’® **add a new
    code block**ï¼Œå¹¶å°†ä»¥ä¸‹ä»£ç æ·»åŠ åˆ°å•å…ƒæ ¼ä¸­ï¼š

> CodeCopy
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> \# Add columns IsFlagged, CreatedTS and ModifiedTS
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \<
> '2019-08-01',True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()).withColumn("ModifiedTS",
> current_timestamp())
>
> \# Update CustomerName to "Unknown" if CustomerName null or empty
>
> df = df.withColumn("CustomerName", when((col("CustomerName").isNull()
> |
> (col("CustomerName")=="")),lit("Unknown")).otherwise(col("CustomerName")))
>
> ä»£ç çš„ç¬¬ä¸€è¡Œä» PySpark å¯¼å…¥å¿…è¦çš„å‡½æ•°ã€‚ç„¶åï¼Œæ‚¨å°†å‘ DataFrame
> æ·»åŠ æ–°åˆ—ï¼Œä»¥ä¾¿è·Ÿè¸ªæºæ–‡ä»¶åã€è®¢å•æ˜¯å¦åœ¨æ„Ÿå…´è¶£çš„ä¼šè®¡å¹´åº¦ä¹‹å‰è¢«æ ‡è®°ä¸º
> aï¼Œä»¥åŠè¡Œçš„åˆ›å»ºå’Œä¿®æ”¹æ—¶é—´ã€‚
>
> æœ€åï¼Œå¦‚æœ CustomerName åˆ—ä¸º null æˆ–ä¸ºç©ºï¼Œåˆ™å°†å…¶æ›´æ–°ä¸º â€œUnknownâ€ã€‚
>
> ç„¶åï¼Œä½¿ç”¨ \*\***â–· ï¼ˆ**Run cell*ï¼‰\*\* æŒ‰é’®*è¿è¡Œå•å…ƒæ ¼ä»¥æ‰§è¡Œä»£ç ã€‚

![A screenshot of a computer Description automatically
generated](./media/image29.png)

![A screenshot of a computer Description automatically
generated](./media/image30.png)

9.  æ¥ä¸‹æ¥ï¼Œæ‚¨å°† ä½¿ç”¨ Delta Lake æ ¼å¼å®šä¹‰ sales æ•°æ®åº“ä¸­
    **sales_silver**
    è¡¨çš„æ¶æ„ã€‚åˆ›å»ºä¸€ä¸ªæ–°çš„ä»£ç å—ï¼Œå¹¶å°†ä»¥ä¸‹ä»£ç æ·»åŠ åˆ°å•å…ƒæ ¼ä¸­ï¼š

> CodeCopy

from pyspark.sql.types import \*

from delta.tables import \*

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

Â  Â  StructField("SalesOrderNumber", StringType(), True),

Â  Â  StructField("SalesOrderLineNumber", IntegerType(), True),

Â  Â  StructField("OrderDate", DateType(), True),

Â  Â  StructField("CustomerName", StringType(), True),

Â  Â  StructField("Email", StringType(), True),

Â  Â  StructField("Item", StringType(), True),

Â  Â  StructField("Quantity", IntegerType(), True),

Â  Â  StructField("UnitPrice", FloatType(), True),

Â  Â  StructField("Tax", FloatType(), True),

Â  Â  StructField("FileName", StringType(), True),

Â  Â  StructField("IsFlagged", BooleanType(), True),

Â  Â  StructField("CreatedTS", TimestampType(), True),

Â  Â  StructField("ModifiedTS", TimestampType(), True)

\])

\# Create or replace the sales_silver table with the defined schema

DeltaTable.createIfNotExists(spark) \\

Â  Â  .tableName("wwilakehouse.sales_silver") \\

Â  Â  .addColumns(silver_table_schema) \\

Â  Â  .execute()

Â  Â 

10. ä½¿ç”¨ \*\***â–· ï¼ˆ**Run cell*ï¼‰\*\* æŒ‰é’®*è¿è¡Œå•å…ƒæ ¼ä»¥æ‰§è¡Œä»£ç ã€‚

11. é€‰æ‹© **...** ï¼Œç„¶åé€‰æ‹© **Refresh**ã€‚æ‚¨ç°åœ¨åº”è¯¥ä¼šçœ‹åˆ°åˆ—å‡ºäº†æ–°çš„
    **sales_silver** è¡¨ã€‚â–² ï¼ˆä¸‰è§’å½¢å›¾æ ‡ï¼‰è¡¨ç¤ºå®ƒæ˜¯ä¸€ä¸ª Delta è¡¨ã€‚

> **æ³¨æ„**ï¼šå¦‚æœæ‚¨æ²¡æœ‰çœ‹åˆ°æ–°è¡¨ï¼Œè¯·ç­‰å¾…å‡ ç§’é’Ÿï¼Œç„¶åå†æ¬¡é€‰æ‹©
> **Refresh**ï¼Œæˆ–åˆ·æ–°æ•´ä¸ªæµè§ˆå™¨é€‰é¡¹å¡ã€‚
>
> ![A screenshot of a computer Description automatically
> generated](./media/image31.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image32.png)

12. ç°åœ¨ï¼Œæ‚¨å°†å¯¹ Delta è¡¨æ‰§è¡Œ **upsert
    operation**ï¼Œæ ¹æ®ç‰¹å®šæ¡ä»¶æ›´æ–°ç°æœ‰è®°å½•ï¼Œå¹¶åœ¨æ‰¾ä¸åˆ°åŒ¹é…é¡¹æ—¶æ’å…¥æ–°è®°å½•ã€‚æ·»åŠ æ–°ä»£ç å—å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ï¼š

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> from delta.tables import \*
>
> \# Define the schema for the source data
>
> orderSchema = StructType(\[
>
> StructField("SalesOrderNumber", StringType(), True),
>
> StructField("SalesOrderLineNumber", IntegerType(), True),
>
> StructField("OrderDate", DateType(), True),
>
> StructField("CustomerName", StringType(), True),
>
> StructField("Email", StringType(), True),
>
> StructField("Item", StringType(), True),
>
> StructField("Quantity", IntegerType(), True),
>
> StructField("UnitPrice", FloatType(), True),
>
> StructField("Tax", FloatType(), True)
>
> \])
>
> \# Read data from the bronze folder into a DataFrame
>
> df = spark.read.format("csv").option("header",
> "true").schema(orderSchema).load("Files/bronze/\*.csv")
>
> \# Add additional columns
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \< '2019-08-01',
> True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()) \\
>
> .withColumn("ModifiedTS", current_timestamp()) \\
>
> .withColumn("CustomerName", when((col("CustomerName").isNull()) |
> (col("CustomerName") == ""),
> lit("Unknown")).otherwise(col("CustomerName")))
>
> \# Define the path to the Delta table
>
> deltaTablePath = "Tables/sales_silver"
>
> \# Create a DeltaTable object for the existing Delta table
>
> deltaTable = DeltaTable.forPath(spark, deltaTablePath)
>
> \# Perform the merge (upsert) operation
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> df.alias('updates'),
>
> 'silver.SalesOrderNumber = updates.SalesOrderNumber AND \\
>
> silver.OrderDate = updates.OrderDate AND \\
>
> silver.CustomerName = updates.CustomerName AND \\
>
> silver.Item = updates.Item'
>
> ) \\
>
> .whenMatchedUpdate(set = {
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "Email": "updates.Email",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .whenNotMatchedInsert(values = {
>
> "SalesOrderNumber": "updates.SalesOrderNumber",
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "OrderDate": "updates.OrderDate",
>
> "CustomerName": "updates.CustomerName",
>
> "Email": "updates.Email",
>
> "Item": "updates.Item",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "CreatedTS": "current_timestamp()",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .execute()

13. ä½¿ç”¨ \*\***â–· ï¼ˆ**Run cell*ï¼‰\*\* æŒ‰é’®*è¿è¡Œå•å…ƒæ ¼ä»¥æ‰§è¡Œä»£ç ã€‚

![A screenshot of a computer Description automatically
generated](./media/image33.png)

æ­¤ä½œéå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä½¿æ‚¨èƒ½å¤Ÿæ ¹æ®ç‰¹å®šåˆ—çš„å€¼æ›´æ–°è¡¨ä¸­çš„ç°æœ‰è®°å½•ï¼Œå¹¶åœ¨æ‰¾ä¸åˆ°åŒ¹é…é¡¹æ—¶æ’å…¥æ–°è®°å½•ã€‚å½“æ‚¨ä»å¯èƒ½åŒ…å«ç°æœ‰è®°å½•å’Œæ–°è®°å½•æ›´æ–°çš„æºç³»ç»ŸåŠ è½½æ•°æ®æ—¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¸è§è¦æ±‚ã€‚

ç°åœ¨ï¼Œæ‚¨çš„ silver delta è¡¨ä¸­æœ‰æ•°æ®ï¼Œå¯ä»¥è¿›è¡Œè¿›ä¸€æ­¥çš„è½¬æ¢å’Œå»ºæ¨¡ã€‚

æ‚¨å·²æˆåŠŸä» bronze å±‚è·å–æ•°æ®ï¼Œå¯¹å…¶è¿›è¡Œè½¬æ¢ï¼Œå¹¶å°†å…¶åŠ è½½åˆ° silver Delta
è¡¨ä¸­ã€‚ç°åœ¨ï¼Œæ‚¨å°†ä½¿ç”¨ä¸€ä¸ªæ–°çš„ç¬”è®°æœ¬è¿›ä¸€æ­¥è½¬æ¢æ•°æ®ï¼Œå°†å…¶å»ºæ¨¡ä¸ºæ˜Ÿå‹æ¶æ„ï¼Œå¹¶å°†å…¶åŠ è½½åˆ°é»„é‡‘
Delta è¡¨ä¸­ã€‚

*è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥åœ¨å•ä¸ªç¬”è®°æœ¬ä¸­å®Œæˆæ‰€æœ‰è¿™äº›ä½œï¼Œä½†å‡ºäºæœ¬ç»ƒä¹ çš„ç›®çš„ï¼Œæ‚¨å°†ä½¿ç”¨å•ç‹¬çš„ç¬”è®°æœ¬æ¥æ¼”ç¤ºå°†æ•°æ®ä»é“œçº§è½¬æ¢ä¸ºé“¶çº§ï¼Œç„¶åä»é“¶çº§è½¬æ¢ä¸ºé‡‘çº§çš„è¿‡ç¨‹ã€‚è¿™æœ‰åŠ©äºè°ƒè¯•ã€æ•…éšœæ’é™¤å’Œé‡ç”¨*ã€‚

## **ä»»åŠ¡ 2ï¼šå°†æ•°æ®åŠ è½½åˆ° Gold Delta è¡¨ä¸­**

1.  è¿”å›åˆ° Fabric Lakehouse Tutorial-29 ä¸»é¡µã€‚

> ![A screenshot of a computer Description automatically
> generated](./media/image34.png)

2.  é€‰æ‹© **wwilakehouseã€‚**

![A screenshot of a computer Description automatically
generated](./media/image35.png)

3.  åœ¨ Lakehouse Explorer çª—æ ¼ä¸­ï¼Œæ‚¨åº”è¯¥ä¼šçœ‹åˆ° **sales_silver** è¡¨åˆ—åœ¨
    èµ„æºç®¡ç†å™¨çª—æ ¼çš„ **Tables** éƒ¨åˆ†ä¸­ã€‚

![A screenshot of a computer Description automatically
generated](./media/image36.png)

4.  ç°åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªåä¸º **Transform data for Gold**
    çš„æ–°ç¬”è®°æœ¬ã€‚ä¸ºæ­¤ï¼Œè¯·å¯¼èˆªå¹¶å•å‡»å‘½ä»¤æ ä¸­çš„ **Open notebook**
    dropï¼Œç„¶åé€‰æ‹© **New notebook**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image37.png)

5.  åœ¨ç°æœ‰ä»£ç å—ä¸­ï¼Œåˆ é™¤æ ·æ¿æ–‡æœ¬å¹¶ **add the following
    codeâ€¯**ä»¥å°†æ•°æ®åŠ è½½åˆ°æ•°æ®å¸§å¹¶å¼€å§‹æ„å»ºæ˜Ÿå‹æ¶æ„ï¼Œç„¶åè¿è¡Œå®ƒï¼š

> CodeCopy

\# Load data to the dataframe as a starting point to create the gold
layer

df = spark.read.table("wwilakehouse.sales_silver")

\# Display the first few rows of the dataframe to verify the data

df.show()

![A screenshot of a computer Description automatically
generated](./media/image38.png)

6.  æ¥ä¸‹æ¥**ï¼ŒAdd a new code block**
    å¹¶ç²˜è´´ä»¥ä¸‹ä»£ç ä»¥åˆ›å»ºæ—¥æœŸç»´åº¦è¡¨å¹¶è¿è¡Œå®ƒï¼š

Â from pyspark.sql.types import \*

Â from delta.tables import\*

Â  Â 

Â # Define the schema for the dimdate_gold table

Â DeltaTable.createIfNotExists(spark) \\

Â  Â  Â .tableName("wwilakehouse.dimdate_gold") \\

Â  Â  Â .addColumn("OrderDate", DateType()) \\

Â  Â  Â .addColumn("Day", IntegerType()) \\

Â  Â  Â .addColumn("Month", IntegerType()) \\

Â  Â  Â .addColumn("Year", IntegerType()) \\

Â  Â  Â .addColumn("mmmyyyy", StringType()) \\

Â  Â  Â .addColumn("yyyymm", StringType()) \\

Â  Â  Â .execute()

![A screenshot of a computer Description automatically
generated](./media/image39.png)

**æ³¨æ„**ï¼šæ‚¨å¯ä»¥éšæ—¶è¿è¡Œ displayï¼ˆdfï¼‰
å‘½ä»¤æ¥æ£€æŸ¥æ‚¨çš„å·¥ä½œè¿›åº¦ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å°†è¿è¡Œ
'displayï¼ˆdfdimDate_goldï¼‰' æ¥æŸ¥çœ‹ dimDate_gold DataFrame çš„å†…å®¹ã€‚

7.  åœ¨æ–°ä»£ç å—ä¸­ï¼Œ**add and run the following
    code**ï¼Œä¸ºæ‚¨çš„æ—¥æœŸç»´åº¦åˆ›å»ºæ•°æ®å¸§ **dimdate_gold**ï¼š

> CodeCopy

from pyspark.sql.functions import col, dayofmonth, month, year,
date_format

Â  Â 

Â # Create dataframe for dimDate_gold

Â  Â 

dfdimDate_gold
=df.dropDuplicates(\["OrderDate"\]).select(col("OrderDate"), \\

Â  Â  Â  Â  Â dayofmonth("OrderDate").alias("Day"), \\

Â  Â  Â  Â  Â month("OrderDate").alias("Month"), \\

Â  Â  Â  Â  Â year("OrderDate").alias("Year"), \\

Â  Â  Â  Â  Â date_format(col("OrderDate"), "MMM-yyyy").alias("mmmyyyy"), \\

Â  Â  Â  Â  Â date_format(col("OrderDate"), "yyyyMM").alias("yyyymm"), \\

Â  Â  Â ).orderBy("OrderDate")

Â # Display the first 10 rows of the dataframe to preview your data

display(dfdimDate_gold.head(10))

![A screenshot of a computer Description automatically
generated](./media/image40.png)

![A screenshot of a computer Description automatically
generated](./media/image41.png)

8.  æ‚¨å°†ä»£ç åˆ†ç¦»åˆ°æ–°çš„ä»£ç å—ä¸­ï¼Œä»¥ä¾¿åœ¨è½¬æ¢æ•°æ®æ—¶å¯ä»¥ç†è§£å’Œè§‚å¯Ÿç¬”è®°æœ¬ä¸­å‘ç”Ÿçš„æƒ…å†µã€‚åœ¨å¦ä¸€ä¸ªæ–°ä»£ç å—ä¸­ï¼Œ**add
    and run the following code**ï¼Œä»¥ä¾¿åœ¨æ–°æ•°æ®ä¼ å…¥æ—¶æ›´æ–°æ—¥æœŸç»´åº¦ï¼š

> CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')
>
> dfUpdates = dfdimDate_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "OrderDate": "updates.OrderDate",
>
> "Day": "updates.Day",
>
> "Month": "updates.Month",
>
> "Year": "updates.Year",
>
> "mmmyyyy": "updates.mmmyyyy",
>
> "yyyymm": "yyyymm"
>
> }
>
> ) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image42.png)

> æ‚¨çš„æ—¥æœŸç»´åº¦å·²å…¨éƒ¨è®¾ç½®å®Œæ¯•ã€‚

![A screenshot of a computer Description automatically
generated](./media/image43.png)

## **ä»»åŠ¡ 3ï¼šåˆ›å»ºæ‚¨çš„å®¢æˆ·ç»´åº¦ã€‚**

1.  è¦æ„å»ºå®¢æˆ·ç»´åº¦è¡¨ï¼Œ**add a new code block**ï¼Œç²˜è´´å¹¶è¿è¡Œä»¥ä¸‹ä»£ç ï¼š

> CodeCopy

Â from pyspark.sql.types import \*

Â from delta.tables import \*

Â  Â 

Â # Create customer_gold dimension delta table

Â DeltaTable.createIfNotExists(spark) \\

Â  Â  Â .tableName("wwilakehouse.dimcustomer_gold") \\

Â  Â  Â .addColumn("CustomerName", StringType()) \\

Â  Â  Â .addColumn("Email", Â StringType()) \\

Â  Â  Â .addColumn("First", StringType()) \\

Â  Â  Â .addColumn("Last", StringType()) \\

Â  Â  Â .addColumn("CustomerID", LongType()) \\

Â  Â  Â .execute()

![A screenshot of a computer Description automatically
generated](./media/image44.png)

![A screenshot of a computer Description automatically
generated](./media/image45.png)

2.  åœ¨æ–°ä»£ç å—ä¸­ï¼Œ**add and run the following
    codeÂ **ä»¥åˆ é™¤é‡å¤çš„å®¢æˆ·ï¼Œé€‰æ‹©ç‰¹å®šåˆ—ï¼Œå¹¶æ‹†åˆ†â€œCustomerNameâ€åˆ—ä»¥åˆ›å»ºâ€œFirstâ€å’Œâ€œLastâ€åç§°åˆ—ï¼š

> CodeCopy
>
> from pyspark.sql.functions import col, split
>
> \# Create customer_silver dataframe
>
> dfdimCustomer_silver =
> df.dropDuplicates(\["CustomerName","Email"\]).select(col("CustomerName"),col("Email"))
> \\
>
> .withColumn("First",split(col("CustomerName"), " ").getItem(0)) \\
>
> .withColumn("Last",split(col("CustomerName"), " ").getItem(1))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimCustomer_silver.head(10))

![A screenshot of a computer Description automatically
generated](./media/image46.png)

åœ¨è¿™é‡Œï¼Œæ‚¨é€šè¿‡æ‰§è¡Œå„ç§è½¬æ¢ï¼ˆä¾‹å¦‚åˆ é™¤é‡å¤é¡¹ã€é€‰æ‹©ç‰¹å®šåˆ—ä»¥åŠæ‹†åˆ†â€œCustomerNameâ€åˆ—ä»¥åˆ›å»ºâ€œFirstâ€å’Œâ€œLastâ€åç§°åˆ—ï¼‰åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„
DataFrame dfdimCustomer_silverã€‚ç»“æœæ˜¯ä¸€ä¸ª
DataFrameï¼Œå…¶ä¸­åŒ…å«ç»è¿‡æ¸…ç†å’Œç»“æ„åŒ–çš„å®¢æˆ·æ•°æ®ï¼ŒåŒ…æ‹¬ä» â€œCustomerNameâ€
åˆ—ä¸­æå–çš„å•ç‹¬çš„ â€œFirstâ€ å’Œ â€œLastâ€ åç§°åˆ—ã€‚

![A screenshot of a computer Description automatically
generated](./media/image47.png)

3.  æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°† **create the ID column for our
    customers**ã€‚åœ¨æ–°ä»£ç å—ä¸­ï¼Œç²˜è´´å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col,
when, coalesce, max, lit

\# Read the existing data from the Delta table

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

\# Find the maximum CustomerID or use 0 if the table is empty

MAXCustomerID =
dfdimCustomer_temp.select(coalesce(max(col("CustomerID")),
lit(0)).alias("MAXCustomerID")).first()\[0\]

\# Assume dfdimCustomer_silver is your source DataFrame with new data

\# Here, we select only the new customers by doing a left anti join

dfdimCustomer_gold = dfdimCustomer_silver.join(

Â  Â  dfdimCustomer_temp,

Â  Â  (dfdimCustomer_silver.CustomerName ==
dfdimCustomer_temp.CustomerName) &

Â  Â  (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email),

Â  Â  "left_anti"

)

\# Add the CustomerID column with unique values starting from
MAXCustomerID + 1

dfdimCustomer_gold = dfdimCustomer_gold.withColumn(

Â  Â  "CustomerID",

Â  Â  monotonically_increasing_id() + MAXCustomerID + 1

)

\# Display the first 10 rows of the dataframe to preview your data

dfdimCustomer_gold.show(10)

![](./media/image48.png)

![A screenshot of a computer Description automatically
generated](./media/image49.png)

4.  ç°åœ¨ï¼Œæ‚¨å°†ç¡®ä¿æ‚¨çš„ customer è¡¨åœ¨æ–°æ•°æ®ä¼ å…¥æ—¶ä¿æŒæœ€æ–°ã€‚ **In a new
    code block**ï¼Œç²˜è´´å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

> CodeCopy

from delta.tables import DeltaTable

\# Define the Delta table path

deltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')

\# Use dfUpdates to refer to the DataFrame with new or updated records

dfUpdates = dfdimCustomer_gold

\# Perform the merge operation to update or insert new records

deltaTable.alias('silver') \\

Â  .merge(

Â  Â  dfUpdates.alias('updates'),

Â  Â  'silver.CustomerName = updates.CustomerName AND silver.Email =
updates.Email'

Â  ) \\

Â  .whenMatchedUpdate(set =

Â  Â  {

Â  Â  Â  "CustomerName": "updates.CustomerName",

Â  Â  Â  "Email": "updates.Email",

Â  Â  Â  "First": "updates.First",

Â  Â  Â  "Last": "updates.Last",

Â  Â  Â  "CustomerID": "updates.CustomerID"

Â  Â  }

Â  ) \\

Â  .whenNotMatchedInsert(values =

Â  Â  {

Â  Â  Â  "CustomerName": "updates.CustomerName",

Â  Â  Â  "Email": "updates.Email",

Â  Â  Â  "First": "updates.First",

Â  Â  Â  "Last": "updates.Last",

Â  Â  Â  "CustomerID": "updates.CustomerID"

Â  Â  }

Â  ) \\

Â  .execute()

![](./media/image50.png)

![A screenshot of a computer Description automatically
generated](./media/image51.png)

5.  ç°åœ¨ï¼Œæ‚¨å°†**repeat those steps to create your product
    dimension**ã€‚åœ¨æ–°ä»£ç å—ä¸­ï¼Œç²˜è´´å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.dimproduct_gold") \\
>
> .addColumn("ItemName", StringType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("ItemInfo", StringType()) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image52.png)

![A screenshot of a computer Description automatically
generated](./media/image53.png)

6.  **Add another code blockÂ **ä»¥åˆ›å»º **product_silver** DataFrameã€‚

> CodeCopy
>
> from pyspark.sql.functions import col, split, lit
>
> \# Create product_silver dataframe
>
> dfdimProduct_silver =
> df.dropDuplicates(\["Item"\]).select(col("Item")) \\
>
> .withColumn("ItemName",split(col("Item"), ", ").getItem(0)) \\
>
> .withColumn("ItemInfo",when((split(col("Item"), ",
> ").getItem(1).isNull() | (split(col("Item"), ",
> ").getItem(1)=="")),lit("")).otherwise(split(col("Item"), ",
> ").getItem(1)))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimProduct_silver.head(10))

![A screenshot of a computer Description automatically
generated](./media/image54.png)

![A screenshot of a computer Description automatically
generated](./media/image55.png)

7.  ç°åœ¨ï¼Œæ‚¨å°†ä¸º **dimProduct_gold table** åˆ›å»º
    IDã€‚å°†ä»¥ä¸‹è¯­æ³•æ·»åŠ åˆ°æ–°çš„ä»£ç å—å¹¶è¿è¡Œå®ƒï¼š

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col, lit,
max, coalesce

\#dfdimProduct_temp = dfdimProduct_silver

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

MAXProductID =
dfdimProduct_temp.select(coalesce(max(col("ItemID")),lit(0)).alias("MAXItemID")).first()\[0\]

dfdimProduct_gold =
dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName
== dfdimProduct_temp.ItemName) & (dfdimProduct_silver.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left_anti")

dfdimProduct_gold =
dfdimProduct_gold.withColumn("ItemID",monotonically_increasing_id() +
MAXProductID + 1)

\# Display the first 10 rows of the dataframe to preview your data

display(dfdimProduct_gold.head(10))

![A screenshot of a computer Description automatically
generated](./media/image56.png)

è¿™å°†æ ¹æ®è¡¨ä¸­çš„å½“å‰æ•°æ®è®¡ç®—ä¸‹ä¸€ä¸ªå¯ç”¨çš„äº§å“ IDï¼Œå°†è¿™äº›æ–° ID
åˆ†é…ç»™äº§å“ï¼Œç„¶åæ˜¾ç¤ºæ›´æ–°çš„äº§å“ä¿¡æ¯ã€‚

![A screenshot of a computer Description automatically
generated](./media/image57.png)

8.  ä¸å¯¹å…¶ä»–ç»´åº¦æ‰§è¡Œçš„ä½œç±»ä¼¼ï¼Œæ‚¨éœ€è¦ç¡®ä¿ product
    è¡¨åœ¨æ–°æ•°æ®ä¼ å…¥æ—¶ä¿æŒæœ€æ–°ã€‚ **In a new code
    block**ï¼Œç²˜è´´å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

CodeCopy

from delta.tables import \*

deltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')

dfUpdates = dfdimProduct_gold

deltaTable.alias('silver') \\

.merge(

dfUpdates.alias('updates'),

'silver.ItemName = updates.ItemName AND silver.ItemInfo =
updates.ItemInfo'

) \\

.whenMatchedUpdate(set =

{

}

) \\

.whenNotMatchedInsert(values =

{

"ItemName": "updates.ItemName",

"ItemInfo": "updates.ItemInfo",

"ItemID": "updates.ItemID"

}

) \\

.execute()

![A screenshot of a computer Description automatically
generated](./media/image58.png)

![A screenshot of a computer Description automatically
generated](./media/image59.png)

**ç°åœ¨ï¼Œæ‚¨å·²ç»æ„å»ºäº†ç»´åº¦ï¼Œæœ€åä¸€æ­¥æ˜¯åˆ›å»ºäº‹å®æ•°æ®è¡¨ã€‚**

9.  **In a new code block**ï¼Œç²˜è´´å¹¶è¿è¡Œä»¥ä¸‹ä»£ç ä»¥åˆ›å»º **fact table**ï¼š

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.factsales_gold") \\
>
> .addColumn("CustomerID", LongType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("OrderDate", DateType()) \\
>
> .addColumn("Quantity", IntegerType()) \\
>
> .addColumn("UnitPrice", FloatType()) \\
>
> .addColumn("Tax", FloatType()) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image60.png)

![A screenshot of a computer Description automatically
generated](./media/image61.png)

10. **In a new code block**ï¼Œç²˜è´´å¹¶è¿è¡Œä»¥ä¸‹ä»£ç ä»¥åˆ›å»ºæ–°çš„ **new
    dataframe**ï¼Œä»¥å°†é”€å”®æ•°æ®ä¸å®¢æˆ·å’Œäº§å“ä¿¡æ¯ï¼ˆåŒ…æ‹¬å®¢æˆ· IDã€å•†å“
    IDã€è®¢å•æ—¥æœŸã€æ•°é‡ã€å•ä»·å’Œç¨ï¼‰åˆå¹¶ï¼š

CodeCopy

from pyspark.sql import SparkSession

from pyspark.sql.functions import split, col, when, lit

from pyspark.sql.types import StructType, StructField, StringType,
IntegerType, DateType, FloatType, BooleanType, TimestampType

\# Initialize Spark session

spark = SparkSession.builder \\

Â  Â  .appName("DeltaTableUpsert") \\

Â  Â  .config("spark.sql.extensions",
"io.delta.sql.DeltaSparkSessionExtension") \\

Â  Â  .config("spark.sql.catalog.spark_catalog",
"org.apache.spark.sql.delta.catalog.DeltaCatalog") \\

Â  Â  .getOrCreate()

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

Â  Â  StructField("SalesOrderNumber", StringType(), True),

Â  Â  StructField("SalesOrderLineNumber", IntegerType(), True),

Â  Â  StructField("OrderDate", DateType(), True),

Â  Â  StructField("CustomerName", StringType(), True),

Â  Â  StructField("Email", StringType(), True),

Â  Â  StructField("Item", StringType(), True),

Â  Â  StructField("Quantity", IntegerType(), True),

Â  Â  StructField("UnitPrice", FloatType(), True),

Â  Â  StructField("Tax", FloatType(), True),

Â  Â  StructField("FileName", StringType(), True),

Â  Â  StructField("IsFlagged", BooleanType(), True),

Â  Â  StructField("CreatedTS", TimestampType(), True),

Â  Â  StructField("ModifiedTS", TimestampType(), True)

\])

\# Define the path to the Delta table (ensure this path is correct)

delta_table_path =
"abfss://\<container\>@\<storage-account\>.dfs.core.windows.net/path/to/wwilakehouse/sales_silver"

\# Create a DataFrame with the defined schema

empty_df = spark.createDataFrame(\[\], silver_table_schema)

\# Register the Delta table in the Metastore

spark.sql(f"""

Â  Â  CREATE TABLE IF NOT EXISTS wwilakehouse.sales_silver

Â  Â  USING DELTA

Â  Â  LOCATION '{delta_table_path}'

""")

\# Load data into DataFrame

df = spark.read.table("wwilakehouse.sales_silver")

\# Perform transformations on df

df = df.withColumn("ItemName", split(col("Item"), ", ").getItem(0)) \\

Â  Â  .withColumn("ItemInfo", when(

Â  Â  Â  Â  (split(col("Item"), ", ").getItem(1).isNull()) |
(split(col("Item"), ", ").getItem(1) == ""),

Â  Â  Â  Â  lit("")

Â  Â  ).otherwise(split(col("Item"), ", ").getItem(1)))

\# Load additional DataFrames for joins

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

\# Create Sales_gold dataframe

dffactSales_gold = df.alias("df1").join(dfdimCustomer_temp.alias("df2"),
(df.CustomerName == dfdimCustomer_temp.CustomerName) & (df.Email ==
dfdimCustomer_temp.Email), "left") \\

Â  Â  .join(dfdimProduct_temp.alias("df3"), (df.ItemName ==
dfdimProduct_temp.ItemName) & (df.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left") \\

Â  Â  .select(

Â  Â  Â  Â  col("df2.CustomerID"),

Â  Â  Â  Â  col("df3.ItemID"),

Â  Â  Â  Â  col("df1.OrderDate"),

Â  Â  Â  Â  col("df1.Quantity"),

Â  Â  Â  Â  col("df1.UnitPrice"),

Â  Â  Â  Â  col("df1.Tax")

Â  Â  ).orderBy(col("df1.OrderDate"), col("df2.CustomerID"),
col("df3.ItemID"))

\# Show the result

dffactSales_gold.show()

![A screenshot of a computer Description automatically
generated](./media/image62.png)

![A screenshot of a computer Description automatically
generated](./media/image63.png)

![A screenshot of a computer Description automatically
generated](./media/image64.png)

11. ç°åœ¨ï¼Œæ‚¨å°†é€šè¿‡åœ¨ **new code block**
    ä¸­è¿è¡Œä»¥ä¸‹ä»£ç æ¥ç¡®ä¿é”€å”®æ•°æ®ä¿æŒæœ€æ–°ï¼š

> CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')
>
> dfUpdates = dffactSales_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate AND silver.CustomerID =
> updates.CustomerID AND silver.ItemID = updates.ItemID'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "CustomerID": "updates.CustomerID",
>
> "ItemID": "updates.ItemID",
>
> "OrderDate": "updates.OrderDate",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax"
>
> }
>
> ) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image65.png)

åœ¨è¿™é‡Œï¼Œä½ å°†ä½¿ç”¨ Delta Lake çš„åˆå¹¶ä½œæ¥åŒæ­¥å’Œæ›´æ–° factsales_gold
è¡¨ï¼Œå…¶ä¸­åŒ…å«æ–°çš„é”€å”®æ•°æ®
ï¼ˆdffactSales_goldï¼‰ã€‚è¯¥ä½œå°†æ¯”è¾ƒç°æœ‰æ•°æ®ï¼ˆé“¶è‰²è¡¨ï¼‰å’Œæ–°æ•°æ®ï¼ˆæ›´æ–°
DataFrameï¼‰ä¹‹é—´çš„è®¢å•æ—¥æœŸã€å®¢æˆ· ID å’Œå•†å“
IDï¼Œå¹¶æ ¹æ®éœ€è¦æ›´æ–°åŒ¹é…è®°å½•å¹¶æ’å…¥æ–°è®°å½•ã€‚

![A screenshot of a computer Description automatically
generated](./media/image66.png)

ç°åœ¨ï¼Œæ‚¨æ‹¥æœ‰äº†ä¸€ä¸ªç»è¿‡æ•´ç†çš„å»ºæ¨¡ **gold** å±‚ï¼Œå¯ç”¨äºæŠ¥å‘Šå’Œåˆ†æã€‚

# ç»ƒä¹  4ï¼šåœ¨ Azure Databricks å’Œ Azure Data Lake Storage ï¼ˆADLSï¼‰ Gen 2 ä¹‹é—´å»ºç«‹è¿æ¥

ç°åœ¨ï¼Œè®©æˆ‘ä»¬åœ¨ Azure Data Lake Storage ï¼ˆADLSï¼‰ Gen2 å¸æˆ·çš„å¸®åŠ©ä¸‹ä½¿ç”¨
Azure Databricks åˆ›å»ºä¸€ä¸ª Delta è¡¨ã€‚ç„¶åï¼Œæ‚¨å°†åœ¨ ADLS ä¸­åˆ›å»ºæŒ‡å‘ Delta
è¡¨çš„ OneLake å¿«æ·æ–¹å¼ï¼Œå¹¶ä½¿ç”¨ Power BI é€šè¿‡ ADLS å¿«æ·æ–¹å¼åˆ†ææ•°æ®ã€‚

## **ä»»åŠ¡ 0ï¼šå…‘æ¢ Azure é€šè¡Œè¯å¹¶å¯ç”¨ Azure è®¢é˜…**

1.  æµè§ˆä»¥ä¸‹é“¾æ¥ ï¼ï¼https://www.microsoftazurepass.com/ï¼ï¼å¹¶å•å‡»
    **Start** æŒ‰é’®ã€‚

![](./media/image67.png)

2.  åœ¨ Microsoft ç™»å½•é¡µé¢ä¸Šï¼Œè¾“å…¥ **Tenant ID,ï¼Œ**ç„¶åå•å‡» **Next**ã€‚

![](./media/image68.png)

3.  åœ¨ä¸‹ä¸€é¡µä¸Šï¼Œè¾“å…¥æ‚¨çš„å¯†ç ï¼Œç„¶åå•å‡» **Sign In**ã€‚

![](./media/image69.png)

![A screenshot of a computer error Description automatically
generated](./media/image70.png)

4.  ç™»å½•åï¼Œåœ¨ Microsoft Azure é¡µé¢ä¸Šï¼Œå•å‡» **Confirm Microsoft
    Account** é€‰é¡¹å¡ã€‚

![](./media/image71.png)

5.  åœ¨ä¸‹ä¸€é¡µä¸Šï¼Œè¾“å…¥ä¿ƒé”€ä»£ç ã€Captcha å­—ç¬¦ï¼Œç„¶åå•å‡» **Submitã€‚**

![A screenshot of a computer Description automatically
generated](./media/image72.png)

![A screenshot of a computer error Description automatically
generated](./media/image73.png)

6.  åœ¨ Your profile ï¼ˆæ‚¨çš„ä¸ªäººèµ„æ–™ï¼‰
    é¡µé¢ä¸Šï¼Œè¾“å…¥æ‚¨çš„ä¸ªäººèµ„æ–™è¯¦ç»†ä¿¡æ¯ï¼Œç„¶åå•å‡» **Sign upã€‚**

7.  å¦‚æœå‡ºç°æç¤ºï¼Œè¯·æ³¨å†Œå¤šé‡èº«ä»½éªŒè¯ï¼Œç„¶åé€šè¿‡å¯¼èˆªåˆ°ä»¥ä¸‹é“¾æ¥ç™»å½•åˆ° Azure
    é—¨æˆ· !! <https://portal.azure.com/#home>!!

![](./media/image74.png)

8.  åœ¨æœç´¢æ ä¸Šï¼Œé”®å…¥ è®¢é˜… ï¼Œç„¶åå•å‡» Servicesä¸‹çš„ è®¢é˜… å›¾æ ‡ **ã€‚**

![A screenshot of a computer Description automatically
generated](./media/image75.png)

9.  æˆåŠŸå…‘æ¢ Azure Pass åï¼Œå°†ç”Ÿæˆè®¢é˜… IDã€‚

![](./media/image76.png)

## **ä»»åŠ¡ 1ï¼šåˆ›å»º Azure Data Storage å¸æˆ·**

1.  ä½¿ç”¨ Azure å‡­æ®ç™»å½•åˆ° Azure é—¨æˆ·ã€‚

2.  åœ¨ä¸»é¡µä¸Šï¼Œä»å·¦ä¾§é—¨æˆ·èœå•ä¸­é€‰æ‹© **storage accounts**
    ä»¥æ˜¾ç¤ºå­˜å‚¨å¸æˆ·åˆ—è¡¨ã€‚å¦‚æœé—¨æˆ·èœå•ä¸å¯è§ï¼Œè¯·é€‰æ‹©èœå•æŒ‰é’®å°†å…¶æ‰“å¼€ã€‚

![A screenshot of a computer Description automatically
generated](./media/image77.png)

3.  åœ¨ **Storage accounts** é¡µé¢ä¸Šï¼Œé€‰æ‹© **Create** ã€‚

![A screenshot of a computer Description automatically
generated](./media/image78.png)

4.  åœ¨ Basics ï¼ˆåŸºæœ¬ä¿¡æ¯ï¼‰
    é€‰é¡¹å¡ä¸Šï¼Œé€‰æ‹©èµ„æºç»„åï¼Œæä¾›å­˜å‚¨å¸æˆ·çš„åŸºæœ¬ä¿¡æ¯:

[TABLE]

å°†å…¶ä»–è®¾ç½®ä¿ç•™åŸæ ·ï¼Œç„¶åé€‰æ‹© **Review + create**
æ¥å—é»˜è®¤é€‰é¡¹ï¼Œç„¶åç»§ç»­éªŒè¯å’Œåˆ›å»ºå¸æˆ·ã€‚

æ³¨æ„ï¼šå¦‚æœå°šæœªåˆ›å»ºèµ„æºç»„ï¼Œå¯ä»¥å•å‡»â€œ**Create
new**â€ï¼Œç„¶åä¸ºå­˜å‚¨å¸æˆ·åˆ›å»ºæ–°èµ„æºã€‚

![](./media/image79.png)

![](./media/image80.png)

![A screenshot of a computer Description automatically
generated](./media/image81.png)

![A screenshot of a computer Description automatically
generated](./media/image82.png)

![A screenshot of a computer Description automatically
generated](./media/image83.png)

5.  å¯¼èˆªåˆ° **Review + create** é€‰é¡¹å¡æ—¶ï¼ŒAzure
    ä¼šå¯¹æ‰€é€‰çš„å­˜å‚¨å¸æˆ·è®¾ç½®è¿è¡ŒéªŒè¯ã€‚å¦‚æœéªŒè¯é€šè¿‡ï¼Œåˆ™å¯ä»¥ç»§ç»­åˆ›å»ºå­˜å‚¨å¸æˆ·ã€‚

å¦‚æœéªŒè¯å¤±è´¥ï¼Œåˆ™é—¨æˆ·ä¼šæŒ‡ç¤ºéœ€è¦ä¿®æ”¹å“ªäº›è®¾ç½®ã€‚

![A screenshot of a computer Description automatically
generated](./media/image84.png)

![A screenshot of a computer Description automatically
generated](./media/image85.png)

ç°åœ¨ï¼Œæ‚¨å·²æˆåŠŸåˆ›å»º Azure æ•°æ®å­˜å‚¨å¸æˆ·ã€‚

6.  é€šè¿‡æœç´¢é¡µé¢é¡¶éƒ¨çš„æœç´¢æ å¯¼èˆªåˆ°å­˜å‚¨å¸æˆ·é¡µé¢ï¼Œé€‰æ‹©æ–°åˆ›å»ºçš„å­˜å‚¨å¸æˆ·ã€‚

![A screenshot of a computer Description automatically
generated](./media/image86.png)

7.  åœ¨å­˜å‚¨å¸æˆ·é¡µé¢ä¸Šï¼Œå¯¼èˆªåˆ°å·¦ä¾§å¯¼èˆªçª—æ ¼ä¸­**Data storage**ä¸‹çš„
    **Containers**ï¼Œåˆ›å»ºä¸€ä¸ªåç§°ä¸º ï¼ï¼medalion1ï¼ï¼å¹¶å•å‡» **Create**
    æŒ‰é’®ã€‚

Â 

![A screenshot of a computer Description automatically
generated](./media/image87.png)

8.  ç°åœ¨å¯¼èˆªå› **storage account**é¡µé¢ï¼Œä» å·¦ä¾§å¯¼èˆªèœå•ä¸­é€‰æ‹©
    **Endpoints** ã€‚å‘ä¸‹æ»šåŠ¨å¹¶å¤åˆ¶ **Primary endpoint URL**
    å¹¶å°†å…¶ç²˜è´´åˆ°è®°äº‹æœ¬ä¸Šã€‚è¿™åœ¨åˆ›å»ºå¿«æ·æ–¹å¼æ—¶ä¼šå¾ˆæœ‰å¸®åŠ©ã€‚

![](./media/image88.png)

9.  åŒæ ·ï¼Œå¯¼èˆªåˆ°åŒä¸€å¯¼èˆªé¢æ¿ä¸Šçš„ **Access keys**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image89.png)

## **ä»»åŠ¡ 2ï¼šåˆ›å»º Delta è¡¨ï¼Œåˆ›å»ºå¿«æ·æ–¹å¼ï¼Œå¹¶åˆ†æ Lakehouse ä¸­çš„æ•°æ®**

1.  åœ¨ Lakehouse ä¸­ï¼Œé€‰æ‹©çœç•¥å· **ï¼ˆ...ï¼‰** ï¼Œç„¶åé€‰æ‹© **New
    shortcut**ã€‚

![](./media/image90.png)

2.  åœ¨ **New shortcut** å±å¹•ä¸­ï¼Œé€‰æ‹©â€œ**Azure Data Lake Storage
    Gen2**â€ç£è´´ã€‚

![Screenshot of the tile options in the New shortcut
screen.](./media/image91.png)

3.  æŒ‡å®šå¿«æ·æ–¹å¼çš„è¿æ¥è¯¦ç»†ä¿¡æ¯ï¼š

[TABLE]

4.  ç„¶åç‚¹å‡» **Next** ã€‚

![A screenshot of a computer Description automatically
generated](./media/image92.png)

5.  è¿™å°†ä¸æ‚¨çš„ Azure å­˜å‚¨å®¹å™¨å»ºç«‹é“¾æ¥ã€‚é€‰æ‹©å­˜å‚¨ï¼Œç„¶åé€‰æ‹© **Next**
    æŒ‰é’®ã€‚

![A screenshot of a computer Description automatically
generated](./media/image93.png)

![A screenshot of a computer Description automatically
generated](./media/image94.png)![A screenshot of a computer Description
automatically generated](./media/image95.png)

6.  å¯åŠ¨å‘å¯¼åï¼Œé€‰æ‹© **Files** å¹¶é€‰æ‹© **â€œ...â€œ** åœ¨ **bronze** æ¡£æ¡ˆä¸Šã€‚

![A screenshot of a computer Description automatically
generated](./media/image96.png)

7.  é€‰æ‹© **Load to tables** å’Œ **New table** ã€‚

![](./media/image97.png)

8.  åœ¨å¼¹å‡ºçª—å£ä¸­ï¼Œå°†è¡¨çš„åç§°è®¾ç½®ä¸º **bronze_01** å¹¶é€‰æ‹©æ–‡ä»¶ç±»å‹ä½œä¸º
    **parquet**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image98.png)

![A screenshot of a computer Description automatically
generated](./media/image99.png)

![A screenshot of a computer Description automatically
generated](./media/image100.png)

![A screenshot of a computer Description automatically
generated](./media/image101.png)

9.  æ–‡ä»¶ **bronze_01** ç°åœ¨åœ¨æ–‡ä»¶ä¸­å¯è§ã€‚

![A screenshot of a computer Description automatically
generated](./media/image102.png)

10. æ¥ä¸‹æ¥ï¼Œé€‰æ‹© **â€œ...â€œ** åœ¨ **bronze** æ¡£æ¡ˆä¸Šã€‚é€‰æ‹© **Load to tables**
    å’Œ **existing tableã€‚**

![A screenshot of a computer Description automatically
generated](./media/image103.png)

11. æä¾›ç°æœ‰è¡¨åä½œä¸º **dimcustomer_goldã€‚** é€‰æ‹©æ–‡ä»¶ç±»å‹ä½œä¸º
    **parquet**ï¼Œç„¶åé€‰æ‹© **loadã€‚**

![A screenshot of a computer Description automatically
generated](./media/image104.png)

![A screenshot of a computer Description automatically
generated](./media/image105.png)

## **ä»»åŠ¡ 3ï¼šåˆ›å»ºè¯­ä¹‰æ¨¡å‹ä½¿ç”¨é»„é‡‘å±‚åˆ›å»ºæŠ¥å‘Š**

åœ¨æ‚¨çš„å·¥ä½œåŒºä¸­ï¼Œæ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨é»„é‡‘å±‚åˆ›å»ºæŠ¥è¡¨å¹¶åˆ†ææ•°æ®ã€‚æ‚¨å¯ä»¥ç›´æ¥åœ¨å·¥ä½œåŒºä¸­è®¿é—®è¯­ä¹‰æ¨¡å‹ï¼Œä»¥åˆ›å»ºç”¨äºæŠ¥å‘Šçš„å…³ç³»å’Œåº¦é‡ã€‚

*è¯·æ³¨æ„ï¼Œæ‚¨ä¸èƒ½ä½¿ç”¨ åœ¨åˆ›å»º Lakehouse æ—¶è‡ªåŠ¨åˆ›å»ºçš„ **default semantic
modelÂ **ã€‚æ‚¨å¿…é¡»ä» Lakehouse Explorer
åˆ›å»ºä¸€ä¸ªæ–°çš„è¯­ä¹‰æ¨¡å‹ï¼Œå…¶ä¸­åŒ…å«æ‚¨åœ¨æ­¤å®éªŒå®¤ä¸­åˆ›å»ºçš„é»„é‡‘è¡¨ã€‚*

1.  åœ¨æ‚¨çš„å·¥ä½œåŒºä¸­ï¼Œå¯¼èˆªåˆ°æ‚¨çš„ **wwilakehouse** Lakehouseã€‚ç„¶åä»
    Lakehouse Explorer è§†å›¾çš„åŠŸèƒ½åŒºä¸­é€‰æ‹© **New semantic model**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image106.png)

2.  åœ¨å¼¹å‡ºçª—å£ä¸­ï¼Œå°†åç§° **DatabricksTutorial**
    åˆ†é…ç»™æ–°çš„è¯­ä¹‰æ¨¡å‹ï¼Œç„¶åé€‰æ‹©å·¥ä½œåŒºä½œä¸º **Fabric Lakehouse
    Tutorial-29**ã€‚

![](./media/image107.png)

3.  æ¥ä¸‹æ¥ï¼Œå‘ä¸‹æ»šåŠ¨å¹¶é€‰æ‹© all to include in your semantic
    modelï¼Œç„¶åé€‰æ‹© **Confirm**ã€‚

è¿™å°†åœ¨ Fabric ä¸­æ‰“å¼€è¯­ä¹‰æ¨¡å‹ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­åˆ›å»ºå…³ç³»å’Œåº¦é‡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

![A screenshot of a computer Description automatically
generated](./media/image108.png)

åœ¨è¿™é‡Œï¼Œæ‚¨æˆ–æ‚¨çš„æ•°æ®å›¢é˜Ÿçš„å…¶ä»–æˆå‘˜å¯ä»¥æ ¹æ® Lakehouse
ä¸­çš„æ•°æ®åˆ›å»ºæŠ¥å‘Šå’Œæ§åˆ¶é¢æ¿ã€‚è¿™äº›æŠ¥å‘Šå°†ç›´æ¥è¿æ¥åˆ° Lakehouse
çš„é»„é‡‘å±‚ï¼Œå› æ­¤å®ƒä»¬å°†å§‹ç»ˆåæ˜ æœ€æ–°æ•°æ®ã€‚

# ç»ƒä¹  5ï¼šä½¿ç”¨ Azure Databricks å¼•å…¥æ•°æ®å’Œåˆ†æ

1.  å¯¼èˆªåˆ° Power BI æœåŠ¡ä¸­çš„ Lakehouseï¼Œé€‰æ‹© **Get data**ï¼Œç„¶åé€‰æ‹©
    **New data pipeline**

![Screenshot showing how to navigate to new data pipeline option from
within the UI.](./media/image109.png)

2.  åœ¨ **New Pipeline** æç¤ºç¬¦ä¸­ï¼Œè¾“å…¥æ–°ç®¡é“çš„åç§°ï¼Œç„¶åé€‰æ‹© **Create**
    ã€‚ **IngestDatapipeline01**

![](./media/image110.png)

3.  åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œé€‰æ‹© **NYC Taxi - Green** æ ·æœ¬æ•°æ®ä½œä¸ºæ•°æ®æºã€‚

![A screenshot of a computer Description automatically
generated](./media/image111.png)

4.  åœ¨é¢„è§ˆå±å¹•ä¸Šï¼Œé€‰æ‹© **Next**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image112.png)

5.  å¯¹äº data destinationï¼Œé€‰æ‹©è¦ç”¨äºå­˜å‚¨ OneLake Delta
    è¡¨æ•°æ®çš„è¡¨çš„åç§°ã€‚æ‚¨å¯ä»¥é€‰æ‹©ç°æœ‰è¡¨æˆ–åˆ›å»ºæ–°è¡¨ã€‚å¯¹äºæ­¤å®éªŒå®¤ï¼Œè¯·é€‰æ‹©
    **Load into new table**ï¼Œç„¶åé€‰æ‹© **Next**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image113.png)

6.  åœ¨ **Review + Save** å±å¹•ä¸Šï¼Œé€‰æ‹© **â€¯Start data transfer
    immediately**ï¼Œç„¶åé€‰æ‹© **Save + Run**ã€‚

![Screenshot showing how to enter table name.](./media/image114.png)

![A screenshot of a computer Description automatically
generated](./media/image115.png)

7.  ä½œä¸šå®Œæˆåï¼Œå¯¼èˆªåˆ°æ‚¨çš„ Lakehouse å¹¶æŸ¥çœ‹ /Tables ä¸‹åˆ—å‡ºçš„å¢é‡è¡¨ã€‚

![A screenshot of a computer Description automatically
generated](./media/image116.png)

8.  é€šè¿‡å³é”®å•å‡»èµ„æºç®¡ç†å™¨è§†å›¾ä¸­çš„è¡¨åç§°å¹¶é€‰æ‹© **Properties**ï¼Œå°† Azure
    Blob æ–‡ä»¶ç³»ç»Ÿ ï¼ˆABFSï¼‰ è·¯å¾„å¤åˆ¶åˆ°å¢é‡è¡¨ã€‚

![A screenshot of a computer Description automatically
generated](./media/image117.png)

9.  æ‰“å¼€ Azure Databricks ç¬”è®°æœ¬å¹¶è¿è¡Œä»£ç ã€‚

olsPath = "**abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample**"

df=spark.read.format('delta').option("inferSchema","true").load(olsPath)

df.show(5)

*æ³¨æ„ï¼šå°†ç²—ä½“æ–‡ä»¶è·¯å¾„æ›¿æ¢ä¸ºæ‚¨å¤åˆ¶çš„æ–‡ä»¶è·¯å¾„ã€‚*

![](./media/image118.png)

10. é€šè¿‡æ›´æ”¹å­—æ®µå€¼æ¥æ›´æ–° Delta è¡¨æ•°æ®ã€‚

%sql

update delta.\`abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample\` set vendorID = 99999 where vendorID
= 1;

*æ³¨æ„ï¼šå°†ç²—ä½“æ–‡ä»¶è·¯å¾„æ›¿æ¢ä¸ºæ‚¨å¤åˆ¶çš„æ–‡ä»¶è·¯å¾„ã€‚*

![A screenshot of a computer Description automatically
generated](./media/image119.png)

# ç»ƒä¹  6ï¼šæ¸…ç†èµ„æº

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæ‚¨å­¦ä¹ äº†å¦‚ä½•åœ¨ Microsoft Fabric Lakehouse
ä¸­åˆ›å»ºå¥–ç« ä½“ç³»ç»“æ„ã€‚

å¦‚æœæ‚¨å·²å®Œæˆå¯¹ Lakehouse çš„æ¢ç´¢ï¼Œåˆ™å¯ä»¥åˆ é™¤ä¸ºæœ¬ç»ƒä¹ åˆ›å»ºçš„å·¥ä½œåŒºã€‚

1.  ä»å·¦ä¾§å¯¼èˆªèœå•ä¸­é€‰æ‹©æ‚¨çš„å·¥ä½œåŒº **Fabric Lakehouse
    Tutorial-29**ã€‚æ­¤æ—¶å°†æ‰“å¼€å·¥ä½œåŒºé¡¹è§†å›¾ã€‚

![A screenshot of a computer Description automatically
generated](./media/image120.png)

2.  é€‰æ‹© ***...*** é€‰é¡¹ï¼Œç„¶åé€‰æ‹© **Workspace settings**ã€‚

![A screenshot of a computer Description automatically
generated](./media/image121.png)

3.  å‘ä¸‹æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œç„¶å **Remove this workspace.**

![A screenshot of a computer Description automatically
generated](./media/image122.png)

4.  ç‚¹å‡» **Delete** åœ¨å¼¹å‡ºçš„è­¦å‘Šä¸­ã€‚

![A white background with black text Description automatically
generated](./media/image123.png)

5.  ç­‰å¾…å·¥ä½œåŒºå·²åˆ é™¤çš„é€šçŸ¥ï¼Œç„¶åå†ç»§ç»­è¿›è¡Œä¸‹ä¸€ä¸ªå®éªŒã€‚

![A screenshot of a computer Description automatically
generated](./media/image124.png)

**æ‘˜è¦**ï¼š

æ­¤å®éªŒå®¤æŒ‡å¯¼å‚ä¸è€…ä½¿ç”¨ç¬”è®°æœ¬åœ¨ Microsoft Fabric Lakehouse
ä¸­æ„å»ºå¥–ç« ä½“ç³»ç»“æ„ã€‚å…³é”®æ­¥éª¤åŒ…æ‹¬è®¾ç½®å·¥ä½œåŒºã€å»ºç«‹æ¹–ä»“ä¸€ä½“ã€å°†æ•°æ®ä¸Šä¼ åˆ°é’é“œå±‚ä»¥è¿›è¡Œåˆå§‹æ‘„å–ã€å°†å…¶è½¬æ¢ä¸ºé“¶ç‰ˆ
Delta è¡¨ä»¥è¿›è¡Œç»“æ„åŒ–å¤„ç†ã€è¿›ä¸€æ­¥ç»†åŒ–ä¸ºé»„é‡‘ Delta
è¡¨ä»¥è¿›è¡Œé«˜çº§åˆ†æã€æ¢ç´¢è¯­ä¹‰æ¨¡å‹ä»¥åŠåˆ›å»ºæ•°æ®å…³ç³»ä»¥è¿›è¡Œæ·±å…¥åˆ†æã€‚

## 
