# ì‚¬ìš© ì‚¬ë¡€ 04: Azure Databricks ë° Microsoft Fabricì„ ì‚¬ìš©í•œ ìµœì‹  í´ë¼ìš°ë“œ ê·œëª¨ ë¶„ì„

**ì†Œê°œ**

ì´ ë©ì—ì„œëŠ” Azure Databricksì™€ Microsoft Fabricì˜ í†µí•©ì„ í†µí•´ Medallion
ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Lakehouseë¥¼ ë§Œë“¤ê³  ê´€ë¦¬í•˜ê³ , Azure Databricksë¥¼
ì‚¬ìš©í•˜ì—¬ ADLS(Azure Data Lake Storage) Gen2 ê³„ì •ì˜ ë„ì›€ìœ¼ë¡œ ë¸íƒ€
í…Œì´ë¸”ì„ ë§Œë“¤ê³ , Azure Databricksë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë°©ë²•ì„
ì‚´í´ë´…ë‹ˆë‹¤. ì´ ì‹¤ìŠµ ê°€ì´ë“œëŠ” Lakehouseë¥¼ ë§Œë“¤ê³ , Lakehouseì— ë°ì´í„°ë¥¼
ë¡œë“œí•˜ê³ , êµ¬ì¡°í™”ëœ ë°ì´í„° ê³„ì¸µì„ íƒìƒ‰í•˜ì—¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ë¶„ì„ ë° ë³´ê³ ë¥¼
ìš©ì´í•˜ê²Œ í•˜ëŠ” ë° í•„ìš”í•œ ë‹¨ê³„ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.

ë©”ë‹¬ë¦¬ì˜¨ ì•„í‚¤í…ì²˜ëŠ” ì„¸ ê°œì˜ ê³ ìœ í•œ ê³„ì¸µ(ë˜ëŠ” ì˜ì—­)ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

- ë¸Œë¡ ì¦ˆ: ì›ì‹œ ì˜ì—­ì´ë¼ê³ ë„ í•˜ëŠ” ì´ ì²« ë²ˆì§¸ ê³„ì¸µì€ ì›ë³¸ ë°ì´í„°ë¥¼ ì›ë˜
  í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ì´ ê³„ì¸µì˜ ë°ì´í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¶”ê°€ ì „ìš©ì´ë©°
  ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

- ì‹¤ë²„: ë³´ê°•ëœ ì˜ì—­ì´ë¼ê³ ë„ í•˜ëŠ” ì´ ë ˆì´ì–´ëŠ” ë¸Œë¡ ì¦ˆ ë ˆì´ì–´ì—ì„œ ê°€ì ¸ì˜¨
  ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ì›ì‹œ ë°ì´í„°ëŠ” ì •ë¦¬ë˜ê³  í‘œì¤€í™”ë˜ì—ˆìœ¼ë©° ì´ì œ
  í…Œì´ë¸”(í–‰ ë° ì—´)ë¡œ êµ¬ì¡°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ ë‹¤ë¥¸ ë°ì´í„°ì™€ í†µí•©í•˜ì—¬ ê³ ê°,
  ì œí’ˆ ë“±ê³¼ ê°™ì€ ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ì—”í„°í‹°ì— ëŒ€í•œ ì—”í„°í”„ë¼ì´ì¦ˆ ë³´ê¸°ë¥¼ ì œê³µí• 
  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

- ê³¨ë“œ: íë ˆì´íŒ…ëœ ì˜ì—­ì´ë¼ê³ ë„ í•˜ëŠ” ì´ ìµœì¢… ë ˆì´ì–´ëŠ” ì‹¤ë²„ ë ˆì´ì–´ì—ì„œ
  ì†Œì‹±ëœ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ë°ì´í„°ëŠ” íŠ¹ì • ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë° ë¶„ì„
  ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•˜ë„ë¡ êµ¬ì²´í™”ë©ë‹ˆë‹¤. í…Œì´ë¸”ì€ ì¼ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ ë°
  ìœ ìš©ì„±ì— ìµœì í™”ëœ ë°ì´í„° ëª¨ë¸ì˜ ê°œë°œì„ ì§€ì›í•˜ëŠ” ìŠ¤íƒ€ ìŠ¤í‚¤ë§ˆ ë””ìì¸ì„
  ë”°ë¦…ë‹ˆë‹¤.

**ëª©í‘œ**:

- Microsoft Fabric Lakehouse ë‚´ì—ì„œ Medallion Architectureì˜ ì›ì¹™ì„
  ì´í•´í•©ë‹ˆë‹¤.

- ë©”ë‹¬ë¦¬ì˜¨ ë ˆì´ì–´(Bronze, Silver, Gold)ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„° ê´€ë¦¬
  í”„ë¡œì„¸ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

- ì›ì‹œ ë°ì´í„°ë¥¼ ê²€ì¦ë˜ê³  ë³´ê°•ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ì—¬ ê³ ê¸‰ ë¶„ì„ ë° ë³´ê³ ë¥¼
  ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ë°ì´í„° ë³´ì•ˆ, CI/CD ë° íš¨ìœ¨ì ì¸ ë°ì´í„° ì¿¼ë¦¬ë¥¼ ìœ„í•œ ëª¨ë²” ì‚¬ë¡€ë¥¼
  ì•Œì•„ë³´ì„¸ìš”.

&nbsp;

- OneLake íŒŒì¼ íƒìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ OneLakeì— ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.

- Fabric ë…¸íŠ¸ë¶ì„ ì‚¬ìš©í•˜ì—¬ OneLakeì—ì„œ ë°ì´í„°ë¥¼ ì½ê³  ë¸íƒ€ í…Œì´ë¸”ë¡œ ë‹¤ì‹œ
  ì”ë‹ˆë‹¤.

- Fabric ë…¸íŠ¸ë¶ì„ ì‚¬ìš©í•˜ì—¬ Sparkë¡œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.

- SQLì„ ì‚¬ìš©í•˜ì—¬ OneLakeì—ì„œ ë°ì´í„° ì‚¬ë³¸ í•˜ë‚˜ë¥¼ ì¿¼ë¦¬í•©ë‹ˆë‹¤.

- Azure Databricksë¥¼ ì‚¬ìš©í•˜ì—¬ ADLS(Azure Data Lake Storage) Gen2 ê³„ì •ì—
  ë¸íƒ€ í…Œì´ë¸”ì„ ë§Œë“­ë‹ˆë‹¤.

- ADLSì—ì„œ ë¸íƒ€ í…Œì´ë¸”ì— ëŒ€í•œ OneLake ë°”ë¡œ ê°€ê¸°ë¥¼ ë§Œë“­ë‹ˆë‹¤.

- Power BIë¥¼ ì‚¬ìš©í•˜ì—¬ ADLS ë°”ë¡œ ê°€ê¸°ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

- Azure Databricksë¥¼ ì‚¬ìš©í•˜ì—¬ OneLakeì—ì„œ ë¸íƒ€ í…Œì´ë¸”ì„ ì½ê³  ìˆ˜ì •í•©ë‹ˆë‹¤.

# ì—°ìŠµ 1: ìƒ˜í”Œ ë°ì´í„°ë¥¼ Lakehouseë¡œ ê°€ì ¸ì˜¤ê¸°

ì´ ì—°ìŠµì—ì„œëŠ” Microsoft Fabricì„ ì‚¬ìš©í•˜ì—¬ Lakehouseë¥¼ ë§Œë“¤ê³  ë°ì´í„°ë¥¼
ë¡œë“œí•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

ì‘ì—…:íŠ¸ë ˆì¼

## **ì‘ì—… 1: Fabric ì‘ì—… ì˜ì—­ ë§Œë“¤ê¸°**

ì´ ì‘ì—…ì—ì„œëŠ” Fabric ì‘ì—… ê³µê°„ì„ ë§Œë“­ë‹ˆë‹¤. ì‘ì—… ì˜ì—­ì—ëŠ” Lakehouse,
ë°ì´í„° íë¦„, ë°ì´í„° íŒ©í„°ë¦¬ íŒŒì´í”„ë¼ì¸, ë…¸íŠ¸ë¶, Power BI ë°ì´í„° ì„¸íŠ¸ ë°
ë³´ê³ ì„œë¥¼ í¬í•¨í•˜ì—¬ ì´ Lakehouse ììŠµì„œì— í•„ìš”í•œ ëª¨ë“  í•­ëª©ì´ í¬í•¨ë˜ì–´
ìˆìŠµë‹ˆë‹¤.

1.  ë¸Œë¼ìš°ì €ë¥¼ ì—´ê³  ì£¼ì†Œ í‘œì‹œì¤„ë¡œ ì´ë™í•œ ë‹¤ìŒ ë‹¤ìŒ URLì„ ì…ë ¥í•˜ê±°ë‚˜
    ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤:
    [https://app.fabric.microsoft.com/](https://app.fabric.microsoft.com/,)
    ê·¸ëŸ° ë‹¤ìŒ **Enter** ë²„íŠ¼ì„ ëˆ„ë¦…ë‹ˆë‹¤.

> ![A search engine window with a red box Description automatically
> generated with medium confidence](./media/image1.png)

2.  **Power BI ì°½**ìœ¼ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤. Power BI í™ˆí˜ì´ì§€ì˜ ì™¼ìª½ íƒìƒ‰
    ë©”ë‰´ì—ì„œ **ì‘ì—…** ì˜ì—­ì„ íƒìƒ‰í•˜ê³  í´ë¦­í•©ë‹ˆë‹¤.

![](./media/image2.png)

3.  ì‘ì—… ì˜ì—­ ì°½ì—ì„œ **+New workspace** ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤**.**

> ![](./media/image3.png)

4.  ì˜¤ë¥¸ìª½ì— í‘œì‹œë˜ëŠ” **Create a workspace** ì°½ì—ì„œ ë‹¤ìŒ ì„¸ë¶€ ì •ë³´ë¥¼
    ì…ë ¥í•˜ê³  **Apply** ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.

[TABLE]

> ![](./media/image4.png)ã…

![A screenshot of a computer Description automatically
generated](./media/image5.png)

5.  ë°°í¬ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤. ì™„ë£Œí•˜ëŠ” ë° 2-3ë¶„ì´ ê±¸ë¦½ë‹ˆë‹¤.

![](./media/image6.png)

## **ì‘ì—… 2: Lakehouse ë§Œë“¤ê¸°**

1.  In the **Power BI Fabric Lakehouse Tutorial-XX** page, click on the
    **Power BI** icon located at the bottom left and selectÂ **Data
    Engineering**. **Power BI Fabric Lakehouse Tutorial-XX** í˜ì´ì§€ì—ì„œ
    ì™¼ìª½ í•˜ë‹¨ì— ìˆëŠ” **Power BI** ì•„ì´ì½˜ì„ í´ë¦­í•˜ê³  **Data
    Engineering**ì„ ì„ íƒí•©ë‹ˆë‹¤.![](./media/image7.png)

2.  **Synapse Data Engineering í™ˆí˜ì´ì§€**ì—ì„œ **Lakehouse**ë¥¼ ì„ íƒí•˜ì—¬
    Lakehouseë¥¼ ë§Œë“­ë‹ˆë‹¤.

![](./media/image8.png)

![A screenshot of a computer Description automatically
generated](./media/image9.png)

3.  **New lakehouse**Â ëŒ€í™” ìƒìì—ì„œ **Name** í•„ë“œì— **wwilakehouse**ë¥¼
    ì…ë ¥í•˜ê³  **Create** ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìƒˆ Lakehouseë¥¼ ì—½ë‹ˆë‹¤.

> **ì°¸ê³ **: **wwilakehouse** ì•ì— ìˆëŠ” ê³µê°„ì„ ì œê±°í•˜ì„¸ìš”.
>
> ![](./media/image10.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image11.png)
>
> ![](./media/image12.png)

4.  **Successfully created SQL endpoint**.ë¼ëŠ” ì•Œë¦¼ì´ í‘œì‹œë©ë‹ˆë‹¤.

> ![](./media/image13.png)

# ì—°ìŠµ 2: Azure Databricksë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ë‹¬ë¦¬ì˜¨ ì•„í‚¤í…ì²˜ êµ¬í˜„

## **ì‘ì—… 1: ë¸Œë¡ ì¦ˆ ë ˆì´ì–´ ì„¤ì •**

1.  **wwilakehouse** í˜ì´ì§€ì—ì„œ íŒŒì¼ ì˜†ì— ìˆëŠ” ì¶”ê°€ ì•„ì´ì½˜ì„ ì„ íƒí•˜ê³ 
    **New subfolder**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

![](./media/image14.png)

2.  íŒì—…ì—ì„œ í´ë” ì´ë¦„ì„ **bronze**ë¡œ ì…ë ¥í•˜ê³  ë§Œë“¤ê¸°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image15.png)

3.  ì´ì œ ì²­ë™ íŒŒì¼ ì˜†ì— ìˆëŠ” ì¶”ê°€ ì•„ì´ì½˜ì„ ì„ íƒí•˜ê³  **Upload**ë¥¼ ì„ íƒí•œ
    ë‹¤ìŒ **íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤**.

![A screenshot of a computer Description automatically
generated](./media/image16.png)

4.  **upload file**Â  ì°½ì—ì„œ **Upload file**ë¼ë””ì˜¤ ë²„íŠ¼ì„ ì„ íƒí•©ë‹ˆë‹¤.
    **Browse button** ë²„íŠ¼ì„ í´ë¦­í•˜ê³  **C:\LabFiles**ë¡œ ì´ë™í•œ í›„,
    í•„ìš”í•œ íŒë§¤ ë°ì´í„° íŒŒì¼(2019, 2020, 2021)ì„ ì„ íƒí•˜ê³  **Open** ë²„íŠ¼ì„
    í´ë¦­í•©ë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ **Upload**ë¥¼ ì„ íƒí•˜ì—¬ Lakehouseì˜ ìƒˆ 'bronze' í´ë”ì— íŒŒì¼ì„
ì—…ë¡œë“œí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image17.png)

> ![A screenshot of a computer Description automatically
> generated](./media/image18.png)

5.  **Bronze**í´ë”ë¥¼ í´ë¦­í•˜ì—¬ íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆê³  íŒŒì¼ì´
    ë°˜ì˜ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image19.png)

# ì—°ìŠµ 3: Apache Sparkë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ë³€í™˜ ë° ë©”ë‹¬ë¦¬ì˜¨ ì•„í‚¤í…ì²˜ì—ì„œ SQLì„ ì‚¬ìš©í•œ ì¿¼ë¦¬

## **ì‘ì—… 1: ë°ì´í„° ë³€í™˜ ë° ì‹¤ë²„ ë¸íƒ€ í…Œì´ë¸”ì— ë¡œë“œ**

**wwilakehouse** í˜ì´ì§€ì—ì„œ ëª…ë ¹ ëª¨ìŒì— ìˆëŠ” **Open notebook**ë¥¼
íƒìƒ‰í•˜ì—¬ í´ë¦­í•œ ë‹¤ìŒ **New notebook**ì„ ì„ íƒí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image20.png)

1.  ì²« ë²ˆì§¸ ì…€(í˜„ì¬ëŠ” ì½”ë“œ ì…€)ì„ ì„ íƒí•œ ë‹¤ìŒ, ì˜¤ë¥¸ìª½ ìƒë‹¨ì˜ ë™ì  ë„êµ¬
    ëª¨ìŒì—ì„œ **Mâ†“** ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ ì…€ì„ ë§ˆí¬ë‹¤ìš´ ì…€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image21.png)

2.  ì…€ì´ ë§ˆí¬ë‹¤ìš´ ì…€ë¡œ ë³€ê²½ë˜ë©´ ì…€ì— í¬í•¨ëœ í…ìŠ¤íŠ¸ê°€ ë Œë”ë§ë©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image22.png)

3.  ğŸ–‰ (í¸ì§‘) ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ ì…€ì„ í¸ì§‘ ëª¨ë“œë¡œ ì „í™˜í•˜ê³ , ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼
    ë°”ê¾¼ í›„ ë‹¤ìŒê³¼ ê°™ì´ ë§ˆí¬ë‹¤ìš´ì„ ìˆ˜ì •í•©ë‹ˆë‹¤.

CodeCopy

\# Sales order data exploration

Use the code in this notebook to explore sales order data.

![A screenshot of a computer Description automatically
generated](./media/image23.png)

![A screenshot of a computer Description automatically
generated](./media/image24.png)

4.  ì…€ ë°–ì˜ ë…¸íŠ¸ë¶ ì•„ë¬´ ê³³ì´ë‚˜ í´ë¦­í•˜ë©´ í¸ì§‘ì„ ì¤‘ë‹¨í•˜ê³  ë Œë”ë§ëœ
    ë§ˆí¬ë‹¤ìš´ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image25.png)

5.  ì…€ ì¶œë ¥ ì•„ë˜ì— ìˆëŠ” + ì½”ë“œ ì•„ì´ì½˜ì„ ì‚¬ìš©í•˜ì—¬ ë…¸íŠ¸ë¶ì— ìƒˆë¡œìš´ ì½”ë“œ
    ì…€ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image26.png)

6.  ì´ì œ ë…¸íŠ¸ë¶ì„ ì‚¬ìš©í•˜ì—¬ ì²­ë™ ë ˆì´ì–´ì˜ ë°ì´í„°ë¥¼ Spark DataFrameìœ¼ë¡œ
    ë¡œë“œí•©ë‹ˆë‹¤.

ë…¸íŠ¸ë¶ì—ì„œ ê°„ë‹¨í•œ ì£¼ì„ ì²˜ë¦¬ëœ ì½”ë“œê°€ í¬í•¨ëœ ê¸°ì¡´ ì…€ì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ ë‘
ì¤„ì„ ê°•ì¡° í‘œì‹œí•˜ê³  ì‚­ì œí•©ë‹ˆë‹¤. ì´ ì½”ë“œëŠ” í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

*ì°¸ê³ : ë…¸íŠ¸ë¶ì„ ì‚¬ìš©í•˜ë©´ Python, Scala, SQL ë“± ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ ì½”ë“œë¥¼
ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì—°ìŠµì—ì„œëŠ” PySparkì™€ SQLì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´
ì…€ì„ ì¶”ê°€í•˜ì—¬ ì„œì‹ ìˆëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ì œê³µí•˜ì—¬ ì½”ë“œë¥¼ ë¬¸ì„œí™”í•  ìˆ˜ë„
ìˆìŠµë‹ˆë‹¤.*

ì´ë¥¼ ìœ„í•´ ë‹¤ìŒ ì½”ë“œë¥¼ ì…ë ¥í•˜ê³  **Run**ì„ í´ë¦­í•˜ì„¸ìš”.

CodeCopy

from pyspark.sql.types import \*

\# Create the schema for the table

orderSchema = StructType(\[

StructField("SalesOrderNumber", StringType()),

StructField("SalesOrderLineNumber", IntegerType()),

StructField("OrderDate", DateType()),

StructField("CustomerName", StringType()),

StructField("Email", StringType()),

StructField("Item", StringType()),

StructField("Quantity", IntegerType()),

StructField("UnitPrice", FloatType()),

StructField("Tax", FloatType())

\])

\# Import all files from bronze folder of lakehouse

df = spark.read.format("csv").option("header",
"true").schema(orderSchema).load("Files/bronze/\*.csv")

\# Display the first 10 rows of the dataframe to preview your data

display(df.head(10))

![](./media/image27.png)

***ì°¸ê³ :** ì´ ë…¸íŠ¸ë¶ì—ì„œ Spark ì½”ë“œë¥¼ ì²˜ìŒ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ë¯€ë¡œ Spark
ì„¸ì…˜ì„ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰, ì²« ë²ˆì§¸ ì‹¤í–‰ì€ ì™„ë£Œí•˜ëŠ” ë° 1ë¶„ ì •ë„ ê±¸ë¦´ ìˆ˜
ìˆìŠµë‹ˆë‹¤. ì´í›„ ì‹¤í–‰ì€ ë” ë¹¨ë¦¬ ì™„ë£Œë©ë‹ˆë‹¤.*

![A screenshot of a computer Description automatically
generated](./media/image28.png)

7.  ì‹¤í–‰í•œ ì½”ë“œëŠ” **bronze** í´ë”ì— ìˆëŠ” CSV íŒŒì¼ì˜ ë°ì´í„°ë¥¼ Spark
    ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë¡œë“œí•œ ë‹¤ìŒ, ë°ì´í„°í”„ë ˆì„ì˜ ì²˜ìŒ ëª‡ í–‰ì„
    í‘œì‹œí–ˆìŠµë‹ˆë‹¤.

> **ì°¸ê³ **: ì¶œë ¥ ì°½ì˜ ì™¼ìª½ ìƒë‹¨ì— ìˆëŠ” â€¦ ë©”ë‰´ë¥¼ ì„ íƒí•˜ë©´ ì…€ ì¶œë ¥ ë‚´ìš©ì„
> ì§€ìš°ê³ , ìˆ¨ê¸°ê³ , í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

8.  ì´ì œ PySpark ë°ì´í„°í”„ë ˆì„ì„ ì‚¬ìš©í•˜ì—¬ ì—´ì„ ì¶”ê°€í•˜ê³  ê¸°ì¡´ ì—´ ì¤‘ ì¼ë¶€ì˜
    ê°’ì„ ì—…ë°ì´íŠ¸í•˜ì—¬ **ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬ë¥¼ ìœ„í•œ ì—´ì„ ì¶”ê°€í•©ë‹ˆë‹¤.** +
    ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ **ìƒˆ ì½”ë“œ ë¸”ë¡ì„ ì¶”ê°€í•˜ê³ ** ë‹¤ìŒ ì½”ë“œë¥¼ ì…€ì—
    ì¶”ê°€í•©ë‹ˆë‹¤:

> CodeCopy
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> \# Add columns IsFlagged, CreatedTS and ModifiedTS
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \<
> '2019-08-01',True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()).withColumn("ModifiedTS",
> current_timestamp())
>
> \# Update CustomerName to "Unknown" if CustomerName null or empty
>
> df = df.withColumn("CustomerName", when((col("CustomerName").isNull()
> |
> (col("CustomerName")=="")),lit("Unknown")).otherwise(col("CustomerName")))
>
> ì½”ë“œì˜ ì²« ë²ˆì§¸ ì¤„ì€ PySparkì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ
> ë°ì´í„°í”„ë ˆì„ì— ìƒˆ ì—´ì„ ì¶”ê°€í•˜ì—¬ ì†ŒìŠ¤ íŒŒì¼ ì´ë¦„, ì£¼ë¬¸ì´ í•´ë‹¹ íšŒê³„ì—°ë„
> ì´ì „ì— í”Œë˜ê·¸ê°€ ì§€ì •ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€, ê·¸ë¦¬ê³  í–‰ì´ ìƒì„±ë˜ê³  ìˆ˜ì •ëœ ì‹œì ì„
> ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
>
> ë§ˆì§€ë§‰ìœ¼ë¡œ, CustomerName ì—´ì´ nullì´ê±°ë‚˜ ë¹„ì–´ ìˆìœ¼ë©´ í•´ë‹¹ ì—´ì„ "ì•Œ ìˆ˜
> ì—†ìŒ"ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
>
> ê·¸ëŸ° ë‹¤ìŒ \*\*â–· (*Run cell*)\*\* ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì½”ë“œë¥¼
> ì‹¤í–‰í•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image29.png)

![A screenshot of a computer Description automatically
generated](./media/image30.png)

9.  ë‹¤ìŒìœ¼ë¡œ, Delta Lake í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ sales ë°ì´í„°ë² ì´ìŠ¤ì˜
    **sales_silver** í…Œì´ë¸”ì— ëŒ€í•œ ìŠ¤í‚¤ë§ˆë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ìƒˆ ì½”ë“œ ë¸”ë¡ì„
    ë§Œë“¤ê³  ë‹¤ìŒ ì½”ë“œë¥¼ ì…€ì— ì¶”ê°€í•©ë‹ˆë‹¤:

> CodeCopy

from pyspark.sql.types import \*

from delta.tables import \*

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

Â  Â  StructField("SalesOrderNumber", StringType(), True),

Â  Â  StructField("SalesOrderLineNumber", IntegerType(), True),

Â  Â  StructField("OrderDate", DateType(), True),

Â  Â  StructField("CustomerName", StringType(), True),

Â  Â  StructField("Email", StringType(), True),

Â  Â  StructField("Item", StringType(), True),

Â  Â  StructField("Quantity", IntegerType(), True),

Â  Â  StructField("UnitPrice", FloatType(), True),

Â  Â  StructField("Tax", FloatType(), True),

Â  Â  StructField("FileName", StringType(), True),

Â  Â  StructField("IsFlagged", BooleanType(), True),

Â  Â  StructField("CreatedTS", TimestampType(), True),

Â  Â  StructField("ModifiedTS", TimestampType(), True)

\])

\# Create or replace the sales_silver table with the defined schema

DeltaTable.createIfNotExists(spark) \\

Â  Â  .tableName("wwilakehouse.sales_silver") \\

Â  Â  .addColumns(silver_table_schema) \\

Â  Â  .execute()

Â  Â 

10. **\*\*â–·**Â (*Run cell*)\*\*ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì½”ë“œë¥¼
    ì‹¤í–‰í•©ë‹ˆë‹¤.

11. Lakehouse íƒìƒ‰ê¸° ì°½ì˜ í…Œì´ë¸” ì„¹ì…˜ì—ì„œ **â€¦**ì„ ì„ íƒí•˜ê³  **Refreshë¥¼**
    ì„ íƒí•©ë‹ˆë‹¤. ì´ì œ ìƒˆ **sales_silver** í…Œì´ë¸”ì´ ë‚˜ì—´ë©ë‹ˆë‹¤. â–²(ì‚¼ê°í˜•
    ì•„ì´ì½˜)ëŠ” í•´ë‹¹ í…Œì´ë¸”ì´ ë¸íƒ€ í…Œì´ë¸”ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

> **ì°¸ê³ :** ìƒˆ í‘œê°€ ë³´ì´ì§€ ì•Šìœ¼ë©´ ëª‡ ì´ˆê°„ ê¸°ë‹¤ë¦° í›„ ë‹¤ì‹œ **Refresh**ì„
> ì„ íƒí•˜ê±°ë‚˜ ë¸Œë¼ìš°ì € íƒ­ ì „ì²´ë¥¼ ìƒˆë¡œ ê³ ì¹¨í•˜ì„¸ìš”.
>
> ![A screenshot of a computer Description automatically
> generated](./media/image31.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image32.png)

12. ì´ì œ ë¸íƒ€ í…Œì´ë¸”ì— ëŒ€í•´ upsert ì‘ì—…ì„ ìˆ˜í–‰í•˜ì—¬ íŠ¹ì • ì¡°ê±´ì— ë”°ë¼ ê¸°ì¡´
    ë ˆì½”ë“œë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œê°€ ì—†ìœ¼ë©´ ìƒˆ ë ˆì½”ë“œë¥¼
    ì‚½ì…í•©ë‹ˆë‹¤. ìƒˆ ì½”ë“œ ë¸”ë¡ì„ ì¶”ê°€í•˜ê³  ë‹¤ìŒ ì½”ë“œë¥¼ ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤:

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import when, lit, col, current_timestamp,
> input_file_name
>
> from delta.tables import \*
>
> \# Define the schema for the source data
>
> orderSchema = StructType(\[
>
> StructField("SalesOrderNumber", StringType(), True),
>
> StructField("SalesOrderLineNumber", IntegerType(), True),
>
> StructField("OrderDate", DateType(), True),
>
> StructField("CustomerName", StringType(), True),
>
> StructField("Email", StringType(), True),
>
> StructField("Item", StringType(), True),
>
> StructField("Quantity", IntegerType(), True),
>
> StructField("UnitPrice", FloatType(), True),
>
> StructField("Tax", FloatType(), True)
>
> \])
>
> \# Read data from the bronze folder into a DataFrame
>
> df = spark.read.format("csv").option("header",
> "true").schema(orderSchema).load("Files/bronze/\*.csv")
>
> \# Add additional columns
>
> df = df.withColumn("FileName", input_file_name()) \\
>
> .withColumn("IsFlagged", when(col("OrderDate") \< '2019-08-01',
> True).otherwise(False)) \\
>
> .withColumn("CreatedTS", current_timestamp()) \\
>
> .withColumn("ModifiedTS", current_timestamp()) \\
>
> .withColumn("CustomerName", when((col("CustomerName").isNull()) |
> (col("CustomerName") == ""),
> lit("Unknown")).otherwise(col("CustomerName")))
>
> \# Define the path to the Delta table
>
> deltaTablePath = "Tables/sales_silver"
>
> \# Create a DeltaTable object for the existing Delta table
>
> deltaTable = DeltaTable.forPath(spark, deltaTablePath)
>
> \# Perform the merge (upsert) operation
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> df.alias('updates'),
>
> 'silver.SalesOrderNumber = updates.SalesOrderNumber AND \\
>
> silver.OrderDate = updates.OrderDate AND \\
>
> silver.CustomerName = updates.CustomerName AND \\
>
> silver.Item = updates.Item'
>
> ) \\
>
> .whenMatchedUpdate(set = {
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "Email": "updates.Email",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .whenNotMatchedInsert(values = {
>
> "SalesOrderNumber": "updates.SalesOrderNumber",
>
> "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
>
> "OrderDate": "updates.OrderDate",
>
> "CustomerName": "updates.CustomerName",
>
> "Email": "updates.Email",
>
> "Item": "updates.Item",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax",
>
> "FileName": "updates.FileName",
>
> "IsFlagged": "updates.IsFlagged",
>
> "CreatedTS": "current_timestamp()",
>
> "ModifiedTS": "current_timestamp()"
>
> }) \\
>
> .execute()

13. **\*\*â–·**Â (*Run cell*)\*\*ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì½”ë“œë¥¼
    ì‹¤í–‰í•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image33.png)

ì´ ì‘ì—…ì€ íŠ¹ì • ì—´ì˜ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ í…Œì´ë¸”ì˜ ê¸°ì¡´ ë ˆì½”ë“œë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ ,
ì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œê°€ ì—†ìœ¼ë©´ ìƒˆ ë ˆì½”ë“œë¥¼ ì‚½ì…í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì¤‘ìš”í•©ë‹ˆë‹¤.
ì´ëŠ” ê¸°ì¡´ ë ˆì½”ë“œì™€ ìƒˆ ë ˆì½”ë“œì— ëŒ€í•œ ì—…ë°ì´íŠ¸ê°€ í¬í•¨ëœ ì†ŒìŠ¤ ì‹œìŠ¤í…œì—ì„œ
ë°ì´í„°ë¥¼ ë¡œë“œí•  ë•Œ ì¼ë°˜ì ìœ¼ë¡œ í•„ìš”í•œ ì‘ì—…ì…ë‹ˆë‹¤.

ì´ì œ ì‹¤ë²„ ë¸íƒ€ í…Œì´ë¸”ì— ì¶”ê°€ ë³€í™˜ ë° ëª¨ë¸ë§ì„ ìœ„í•œ ë°ì´í„°ê°€
ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.

ë¸Œë¡ ì¦ˆ ë ˆì´ì–´ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë³€í™˜í•˜ê³  ì‹¤ë²„ ë¸íƒ€ í…Œì´ë¸”ì— ë¡œë“œí•˜ëŠ”
ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ìƒˆ ë…¸íŠ¸ë¶ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼
ì¶”ê°€ë¡œ ë³€í™˜í•˜ê³ , ìŠ¤íƒ€ ìŠ¤í‚¤ë§ˆë¡œ ëª¨ë¸ë§í•˜ê³ , ê³¨ë“œ ë¸íƒ€ í…Œì´ë¸”ì— ë¡œë“œí•´
ë³´ê² ìŠµë‹ˆë‹¤.

*ì´ ëª¨ë“  ì‘ì—…ì„ í•˜ë‚˜ì˜ ë…¸íŠ¸ë¶ì—ì„œ ìˆ˜í–‰í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì´ ì—°ìŠµì—ì„œëŠ”
ë¸Œë¡ ì¦ˆì—ì„œ ì‹¤ë²„ë¡œ, ê·¸ë¦¬ê³  ì‹¤ë²„ì—ì„œ ê³¨ë“œë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„
ë³´ì—¬ì£¼ê¸° ìœ„í•´ ë³„ë„ì˜ ë…¸íŠ¸ë¶ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ë””ë²„ê¹…, ë¬¸ì œ í•´ê²° ë°
ì¬ì‚¬ìš©ì— ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.*

## **ì‘ì—… 2: ê³¨ë“œ ë¸íƒ€ í…Œì´ë¸”ì— ë°ì´í„° ë¡œë“œ**

1.  Fabric Lakehouse Tutorial-29 í™ˆí˜ì´ì§€ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.

> ![A screenshot of a computer Description automatically
> generated](./media/image34.png)

2.  **Wwilakehouse ì„ íƒí•©ë‹ˆë‹¤.**

![A screenshot of a computer Description automatically
generated](./media/image35.png)

3.  Lakehouseíƒìƒ‰ê¸° ì°½ì—ì„œ **Tables** ì„¹ì…˜ì— **sales_silver**Â í…Œì´ë¸”ì´
    ë‚˜ì—´ë˜ì–´ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image36.png)

4.  ì´ì œ **Transform data for Gold**ë¼ëŠ” ìƒˆ ë…¸íŠ¸ë¶ì„ ë§Œë“­ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´
    ëª…ë ¹ì¤„ì—ì„œ **Open notebook** ë“œë¡­ë‹¤ìš´ì„ ì°¾ì•„ í´ë¦­í•œ ë‹¤ìŒ **New
    notebook**ì„ ì„ íƒí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image37.png)

5.  ê¸°ì¡´ ì½”ë“œ ë¸”ë¡ì—ì„œ ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ í…ìŠ¤íŠ¸ë¥¼ ì œê±°í•˜ê³  **ë‹¤ìŒ ì½”ë“œë¥¼
    ì¶”ê°€í•˜ì—¬** ë°ì´í„° í”„ë ˆì„ì— ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ìŠ¤íƒ€ ìŠ¤í‚¤ë§ˆë¥¼ êµ¬ì¶•í•œ
    ë‹¤ìŒ ì‹¤í–‰í•©ë‹ˆë‹¤:

> CodeCopy

\# Load data to the dataframe as a starting point to create the gold
layer

df = spark.read.table("wwilakehouse.sales_silver")

\# Display the first few rows of the dataframe to verify the data

df.show()

![A screenshot of a computer Description automatically
generated](./media/image38.png)

6.  ë‹¤ìŒìœ¼ë¡œ, **ìƒˆ ì½”ë“œ ë¸”ë¡ì„ ì¶”ê°€í•˜ê³ ** ë‹¤ìŒ ì½”ë“œë¥¼ ë¶™ì—¬ë„£ì–´ ë‚ ì§œ ì°¨ì›
    í…Œì´ë¸”ì„ ë§Œë“¤ê³  ì‹¤í–‰í•©ë‹ˆë‹¤:

Â from pyspark.sql.types import \*

Â from delta.tables import\*

Â  Â 

Â # Define the schema for the dimdate_gold table

Â DeltaTable.createIfNotExists(spark) \\

Â  Â  Â .tableName("wwilakehouse.dimdate_gold") \\

Â  Â  Â .addColumn("OrderDate", DateType()) \\

Â  Â  Â .addColumn("Day", IntegerType()) \\

Â  Â  Â .addColumn("Month", IntegerType()) \\

Â  Â  Â .addColumn("Year", IntegerType()) \\

Â  Â  Â .addColumn("mmmyyyy", StringType()) \\

Â  Â  Â .addColumn("yyyymm", StringType()) \\

Â  Â  Â .execute()

![A screenshot of a computer Description automatically
generated](./media/image39.png)

**ì°¸ê³ **: ì–¸ì œë“ ì§€ display(df)ëª…ë ¹ì„ ì‹¤í–‰í•˜ì—¬ ì‘ì—… ì§„í–‰ ìƒí™©ì„ í™•ì¸í•  ìˆ˜
ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°, 'display(dfdimDate_gold)' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì—¬
dimDate_gold ë°ì´í„°í”„ë ˆì„ì˜ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.

7.  ìƒˆ ì½”ë“œ ë¸”ë¡ì—ì„œ **ë‹¤ìŒ ì½”ë“œë¥¼ ì¶”ê°€í•˜ê³  ì‹¤í–‰í•˜ì—¬** ë‚ ì§œ ì°¨ì›
    **dimdate_gold**ì— ëŒ€í•œ ë°ì´í„° í”„ë ˆì„ì„ ë§Œë“­ë‹ˆë‹¤:

> CodeCopy

from pyspark.sql.functions import col, dayofmonth, month, year,
date_format

Â  Â 

Â # Create dataframe for dimDate_gold

Â  Â 

dfdimDate_gold
=df.dropDuplicates(\["OrderDate"\]).select(col("OrderDate"), \\

Â  Â  Â  Â  Â dayofmonth("OrderDate").alias("Day"), \\

Â  Â  Â  Â  Â month("OrderDate").alias("Month"), \\

Â  Â  Â  Â  Â year("OrderDate").alias("Year"), \\

Â  Â  Â  Â  Â date_format(col("OrderDate"), "MMM-yyyy").alias("mmmyyyy"), \\

Â  Â  Â  Â  Â date_format(col("OrderDate"), "yyyyMM").alias("yyyymm"), \\

Â  Â  Â ).orderBy("OrderDate")

Â # Display the first 10 rows of the dataframe to preview your data

display(dfdimDate_gold.head(10))

![A screenshot of a computer Description automatically
generated](./media/image40.png)

![A screenshot of a computer Description automatically
generated](./media/image41.png)

8.  ë°ì´í„°ë¥¼ ë³€í™˜í•  ë•Œ ë…¸íŠ¸ë¶ì—ì„œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì´í•´í•˜ê³  í™•ì¸í• 
    ìˆ˜ ìˆë„ë¡ ì½”ë“œë¥¼ ìƒˆ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. ë˜ ë‹¤ë¥¸ ìƒˆ ì½”ë“œ ë¸”ë¡ì—
    **ë‹¤ìŒ ì½”ë“œë¥¼ ì¶”ê°€í•˜ê³  ì‹¤í–‰í•˜ì—¬** ìƒˆ ë°ì´í„°ê°€ ë“¤ì–´ì˜¬ ë•Œ ë‚ ì§œ ì°¨ì›ì„
    ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:

> CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')
>
> dfUpdates = dfdimDate_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "OrderDate": "updates.OrderDate",
>
> "Day": "updates.Day",
>
> "Month": "updates.Month",
>
> "Year": "updates.Year",
>
> "mmmyyyy": "updates.mmmyyyy",
>
> "yyyymm": "yyyymm"
>
> }
>
> ) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image42.png)

> ë‚ ì§œ ì°¨ì›ì´ ëª¨ë‘ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image43.png)

## **ì‘ì—…3: ê³ ê° ì°¨ì› ë§Œë“¤ê¸°**

1.  ê³ ê° ì°¨ì› í…Œì´ë¸”ì„ ì‘ì„±í•˜ë ¤ë©´ **ìƒˆ ì½”ë“œ ë¸”ë¡ì„ ì¶”ê°€í•˜ê³ ** ë‹¤ìŒ
    ì½”ë“œë¥¼ ë¶™ì—¬ë„£ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.

> CodeCopy

Â from pyspark.sql.types import \*

Â from delta.tables import \*

Â  Â 

Â # Create customer_gold dimension delta table

Â DeltaTable.createIfNotExists(spark) \\

Â  Â  Â .tableName("wwilakehouse.dimcustomer_gold") \\

Â  Â  Â .addColumn("CustomerName", StringType()) \\

Â  Â  Â .addColumn("Email", Â StringType()) \\

Â  Â  Â .addColumn("First", StringType()) \\

Â  Â  Â .addColumn("Last", StringType()) \\

Â  Â  Â .addColumn("CustomerID", LongType()) \\

Â  Â  Â .execute()

![A screenshot of a computer Description automatically
generated](./media/image44.png)

![A screenshot of a computer Description automatically
generated](./media/image45.png)

2.  ìƒˆ ì½”ë“œ ë¸”ë¡ì—ì„œ **ë‹¤ìŒ ì½”ë“œë¥¼ ì¶”ê°€í•˜ê³  ì‹¤í–‰í•˜ì—¬** ì¤‘ë³µëœ ê³ ê°ì„
    ì‚­ì œí•˜ê³ , íŠ¹ì • ì—´ì„ ì„ íƒí•˜ê³ , "CustomerName" ì—´ì„ ë¶„í• í•˜ì—¬ "First"ì™€
    "Last" ì´ë¦„ ì—´ì„ ë§Œë“­ë‹ˆë‹¤:

> CodeCopy
>
> from pyspark.sql.functions import col, split
>
> \# Create customer_silver dataframe
>
> dfdimCustomer_silver =
> df.dropDuplicates(\["CustomerName","Email"\]).select(col("CustomerName"),col("Email"))
> \\
>
> .withColumn("First",split(col("CustomerName"), " ").getItem(0)) \\
>
> .withColumn("Last",split(col("CustomerName"), " ").getItem(1))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimCustomer_silver.head(10))

![A screenshot of a computer Description automatically
generated](./media/image46.png)

ì¤‘ë³µ í•­ëª© ì‚­ì œ, íŠ¹ì • ì—´ ì„ íƒ, "CustomerName" ì—´ì„ ë¶„í• í•˜ì—¬ "First"ê³¼ "
Last " ì—´ ìƒì„± ë“± ë‹¤ì–‘í•œ ë³€í™˜ì„ ìˆ˜í–‰í•˜ì—¬ ìƒˆë¡œìš´ DataFrame
dfdimCustomer_silverë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, "CustomerName" ì—´ì—ì„œ
ì¶”ì¶œëœ ë³„ë„ì˜ "First"ê³¼ "Last" ì—´ì´ í¬í•¨ëœ ì •ë¦¬ë˜ê³  êµ¬ì¡°í™”ëœ ê³ ê°
ë°ì´í„°ê°€ í¬í•¨ëœ DataFrameì´ ìƒì„±ë©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image47.png)

3.  ë‹¤ìŒìœ¼ë¡œ, **ê³ ê°ì˜ ID ì—´ì„ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤**. ìƒˆ ì½”ë“œ ë¸”ë¡ì— ë‹¤ìŒì„
    ë¶™ì—¬ë„£ê³  ì‹¤í–‰í•©ë‹ˆë‹¤:

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col,
when, coalesce, max, lit

\# Read the existing data from the Delta table

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

\# Find the maximum CustomerID or use 0 if the table is empty

MAXCustomerID =
dfdimCustomer_temp.select(coalesce(max(col("CustomerID")),
lit(0)).alias("MAXCustomerID")).first()\[0\]

\# Assume dfdimCustomer_silver is your source DataFrame with new data

\# Here, we select only the new customers by doing a left anti join

dfdimCustomer_gold = dfdimCustomer_silver.join(

Â  Â  dfdimCustomer_temp,

Â  Â  (dfdimCustomer_silver.CustomerName ==
dfdimCustomer_temp.CustomerName) &

Â  Â  (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email),

Â  Â  "left_anti"

)

\# Add the CustomerID column with unique values starting from
MAXCustomerID + 1

dfdimCustomer_gold = dfdimCustomer_gold.withColumn(

Â  Â  "CustomerID",

Â  Â  monotonically_increasing_id() + MAXCustomerID + 1

)

\# Display the first 10 rows of the dataframe to preview your data

dfdimCustomer_gold.show(10)

![](./media/image48.png)

![A screenshot of a computer Description automatically
generated](./media/image49.png)

4.  ì´ì œ ìƒˆ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ê³ ê° í…Œì´ë¸”ì´ ìµœì‹  ìƒíƒœë¡œ ìœ ì§€ë˜ë„ë¡ í•  ìˆ˜
    ìˆìŠµë‹ˆë‹¤. **ìƒˆ ì½”ë“œ ë¸”ë¡ì—** ë‹¤ìŒì„ ë¶™ì—¬ë„£ê³  ì‹¤í–‰í•©ë‹ˆë‹¤:

> CodeCopy

from delta.tables import DeltaTable

\# Define the Delta table path

deltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')

\# Use dfUpdates to refer to the DataFrame with new or updated records

dfUpdates = dfdimCustomer_gold

\# Perform the merge operation to update or insert new records

deltaTable.alias('silver') \\

Â  .merge(

Â  Â  dfUpdates.alias('updates'),

Â  Â  'silver.CustomerName = updates.CustomerName AND silver.Email =
updates.Email'

Â  ) \\

Â  .whenMatchedUpdate(set =

Â  Â  {

Â  Â  Â  "CustomerName": "updates.CustomerName",

Â  Â  Â  "Email": "updates.Email",

Â  Â  Â  "First": "updates.First",

Â  Â  Â  "Last": "updates.Last",

Â  Â  Â  "CustomerID": "updates.CustomerID"

Â  Â  }

Â  ) \\

Â  .whenNotMatchedInsert(values =

Â  Â  {

Â  Â  Â  "CustomerName": "updates.CustomerName",

Â  Â  Â  "Email": "updates.Email",

Â  Â  Â  "First": "updates.First",

Â  Â  Â  "Last": "updates.Last",

Â  Â  Â  "CustomerID": "updates.CustomerID"

Â  Â  }

Â  ) \\

Â  .execute()

![](./media/image50.png)

![A screenshot of a computer Description automatically
generated](./media/image51.png)

5.  ì´ì œ ì´ **ë‹¨ê³„ë¥¼ ë°˜ë³µí•˜ì—¬ ì œí’ˆ ì°¨ì›ì„ ìƒì„±í•©ë‹ˆë‹¤**. ìƒˆ ì½”ë“œ ë¸”ë¡ì—
    ë‹¤ìŒì„ ë¶™ì—¬ë„£ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.dimproduct_gold") \\
>
> .addColumn("ItemName", StringType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("ItemInfo", StringType()) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image52.png)

![A screenshot of a computer Description automatically
generated](./media/image53.png)

6.  **Product_silver** ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•˜ê¸° ìœ„í•´ **ë˜ ë‹¤ë¥¸ ì½”ë“œ
    ë¸”ë¡ì„** ì¶”ê°€í•©ë‹ˆë‹¤.

> CodeCopy
>
> from pyspark.sql.functions import col, split, lit
>
> \# Create product_silver dataframe
>
> dfdimProduct_silver =
> df.dropDuplicates(\["Item"\]).select(col("Item")) \\
>
> .withColumn("ItemName",split(col("Item"), ", ").getItem(0)) \\
>
> .withColumn("ItemInfo",when((split(col("Item"), ",
> ").getItem(1).isNull() | (split(col("Item"), ",
> ").getItem(1)=="")),lit("")).otherwise(split(col("Item"), ",
> ").getItem(1)))
>
> \# Display the first 10 rows of the dataframe to preview your data
>
> display(dfdimProduct_silver.head(10))

![A screenshot of a computer Description automatically
generated](./media/image54.png)

![A screenshot of a computer Description automatically
generated](./media/image55.png)

7.  ì´ì œ **dimProduct_gold table**ì˜ IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ìƒˆ ì½”ë“œ ë¸”ë¡ì—
    ë‹¤ìŒ êµ¬ë¬¸ì„ ì¶”ê°€í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤:

CodeCopy

from pyspark.sql.functions import monotonically_increasing_id, col, lit,
max, coalesce

\#dfdimProduct_temp = dfdimProduct_silver

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

MAXProductID =
dfdimProduct_temp.select(coalesce(max(col("ItemID")),lit(0)).alias("MAXItemID")).first()\[0\]

dfdimProduct_gold =
dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName
== dfdimProduct_temp.ItemName) & (dfdimProduct_silver.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left_anti")

dfdimProduct_gold =
dfdimProduct_gold.withColumn("ItemID",monotonically_increasing_id() +
MAXProductID + 1)

\# Display the first 10 rows of the dataframe to preview your data

display(dfdimProduct_gold.head(10))

![A screenshot of a computer Description automatically
generated](./media/image56.png)

ì´ëŠ” í‘œì˜ í˜„ì¬ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¤ìŒ ì œí’ˆ IDë¥¼ ê³„ì‚°í•˜ê³ ,
ì´ëŸ¬í•œ ìƒˆë¡œìš´ IDë¥¼ ì œí’ˆì— í• ë‹¹í•œ ë‹¤ìŒ ì—…ë°ì´íŠ¸ëœ ì œí’ˆ ì •ë³´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image57.png)

8.  ë‹¤ë¥¸ ì°¨ì›ì—ì„œ ìˆ˜í–‰í•œ ê²ƒê³¼ ìœ ì‚¬í•˜ê²Œ ìƒˆ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ì œí’ˆ
    í…Œì´ë¸”ì´ ìµœì‹  ìƒíƒœë¡œ ìœ ì§€ë˜ë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤. **ìƒˆ ì½”ë“œ ë¸”ë¡ì—**
    ë‹¤ìŒì„ ë¶™ì—¬ë„£ê³  ì‹¤í–‰í•©ë‹ˆë‹¤:

CodeCopy

from delta.tables import \*

deltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')

dfUpdates = dfdimProduct_gold

deltaTable.alias('silver') \\

.merge(

dfUpdates.alias('updates'),

'silver.ItemName = updates.ItemName AND silver.ItemInfo =
updates.ItemInfo'

) \\

.whenMatchedUpdate(set =

{

}

) \\

.whenNotMatchedInsert(values =

{

"ItemName": "updates.ItemName",

"ItemInfo": "updates.ItemInfo",

"ItemID": "updates.ItemID"

}

) \\

.execute()

![A screenshot of a computer Description automatically
generated](./media/image58.png)

![A screenshot of a computer Description automatically
generated](./media/image59.png)

**ì´ì œ ì°¨ì›ì„ êµ¬ì¶•í–ˆìœ¼ë¯€ë¡œ ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” ì‚¬ì‹¤ í‘œë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.**

9.  **ìƒˆ ì½”ë“œ ë¸”ë¡ì—** ë‹¤ìŒ ì½”ë“œë¥¼ ë¶™ì—¬ë„£ê³  ì‹¤í–‰í•˜ì—¬ **fact table**ì„
    ë§Œë“­ë‹ˆë‹¤.

> CodeCopy
>
> from pyspark.sql.types import \*
>
> from delta.tables import \*
>
> DeltaTable.createIfNotExists(spark) \\
>
> .tableName("wwilakehouse.factsales_gold") \\
>
> .addColumn("CustomerID", LongType()) \\
>
> .addColumn("ItemID", LongType()) \\
>
> .addColumn("OrderDate", DateType()) \\
>
> .addColumn("Quantity", IntegerType()) \\
>
> .addColumn("UnitPrice", FloatType()) \\
>
> .addColumn("Tax", FloatType()) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image60.png)

![A screenshot of a computer Description automatically
generated](./media/image61.png)

10. **ìƒˆ ì½”ë“œ ë¸”ë¡ì—** ë‹¤ìŒ ì½”ë“œë¥¼ ë¶™ì—¬ë„£ê³  ì‹¤í–‰í•˜ì—¬ íŒë§¤ ë°ì´í„°ì™€ ê³ ê°
    ë° ì œí’ˆ ì •ë³´ë¥¼ ê²°í•©í•˜ëŠ” **ìƒˆ ë°ì´í„° í”„ë ˆì„**ì„ ë§Œë“­ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ”
    ê³ ê° ID, í’ˆëª© ID, ì£¼ë¬¸ ë‚ ì§œ, ìˆ˜ëŸ‰, ë‹¨ê°€, ì„¸ê¸ˆì´ í¬í•¨ë©ë‹ˆë‹¤:

CodeCopy

from pyspark.sql import SparkSession

from pyspark.sql.functions import split, col, when, lit

from pyspark.sql.types import StructType, StructField, StringType,
IntegerType, DateType, FloatType, BooleanType, TimestampType

\# Initialize Spark session

spark = SparkSession.builder \\

Â  Â  .appName("DeltaTableUpsert") \\

Â  Â  .config("spark.sql.extensions",
"io.delta.sql.DeltaSparkSessionExtension") \\

Â  Â  .config("spark.sql.catalog.spark_catalog",
"org.apache.spark.sql.delta.catalog.DeltaCatalog") \\

Â  Â  .getOrCreate()

\# Define the schema for the sales_silver table

silver_table_schema = StructType(\[

Â  Â  StructField("SalesOrderNumber", StringType(), True),

Â  Â  StructField("SalesOrderLineNumber", IntegerType(), True),

Â  Â  StructField("OrderDate", DateType(), True),

Â  Â  StructField("CustomerName", StringType(), True),

Â  Â  StructField("Email", StringType(), True),

Â  Â  StructField("Item", StringType(), True),

Â  Â  StructField("Quantity", IntegerType(), True),

Â  Â  StructField("UnitPrice", FloatType(), True),

Â  Â  StructField("Tax", FloatType(), True),

Â  Â  StructField("FileName", StringType(), True),

Â  Â  StructField("IsFlagged", BooleanType(), True),

Â  Â  StructField("CreatedTS", TimestampType(), True),

Â  Â  StructField("ModifiedTS", TimestampType(), True)

\])

\# Define the path to the Delta table (ensure this path is correct)

delta_table_path =
"abfss://\<container\>@\<storage-account\>.dfs.core.windows.net/path/to/wwilakehouse/sales_silver"

\# Create a DataFrame with the defined schema

empty_df = spark.createDataFrame(\[\], silver_table_schema)

\# Register the Delta table in the Metastore

spark.sql(f"""

Â  Â  CREATE TABLE IF NOT EXISTS wwilakehouse.sales_silver

Â  Â  USING DELTA

Â  Â  LOCATION '{delta_table_path}'

""")

\# Load data into DataFrame

df = spark.read.table("wwilakehouse.sales_silver")

\# Perform transformations on df

df = df.withColumn("ItemName", split(col("Item"), ", ").getItem(0)) \\

Â  Â  .withColumn("ItemInfo", when(

Â  Â  Â  Â  (split(col("Item"), ", ").getItem(1).isNull()) |
(split(col("Item"), ", ").getItem(1) == ""),

Â  Â  Â  Â  lit("")

Â  Â  ).otherwise(split(col("Item"), ", ").getItem(1)))

\# Load additional DataFrames for joins

dfdimCustomer_temp = spark.read.table("wwilakehouse.dimCustomer_gold")

dfdimProduct_temp = spark.read.table("wwilakehouse.dimProduct_gold")

\# Create Sales_gold dataframe

dffactSales_gold = df.alias("df1").join(dfdimCustomer_temp.alias("df2"),
(df.CustomerName == dfdimCustomer_temp.CustomerName) & (df.Email ==
dfdimCustomer_temp.Email), "left") \\

Â  Â  .join(dfdimProduct_temp.alias("df3"), (df.ItemName ==
dfdimProduct_temp.ItemName) & (df.ItemInfo ==
dfdimProduct_temp.ItemInfo), "left") \\

Â  Â  .select(

Â  Â  Â  Â  col("df2.CustomerID"),

Â  Â  Â  Â  col("df3.ItemID"),

Â  Â  Â  Â  col("df1.OrderDate"),

Â  Â  Â  Â  col("df1.Quantity"),

Â  Â  Â  Â  col("df1.UnitPrice"),

Â  Â  Â  Â  col("df1.Tax")

Â  Â  ).orderBy(col("df1.OrderDate"), col("df2.CustomerID"),
col("df3.ItemID"))

\# Show the result

dffactSales_gold.show()

![A screenshot of a computer Description automatically
generated](./media/image62.png)

![A screenshot of a computer Description automatically
generated](./media/image63.png)

![A screenshot of a computer Description automatically
generated](./media/image64.png)

1.  ì´ì œ **ìƒˆ ì½”ë“œ ë¸”ë¡ì—ì„œ** ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ íŒë§¤ ë°ì´í„°ê°€ ìµœì‹ 
    ìƒíƒœë¡œ ìœ ì§€ë˜ë„ë¡ í•©ë‹ˆë‹¤:

> CodeCopy
>
> from delta.tables import \*
>
> deltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')
>
> dfUpdates = dffactSales_gold
>
> deltaTable.alias('silver') \\
>
> .merge(
>
> dfUpdates.alias('updates'),
>
> 'silver.OrderDate = updates.OrderDate AND silver.CustomerID =
> updates.CustomerID AND silver.ItemID = updates.ItemID'
>
> ) \\
>
> .whenMatchedUpdate(set =
>
> {
>
> }
>
> ) \\
>
> .whenNotMatchedInsert(values =
>
> {
>
> "CustomerID": "updates.CustomerID",
>
> "ItemID": "updates.ItemID",
>
> "OrderDate": "updates.OrderDate",
>
> "Quantity": "updates.Quantity",
>
> "UnitPrice": "updates.UnitPrice",
>
> "Tax": "updates.Tax"
>
> }
>
> ) \\
>
> .execute()

![A screenshot of a computer Description automatically
generated](./media/image65.png)

ì—¬ê¸°ì„œëŠ” Delta Lakeì˜ ë³‘í•© ì‘ì—…ì„ ì‚¬ìš©í•˜ì—¬ factsales_gold í…Œì´ë¸”ì„
ìƒˆë¡œìš´ íŒë§¤ ë°ì´í„°(dffactSales_gold)ë¡œ ë™ê¸°í™”í•˜ê³  ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì´
ì‘ì—…ì€ ê¸°ì¡´ ë°ì´í„°(silver í…Œì´ë¸”)ì™€ ìƒˆ ë°ì´í„°(updates DataFrame)ì˜ ì£¼ë¬¸
ë‚ ì§œ, ê³ ê° ID, í’ˆëª© IDë¥¼ ë¹„êµí•˜ì—¬ ì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  í•„ìš”ì—
ë”°ë¼ ìƒˆ ë ˆì½”ë“œë¥¼ ì‚½ì…í•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image66.png)

ì´ì œ ë³´ê³  ë° ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íë ˆì´íŒ…ë˜ê³  ëª¨ë¸ë§ëœ ê³¨ë“œ ë ˆì´ì–´ê°€
ìƒê²¼ìŠµë‹ˆë‹¤.

# ì—°ìŠµ 4: Azure Databricksì™€ Azure Data Lake Storage(ADLS) Gen 2 ê°„ ì—°ê²° ì„¤ì •

ì´ì œ Azure Databricksë¥¼ ì‚¬ìš©í•˜ì—¬ Azure Data Lake Storage(ADLS) Gen2
ê³„ì •ì˜ ë„ì›€ì„ ë°›ì•„ ë¸íƒ€ í…Œì´ë¸”ì„ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ADLSì—ì„œ
ë¸íƒ€ í…Œì´ë¸”ì— ëŒ€í•œ OneLake ë°”ë¡œ ê°€ê¸°ë¥¼ ë§Œë“¤ê³ , Power BIë¥¼ ì‚¬ìš©í•˜ì—¬ ADLS
ë°”ë¡œ ê°€ê¸°ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë¶„ì„í•´ ë³´ê² ìŠµë‹ˆë‹¤.

## **ì‘ì—… 0: Azure íŒ¨ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê³  Azure êµ¬ë…ì„ í™œì„±í™”í•˜ê¸°**

1.  ë‹¤ìŒ ë§í¬ë¡œ ì´ë™í•©ë‹ˆë‹¤ !!https://www.microsoftazurepass.com/!!
    ê·¸ë¦¬ê³  **Start** ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.

![](./media/image67.png)

2.  Microsoft ë¡œê·¸ì¸ í˜ì´ì§€ì—ì„œ **Tenant ID**ë¥¼ ì…ë ¥í•˜ê³  **Next**ë¥¼
    í´ë¦­í•©ë‹ˆë‹¤.

![](./media/image68.png)

3.  ë‹¤ìŒ í˜ì´ì§€ì—ì„œ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ê³  **Sign In**ì„ í´ë¦­í•©ë‹ˆë‹¤.

![](./media/image69.png)

![A screenshot of a computer error Description automatically
generated](./media/image70.png)

4.  ë¡œê·¸ì¸í•œ í›„ Microsoft Azure í˜ì´ì§€ì—ì„œ **Confirm Microsoft
    Account**íƒ­ì„ í´ë¦­í•©ë‹ˆë‹¤.

![](./media/image71.png)

5.  ë‹¤ìŒ í˜ì´ì§€ì—ì„œ í”„ë¡œëª¨ì…˜ ì½”ë“œì™€ Captcha ë¬¸ìë¥¼ ì…ë ¥í•˜ê³  **Submit**ì„
    í´ë¦­í•˜ì„¸ìš”.

![A screenshot of a computer Description automatically
generated](./media/image72.png)

![A screenshot of a computer error Description automatically
generated](./media/image73.png)

6.  í”„ë¡œí•„ í˜ì´ì§€ì—ì„œ í”„ë¡œí•„ ì„¸ë¶€ ì •ë³´ë¥¼ ì…ë ¥í•˜ê³  **Sign up**ì„
    í´ë¦­í•˜ì„¸ìš”.

7.  ë©”ì‹œì§€ê°€ í‘œì‹œë˜ë©´ ë‹¤ì¤‘ ì¸ì¦ì— ê°€ì…í•œ í›„ ë‹¤ìŒ ë§í¬ë¡œ ì´ë™í•˜ì—¬ Azure
    Portalì— ë¡œê·¸ì¸í•˜ì„¸ìš”!! <https://portal.azure.com/#home>!!

![](./media/image74.png)

8.  ê²€ìƒ‰ì°½ì— êµ¬ë…ì„ ì…ë ¥í•˜ê³  **Services.** ì•„ë˜ì— ìˆëŠ” êµ¬ë… ì•„ì´ì½˜ì„
    í´ë¦­í•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image75.png)

9.  Azure Passë¥¼ ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ êµ¬ë… IDê°€ ìƒì„±ë©ë‹ˆë‹¤.

![](./media/image76.png)

## **ì‘ì—… 1: Azure Data Storage ê³„ì • ë§Œë“¤ê¸°**

1.  Azure ìê²© ì¦ëª…ì„ ì‚¬ìš©í•˜ì—¬ Azure Portalì— ë¡œê·¸ì¸í•©ë‹ˆë‹¤.

2.  í™ˆí˜ì´ì§€ ì™¼ìª½ í¬í„¸ ë©”ë‰´ì—ì„œ **Storage accounts**Â ì„ ì„ íƒí•˜ë©´ ì €ì¥ì†Œ
    ê³„ì • ëª©ë¡ì´ í‘œì‹œë©ë‹ˆë‹¤. í¬í„¸ ë©”ë‰´ê°€ ë³´ì´ì§€ ì•Šìœ¼ë©´ ë©”ë‰´ ë²„íŠ¼ì„
    ì„ íƒí•˜ì—¬ í‚µë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image77.png)

3.  **Storage accounts**Â í˜ì´ì§€ì—ì„œ **Create**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image78.png)

4.  Basics íƒ­ì—ì„œ ë¦¬ì†ŒìŠ¤ ê·¸ë£¹ì„ ì„ íƒí•˜ë©´ ì €ì¥ì†Œ ê³„ì •ì— ëŒ€í•œ í•„ìˆ˜ ì •ë³´ë¥¼
    ì œê³µí•©ë‹ˆë‹¤:

[TABLE]

ë‹¤ë¥¸ ì„¤ì •ì€ ê·¸ëŒ€ë¡œ ë‘ê³  **Review + create**ë¥¼ ì„ íƒí•˜ì—¬ ê¸°ë³¸ ì˜µì…˜ì„
ìˆ˜ë½í•˜ê³  ê³„ì •ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.

ì°¸ê³ : ì•„ì§ ë¦¬ì†ŒìŠ¤ ê·¸ë£¹ì„ ìƒì„±í•˜ì§€ ì•Šì€ ê²½ìš° "**Create new**"ë¥¼ í´ë¦­í•˜ê³ 
ìŠ¤í† ë¦¬ì§€ ê³„ì •ì— ëŒ€í•œ ìƒˆ ë¦¬ì†ŒìŠ¤ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![](./media/image79.png)

![](./media/image80.png)

![A screenshot of a computer Description automatically
generated](./media/image81.png)

![A screenshot of a computer Description automatically
generated](./media/image82.png)

![A screenshot of a computer Description automatically
generated](./media/image83.png)

5.  **Review + create**Â íƒ­ìœ¼ë¡œ ì´ë™í•˜ë©´ Azureì—ì„œ ì„ íƒí•œ ì €ì¥ì†Œ ê³„ì •
    ì„¤ì •ì— ëŒ€í•œ ìœ íš¨ì„± ê²€ì‚¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. ìœ íš¨ì„± ê²€ì‚¬ì— í†µê³¼í•˜ë©´ ì €ì¥ì†Œ
    ê³„ì • ë§Œë“¤ê¸°ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìœ íš¨ì„± ê²€ì‚¬ì— ì‹¤íŒ¨í•˜ë©´ í¬í„¸ì€ ì–´ë–¤ ì„¤ì •ì„ ìˆ˜ì •í•´ì•¼ í•˜ëŠ”ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image84.png)

![A screenshot of a computer Description automatically
generated](./media/image85.png)

ì´ì œ Azure ë°ì´í„° ì €ì¥ì†Œ ê³„ì •ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

6.  í˜ì´ì§€ ìƒë‹¨ì˜ ê²€ìƒ‰ì°½ì—ì„œ ê²€ìƒ‰í•˜ì—¬ ìŠ¤í† ë¦¬ì§€ ê³„ì • í˜ì´ì§€ë¡œ ì´ë™í•œ í›„
    ìƒˆë¡œ ë§Œë“  ìŠ¤í† ë¦¬ì§€ ê³„ì •ì„ ì„ íƒí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image86.png)

7.  ì €ì¥ì†Œ ê³„ì • í˜ì´ì§€ì—ì„œ ì™¼ìª½ íƒìƒ‰ ì°½ì˜ **Data storage**ì•„ë˜ì— ìˆëŠ”
    **Containers**ë¡œ ì´ë™í•˜ì—¬ !!medalion1!!ì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ìƒˆ ì»¨í…Œì´ë„ˆë¥¼
    ë§Œë“¤ê³  **Create** ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.

Â 

![A screenshot of a computer Description automatically
generated](./media/image87.png)

8.  ì´ì œ **storage account**í˜ì´ì§€ë¡œ ëŒì•„ê°€ ì™¼ìª½ íƒìƒ‰ ë©”ë‰´ì—ì„œ
    **Endpoints**ë¥¼ ì„ íƒí•˜ì„¸ìš”. ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤í•˜ì—¬ **Primary endpoint
    URL**ì„ ë³µì‚¬í•˜ì—¬ ë©”ëª¨ì¥ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”. ë°”ë¡œê°€ê¸°ë¥¼ ë§Œë“¤ ë•Œ ë„ì›€ì´ ë 
    ê²ƒì…ë‹ˆë‹¤.![](./media/image88.png)

9.  ë§ˆì°¬ê°€ì§€ë¡œ, ê°™ì€ íƒìƒ‰ íŒ¨ë„ì—ì„œ **Access keys**ë¡œ ì´ë™í•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image89.png)

## **ì‘ì—… 2: Delta í…Œì´ë¸” ë§Œë“¤ê¸°, ë°”ë¡œ ê°€ê¸° ë§Œë“¤ê¸°, Lakehouseì˜ ë°ì´í„° ë¶„ì„**

1.  lakehouseì—ì„œ íŒŒì¼ ì˜†ì— ìˆëŠ” ì¤„ì„í‘œ**(â€¦)**ë¥¼ ì„ íƒí•œ ë‹¤ìŒ **New
    shortcut**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

![](./media/image90.png)

2.  **New shortcut**í™”ë©´ì—ì„œ **Azure Data Lake Storage Gen2** íƒ€ì¼ì„
    ì„ íƒí•©ë‹ˆë‹¤.

![Screenshot of the tile options in the New shortcut
screen.](./media/image91.png)

3.  ë°”ë¡œê°€ê¸°ì— ëŒ€í•œ ì—°ê²° ì„¸ë¶€ ì •ë³´ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.

[TABLE]

4.  **Next**ë¥¼ í´ë¦­í•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image92.png)

5.  Azure storage containerì™€ì˜ ì—°ê²°ì´ ì„¤ì •ë©ë‹ˆë‹¤. ì €ì¥ì†Œë¥¼ ì„ íƒí•˜ê³ 
    **Next**ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.

![A screenshot of a computer Description automatically
generated](./media/image93.png)

![A screenshot of a computer Description automatically
generated](./media/image94.png)![A screenshot of a computer Description
automatically generated](./media/image95.png)

6.  Wizardê°€ ì‹¤í–‰ë˜ë©´ **Files**ì„ ì„ íƒí•˜ê³  **bronze** íŒŒì¼ì—ì„œ
    **"..."**ì„ ì„ íƒí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image96.png)

7.  **load to tables**ê³¼ **new table**. **load to tables**ì„ ì„ íƒí•©ë‹ˆë‹¤.

![](./media/image97.png)

8.  íŒì—… ì°½ì—ì„œ í…Œì´ë¸” ì´ë¦„ì„ **bronze_01**ë¡œ ì…ë ¥í•˜ê³  íŒŒì¼ ìœ í˜•ì„
    **parquet**ë¡œ ì„ íƒí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image98.png)

![A screenshot of a computer Description automatically
generated](./media/image99.png)

![A screenshot of a computer Description automatically
generated](./media/image100.png)

![A screenshot of a computer Description automatically
generated](./media/image101.png)

9.  **bronze_01** íŒŒì¼ì´ ì´ì œ íŒŒì¼ì—ì„œ í‘œì‹œë©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image102.png)

10. ë‹¤ìŒìœ¼ë¡œ, **bronze** íŒŒì¼ì—ì„œ "..."ì„ ì„ íƒí•©ë‹ˆë‹¤. **load to
    tables**ì™€ **existing table**ì„ ì„ íƒí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image103.png)

11. ê¸°ì¡´ í…Œì´ë¸” ì´ë¦„ì„ **dimcustomer_gold**ë¡œ ì§€ì •í•˜ê³ , íŒŒì¼ í˜•ì‹ì„
    **parquet**ë¡œ ì„ íƒí•œ í›„ **load**ë¥¼ ì„ íƒí•˜ì„¸ìš”.

![A screenshot of a computer Description automatically
generated](./media/image104.png)

![A screenshot of a computer Description automatically
generated](./media/image105.png)

## **ì‘ì—… 3: ê³¨ë“œ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ì˜ë¯¸ ëª¨ë¸ ìƒì„±**

ì´ì œ ì‘ì—… ê³µê°„ì—ì„œ ê³¨ë“œ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ë°ì´í„°ë¥¼
ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‘ì—… ê³µê°„ì—ì„œ ì˜ë¯¸ ëª¨ë¸ì— ì§ì ‘ ì•¡ì„¸ìŠ¤í•˜ì—¬ ë³´ê³ ë¥¼
ìœ„í•œ ê´€ê³„ ë° ì¸¡ì •ê°’ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Lakehouseë¥¼ ìƒì„±í•  ë•Œ ìë™ìœ¼ë¡œ ìƒì„±ë˜ëŠ” **ê¸°ë³¸ ì˜ë¯¸ ëª¨ë¸ì„** ì‚¬ìš©í•  ìˆ˜
ì—†ìŠµë‹ˆë‹¤. Lakehouse íƒìƒ‰ê¸°ì—ì„œ ì´ ë©ì—ì„œ ìƒì„±í•œ ê³¨ë“œ í…Œì´ë¸”ì„ í¬í•¨í•˜ëŠ”
ìƒˆ ì˜ë¯¸ ëª¨ë¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

1.  ì‘ì—… ê³µê°„ì—ì„œ **wwilakehouse l**akehouseë¡œ ì´ë™í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ
    lakehouse íƒìƒ‰ê¸° ë·°ì˜ ë¦¬ë³¸ ë©”ë‰´ì—ì„œ **New semantic model**ì„
    ì„ íƒí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image106.png)

2.  íŒì—…ì—ì„œ ìƒˆ ì˜ë¯¸ ëª¨ë¸ì— **DatabricksTutorial**ì´ë¼ëŠ” ì´ë¦„ì„ ì§€ì •í•˜ê³ 
    ì‘ì—… ê³µê°„ìœ¼ë¡œ **Fabric Lakehouse Tutorial-29**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

![](./media/image107.png)

3.  ë‹¤ìŒìœ¼ë¡œ, ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤í•˜ì—¬ ì˜ë¯¸ ëª¨ë¸ì— í¬í•¨í•  ëª¨ë“  í•­ëª©ì„ ì„ íƒí•˜ê³ 
    **Confirm**ì„ ì„ íƒí•©ë‹ˆë‹¤.

ì´ë ‡ê²Œ í•˜ë©´ Fabricì—ì„œ ì˜ë¯¸ ëª¨ë¸ì´ ì—´ë¦¬ê³  ì—¬ê¸°ì„œ ê´€ê³„ì™€ ì¸¡ì •ê°’ì„ ìƒì„±í• 
ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì•„ë˜ ê·¸ë¦¼ ì°¸ì¡°).

![A screenshot of a computer Description automatically
generated](./media/image108.png)

ì—¬ê¸°ì—ì„œ ì‚¬ìš©ì ë˜ëŠ” ë°ì´í„° íŒ€ì˜ ë‹¤ë¥¸ êµ¬ì„±ì›ì´lakehouseì˜ ë°ì´í„°ë¥¼
ê¸°ë°˜ìœ¼ë¡œ ë³´ê³ ì„œì™€ ëŒ€ì‹œë³´ë“œë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë³´ê³ ì„œëŠ”lakehouseì˜
ê³¨ë“œ ë ˆì´ì–´ì— ì§ì ‘ ì—°ê²°ë˜ë¯€ë¡œ í•­ìƒ ìµœì‹  ë°ì´í„°ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤.

# ì—°ìŠµ 5: Azure Databricksë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„

1.  Power BI ì„œë¹„ìŠ¤ì—ì„œlakehouseë¡œ ì´ë™í•˜ì—¬ **Get data**Â ë¥¼ ì„ íƒí•œ ë‹¤ìŒ
    **New data pipeline**ì„ ì„ íƒí•©ë‹ˆë‹¤.

![Screenshot showing how to navigate to new data pipeline option from
within the UI.](./media/image109.png)

1.  **ìƒˆ íŒŒì´í”„ë¼ì¸** í”„ë¡¬í”„íŠ¸ì—ì„œ ìƒˆ íŒŒì´í”„ë¼ì¸ì˜ ì´ë¦„ì„ ì…ë ¥í•œ ë‹¤ìŒ
    **Create**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. **IngestDatapipeline01**

![](./media/image110.png)

2.  ì´ ì—°ìŠµì—ì„œëŠ” ë°ì´í„° ì†ŒìŠ¤ë¡œ **NYC Taxi - Green**Â ìƒ˜í”Œ ë°ì´í„°ë¥¼
    ì„ íƒí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image111.png)

3.  ë¯¸ë¦¬ë³´ê¸° í™”ë©´ì—ì„œ **Next**ì„ ì„ íƒí•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image112.png)

4.  ë°ì´í„° ëŒ€ìƒì—ì„œ OneLake Delta í…Œì´ë¸” ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©í• 
    í…Œì´ë¸” ì´ë¦„ì„ ì„ íƒí•˜ì„¸ìš”. ê¸°ì¡´ í…Œì´ë¸”ì„ ì„ íƒí•˜ê±°ë‚˜ ìƒˆ í…Œì´ë¸”ì„ ë§Œë“¤
    ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì‹¤ìŠµì—ì„œëŠ” **load into new tableì„** ì„ íƒí•˜ê³ 
    **Next**ë¥¼ ì„ íƒí•˜ì„¸ìš”.

![A screenshot of a computer Description automatically
generated](./media/image113.png)

5.  **Review + Save**Â  í™”ë©´ì—ì„œ **Start data transfer immediately**ì„
    ì„ íƒí•œ ë‹¤ìŒ **Save + Run**ì„ ì„ íƒí•©ë‹ˆë‹¤.

![Screenshot showing how to enter table name.](./media/image114.png)

![A screenshot of a computer Description automatically
generated](./media/image115.png)

6.  ì‘ì—…ì´ ì™„ë£Œë˜ë©´ Lakehouseë¡œ ì´ë™í•˜ì—¬ /Tables ì•„ë˜ì— ë‚˜ì—´ëœ ë¸íƒ€
    í…Œì´ë¸”ì„ í™•ì¸í•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image116.png)

7.  íƒìƒ‰ê¸° ë³´ê¸°ì—ì„œ í…Œì´ë¸” ì´ë¦„ì„ ë§ˆìš°ìŠ¤ ì˜¤ë¥¸ìª½ ë²„íŠ¼ìœ¼ë¡œ í´ë¦­í•˜ê³ 
    **Propertiesì„** ì„ íƒí•˜ì—¬ Azure Blob Filesystem(ABFS) ê²½ë¡œë¥¼ ë¸íƒ€
    í…Œì´ë¸”ì— ë³µì‚¬í•©ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image117.png)

8.  Azure Databricks ë…¸íŠ¸ë¶ì„ ì—´ê³  ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

olsPath = "**abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample**"

df=spark.read.format('delta').option("inferSchema","true").load(olsPath)

df.show(5)

*ì°¸ê³ : êµµê²Œ í‘œì‹œëœ íŒŒì¼ ê²½ë¡œë¥¼ ë³µì‚¬í•œ íŒŒì¼ ê²½ë¡œë¡œ ë°”ê¿‰ë‹ˆë‹¤.*

![](./media/image118.png)

9.  í•„ë“œ ê°’ì„ ë³€ê²½í•˜ì—¬ ë¸íƒ€ í…Œì´ë¸” ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

%sql

update delta.\`abfss://\<replace with workspace
name\>@onelake.dfs.fabric.microsoft.com/\<replace with item
name\>.Lakehouse/Tables/nycsample\` set vendorID = 99999 where vendorID
= 1;

*ì°¸ê³ : êµµê²Œ í‘œì‹œëœ íŒŒì¼ ê²½ë¡œë¥¼ ë³µì‚¬í•œ íŒŒì¼ ê²½ë¡œë¡œ ë°”ê¿‰ë‹ˆë‹¤.*

![A screenshot of a computer Description automatically
generated](./media/image119.png)

# ì—°ìŠµ 6: ë¦¬ì†ŒìŠ¤ ì •ë¦¬

ì´ ì—°ìŠµì—ì„œëŠ” Microsoft Fabric Lakehouseì—ì„œ ë©”ë‹¬ë¦¬ì˜¨ ì•„í‚¤í…ì²˜ë¥¼ ë§Œë“œëŠ”
ë°©ë²•ì„ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤.

Lakehouseë¥¼ ì‚´í´ë³´ì•˜ìœ¼ë©´ ì´ ì—°ìŠµì„ ìœ„í•´ ë§Œë“  ì‘ì—… ê³µê°„ì„ ì‚­ì œí•  ìˆ˜
ìˆìŠµë‹ˆë‹¤.

1.  ì™¼ìª½ íƒìƒ‰ ë©”ë‰´ì—ì„œ **Fabric Lakehouse Tutorial-29** ì‘ì—… ê³µê°„ì„
    ì„ íƒí•˜ì„¸ìš”. ì‘ì—… ê³µê°„ í•­ëª© ë³´ê¸°ê°€ ì—´ë¦½ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image120.png)

2.  ì‘ì—… ê³µê°„ ì´ë¦„ ì•„ë˜ì—ì„œ ... ì˜µì…˜ì„ ì„ íƒí•˜ê³  **Workspace settings**ì„
    ì„ íƒí•©ë‹ˆë‹¤.Â 

![A screenshot of a computer Description automatically
generated](./media/image121.png)

3.  ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤í•˜ì—¬ **Remove this workdspace.**

![A screenshot of a computer Description automatically
generated](./media/image122.png)

4.  íŒì—…ë˜ëŠ” ê²½ê³ ì—ì„œ **Deleteë¥¼** í´ë¦­í•©ë‹ˆë‹¤.

![A white background with black text Description automatically
generated](./media/image123.png)

5.  ë‹¤ìŒ ì‹¤ìŠµìœ¼ë¡œ ì§„í–‰í•˜ê¸° ì „ì— ì‘ì—…ê³µê°„ì´ ì‚­ì œë˜ì—ˆë‹¤ëŠ” ì•Œë¦¼ì„
    ê¸°ë‹¤ë¦½ë‹ˆë‹¤.

![A screenshot of a computer Description automatically
generated](./media/image124.png)

**ìš”ì•½**:

ì´ ë©ì€ ì°¸ê°€ìê°€ ë…¸íŠ¸ë¶ì„ ì‚¬ìš©í•˜ì—¬ Microsoft Fabric Lakehouseì—ì„œ
ë©”ë‹¬ë¦¬ì˜¨ ì•„í‚¤í…ì²˜ë¥¼ ë¹Œë“œí•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. ì£¼ìš” ë‹¨ê³„ì—ëŠ” ì‘ì—… ê³µê°„
ì„¤ì •, Lakehouse ì„¤ì •, ì´ˆê¸° ìˆ˜ì§‘ì„ ìœ„í•´ ë¸Œë¡ ì¦ˆ ê³„ì¸µì— ë°ì´í„° ì—…ë¡œë“œ,
êµ¬ì¡°ì  ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‹¤ë²„ ë¸íƒ€ í…Œì´ë¸”ë¡œ ë³€í™˜, ê³ ê¸‰ ë¶„ì„ì„ ìœ„í•´ ê³¨ë“œ ë¸íƒ€
í…Œì´ë¸”ë¡œ ì¶”ê°€ ì„¸ë¶„í™”, ì˜ë¯¸ ì²´ê³„ ëª¨ë¸ íƒìƒ‰, í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ì„ ìœ„í•œ
ë°ì´í„° ê´€ê³„ ìƒì„±ì´ í¬í•¨ë©ë‹ˆë‹¤.

## 
